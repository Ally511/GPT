{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:50:53.809092Z",
     "start_time": "2025-07-30T12:50:53.788786Z"
    }
   },
   "source": [
    "# import necessary imports\n",
    "import numpy as np\n",
    "#from numpy_ml.neural_nets.optimizers import Adam\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn.functional import one_hot"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:29:35.898826Z",
     "start_time": "2025-07-30T13:29:35.883687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_batch(input, batch_size,chunk_size):\n",
    "\n",
    "    input_batch = []\n",
    "    # print(type(input_batch))\n",
    "    target_batch = []\n",
    "    idx = np.random.randint(0,len(input)-(chunk_size+1),size=batch_size)\n",
    "    for i in range(0,len(idx)-1):\n",
    "        input_batch.append(input[idx[i]:idx[i]+chunk_size])\n",
    "        target_batch.append(input[idx[i]+1:idx[i]+(chunk_size+1)])\n",
    "    \n",
    "    input_batch = np.array(input_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    return input_batch, target_batch"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:10:22.690210Z",
     "start_time": "2025-07-30T13:10:20.643894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "  indices_text = eval(f.read())\n",
    "  \n",
    "x,y = get_batch(indices_text,4,8)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(y.shape)\n",
    "print(y)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(3, 8)\n",
      "[[1263  791  190  360  608  127 1263 1285]\n",
      " [ 438 1170 1539 1459  462  622 1401  966]\n",
      " [1285  335 1540 1336   14 1285  485 1274]]\n",
      "(3, 8)\n",
      "[[ 791  190  360  608  127 1263 1285  413]\n",
      " [1170 1539 1459  462  622 1401  966  797]\n",
      " [ 335 1540 1336   14 1285  485 1274 1386]]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T15:02:06.858697Z",
     "start_time": "2025-07-30T15:02:06.828752Z"
    }
   },
   "source": [
    "class neural_embedding:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding_table = np.random.rand(vocab_size, vocab_size)\n",
    "\n",
    "    def calculate_softmax(self, x):\n",
    "        \"\"\"Takes input array x and returns softmax.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # Numerical stability\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "    def calculate_cross_entropy(self, y_hatless, y_hat):\n",
    "        \"\"\"\n",
    "        Takes target (y_hatless) and prediction (y_hat) and computes cross entropy loss.\n",
    "        \"\"\"\n",
    "        # get vocab_size\n",
    "        _, _, vocab_size = y_hat.shape        \n",
    "        y_hat = y_hat.reshape(y_hat.shape[0]*y_hat.shape[1], y_hat.shape[2])\n",
    "        y_hatless_flat = y_hatless.reshape(-1)\n",
    "        # one-hot encode targets\n",
    "        y_hatless_hot = np.eye(vocab_size)[y_hatless_flat]\n",
    "       \n",
    "        y_hat = self.calculate_softmax(y_hat)\n",
    "        \n",
    "        # Clip predictions to avoid log(0)\n",
    "        y_hat = np.clip(y_hat, 1e-15, 1.0) \n",
    "        return -np.sum(y_hatless_hot*np.log(y_hat))\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Implements forward pass with an unnecessary logitte function \n",
    "        which i only did not delete because now I'm emotionally attached.\n",
    "        Args:\n",
    "            idx(np.array): (B,T) numpy array of integers\n",
    "            targets(np.array): (B,T) numpy array of integers\n",
    "        Returns:\n",
    "            input_logits(np.array)\n",
    "            sometimes also: targets(np.array)\n",
    "        \"\"\"\n",
    "        batch_size, chunk_size = idx.shape\n",
    "        logits = np.zeros((batch_size, chunk_size, (self.token_embedding_table[0].size)))\n",
    "\n",
    "        def logitte(batch_size, chunk_size, input):\n",
    "            for batch in range(batch_size):\n",
    "                for chunk in range(chunk_size):\n",
    "                    # (B,T,C) b=batch_size, t=\"time\"=chunk_size, c=vocab_size\n",
    "                    logits[batch][chunk] = self.token_embedding_table[input[batch][chunk]]\n",
    "                    \n",
    "            return logits\n",
    "\n",
    "        input_logits = logitte(batch_size, chunk_size, idx)\n",
    "        \n",
    "        if targets is not None:\n",
    "            loss = self.calculate_cross_entropy(targets, input_logits)\n",
    "\n",
    "            return input_logits, loss\n",
    "\n",
    "        return input_logits\n",
    "\n",
    "    \n",
    "    def backward(self, targets, input_logits):\n",
    "        batch_size = 32\n",
    "        chunk_size = 8\n",
    "        # need to do the same reshaping as we did for cross entropy, apparently\n",
    "        targets_flat = targets.reshape(-1)\n",
    "        one_hot_targets = np.eye(self.vocab_size)[targets_flat]\n",
    "\n",
    "        # shape after: ((batch_size*chunk_size), vocab_size)\n",
    "        input_logits_2d = input_logits.reshape(input_logits.shape[0]*input_logits.shape[1], input_logits.shape[2])\n",
    "        \n",
    "        soft_input = self.calculate_softmax(input_logits_2d)\n",
    "        # somehow this is supposedly the combiantion of the derivative of softmax with the derivative of the CCE\n",
    "        # delta = one_hot_targets - soft_input\n",
    "        delta = soft_input - one_hot_targets\n",
    "\n",
    "        # want shape (80,80) for matrix multiplication, but with correct indices (use one-hot targets for that)\n",
    "        delta_indexed = np.dot(one_hot_targets.transpose(),delta)\n",
    "\n",
    "        # compute gradient for weight matrix: dot product between the transpose of the to layer and delta vector computed above\n",
    "        gradient = (self.token_embedding_table.T @ delta_indexed) \n",
    "        \n",
    "        # Calculate gradients for the embedding table\n",
    "        # gradient = np.zeros_like(self.token_embedding_table)\n",
    "        # for batch in range(batch_size):\n",
    "        #     for chunk in range(chunk_size):\n",
    "        #         gradient[idx[batch][chunk]] += delta[batch][chunk]\n",
    "    \n",
    "        return gradient\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T14:16:31.308038Z",
     "start_time": "2025-07-30T14:16:31.290415Z"
    }
   },
   "source": [
    "# currently do not use loss at all, so something is probably very wrong\n",
    "\n",
    "def train(model, text, optimiser, param_tensor, train_step):\n",
    "    batch_size=32\n",
    "    chunk_size = 8\n",
    "    for steps in range(train_step): \n",
    "        # sample batch of data\n",
    "        xb, yb = get_batch(text, batch_size, chunk_size) \n",
    "        optimiser.zero_grad()  # reset gradients\n",
    "        # get logits and loss\n",
    "        logits, loss = model.forward(xb, yb)\n",
    "        print(f\"loss: {loss}\")\n",
    "\n",
    "        gradient = model.backward(yb, logits)\n",
    "        param_tensor.grad = torch.tensor(gradient, dtype=torch.float32)\n",
    "        optimiser.step()  # apply gradients to parameters\n",
    "        # diff = model.token_embedding_table - np.array(token_embedding_table)\n",
    "        # print(f\"Difference in weights before - afte Adam: {diff}\")\n",
    "        #/model.token_embedding_table = np.array(token_embedding_table)\n",
    "        model.token_embedding_table = param_tensor.detach().numpy()\n",
    "        "
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T14:16:37.357596Z",
     "start_time": "2025-07-30T14:16:36.425371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "    indices_text = eval(f.read())\n",
    "with open(r\"vocab_train.txt\", 'r') as f:\n",
    "    vocab_train = eval(f.read())"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T15:05:54.724449Z",
     "start_time": "2025-07-30T15:02:12.007409Z"
    }
   },
   "source": [
    "vocab_size = len(vocab_train)\n",
    "\n",
    "my_neural_embedding = neural_embedding(vocab_size)\n",
    "param_dict = {\"weight\": my_neural_embedding.token_embedding_table} # one entry, key is weight and value is my_neural\n",
    "\n",
    "tensor = torch.tensor(my_neural_embedding.token_embedding_table, dtype=torch.float32, requires_grad=True)\n",
    "optimiser = optim.Adam([tensor], lr=0.001)\n",
    "train(my_neural_embedding, indices_text, optimiser, tensor, 1000)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1830.4455288006736\n",
      "loss: 1825.7223250474497\n",
      "loss: 1831.6231423994984\n",
      "loss: 1840.6099110637215\n",
      "loss: 1828.8560233262726\n",
      "loss: 1828.0295305374086\n",
      "loss: 1823.9521448857372\n",
      "loss: 1830.7351329123642\n",
      "loss: 1833.9944518333643\n",
      "loss: 1831.468934815202\n",
      "loss: 1830.9594043277352\n",
      "loss: 1822.335135979175\n",
      "loss: 1827.8411042808416\n",
      "loss: 1834.1505480005019\n",
      "loss: 1830.903310449589\n",
      "loss: 1822.0682625903241\n",
      "loss: 1835.2025116570057\n",
      "loss: 1832.2388047136005\n",
      "loss: 1831.249034010582\n",
      "loss: 1825.3996651233278\n",
      "loss: 1824.324886115856\n",
      "loss: 1833.494902428939\n",
      "loss: 1834.1609395989358\n",
      "loss: 1834.8425449005742\n",
      "loss: 1822.9290015418048\n",
      "loss: 1824.1007888497393\n",
      "loss: 1829.496936261189\n",
      "loss: 1826.4796549167515\n",
      "loss: 1817.4240194556237\n",
      "loss: 1822.5614933195473\n",
      "loss: 1831.8927845834066\n",
      "loss: 1827.862267149151\n",
      "loss: 1826.1668415237423\n",
      "loss: 1825.1322033037902\n",
      "loss: 1823.9398950516452\n",
      "loss: 1827.0091176687727\n",
      "loss: 1824.4828369127881\n",
      "loss: 1826.9676541093636\n",
      "loss: 1828.3612363264785\n",
      "loss: 1826.676635764904\n",
      "loss: 1819.9915849538172\n",
      "loss: 1827.715198187534\n",
      "loss: 1829.2960425884553\n",
      "loss: 1817.381541692446\n",
      "loss: 1825.323317569728\n",
      "loss: 1825.028981267929\n",
      "loss: 1824.1283956608527\n",
      "loss: 1823.2254867605354\n",
      "loss: 1822.872309971313\n",
      "loss: 1834.5906899515742\n",
      "loss: 1829.6453778169446\n",
      "loss: 1818.8737713642413\n",
      "loss: 1827.109699581653\n",
      "loss: 1821.3462342615724\n",
      "loss: 1821.687099387941\n",
      "loss: 1817.043599040509\n",
      "loss: 1820.5300166644763\n",
      "loss: 1823.413158892693\n",
      "loss: 1817.371978303912\n",
      "loss: 1825.183981615642\n",
      "loss: 1819.4579251575326\n",
      "loss: 1821.3740962357376\n",
      "loss: 1823.5118908100094\n",
      "loss: 1821.611894349571\n",
      "loss: 1824.2702541046986\n",
      "loss: 1821.2490796503314\n",
      "loss: 1825.0476433086233\n",
      "loss: 1820.7765568960783\n",
      "loss: 1830.8290502176458\n",
      "loss: 1825.0807460994367\n",
      "loss: 1819.8434952334412\n",
      "loss: 1822.203736804059\n",
      "loss: 1817.2361159177012\n",
      "loss: 1827.4326573421013\n",
      "loss: 1828.8918281767442\n",
      "loss: 1817.2477638685398\n",
      "loss: 1814.8643412106032\n",
      "loss: 1809.0736725693764\n",
      "loss: 1825.08314383027\n",
      "loss: 1822.1292108684377\n",
      "loss: 1818.8123088431344\n",
      "loss: 1824.2864252671402\n",
      "loss: 1823.2243436061724\n",
      "loss: 1822.1732855899618\n",
      "loss: 1819.060208596901\n",
      "loss: 1820.0599282676285\n",
      "loss: 1816.7891989747627\n",
      "loss: 1823.3221714430808\n",
      "loss: 1815.9404168956592\n",
      "loss: 1820.2683438645438\n",
      "loss: 1821.1842174780193\n",
      "loss: 1823.8516406992749\n",
      "loss: 1820.0088317732295\n",
      "loss: 1818.1232308423598\n",
      "loss: 1822.2270141602453\n",
      "loss: 1813.5862131713343\n",
      "loss: 1815.2251171891412\n",
      "loss: 1821.4479638740554\n",
      "loss: 1812.8527738775192\n",
      "loss: 1813.3584945843195\n",
      "loss: 1807.0219425052157\n",
      "loss: 1824.908452946151\n",
      "loss: 1816.707555682026\n",
      "loss: 1823.8390995252253\n",
      "loss: 1815.4213177223642\n",
      "loss: 1817.0876993188226\n",
      "loss: 1814.2822565622243\n",
      "loss: 1820.458874668475\n",
      "loss: 1814.918025694429\n",
      "loss: 1820.3847705257497\n",
      "loss: 1816.3226197936488\n",
      "loss: 1815.811931299186\n",
      "loss: 1814.6175385542103\n",
      "loss: 1810.6352401118195\n",
      "loss: 1822.4609343609782\n",
      "loss: 1816.3792972516958\n",
      "loss: 1816.6651509051294\n",
      "loss: 1810.7463664932686\n",
      "loss: 1811.1205609904184\n",
      "loss: 1823.2864717573202\n",
      "loss: 1804.5002821160015\n",
      "loss: 1812.6992808528607\n",
      "loss: 1801.427061462076\n",
      "loss: 1814.023129724983\n",
      "loss: 1813.8652049015407\n",
      "loss: 1808.9037300574564\n",
      "loss: 1812.3192443650023\n",
      "loss: 1818.5050222787752\n",
      "loss: 1807.9548413443238\n",
      "loss: 1816.2421117049316\n",
      "loss: 1808.1070088482234\n",
      "loss: 1816.249105682243\n",
      "loss: 1810.5554403313727\n",
      "loss: 1809.9668655213272\n",
      "loss: 1809.9226381915853\n",
      "loss: 1807.23276558941\n",
      "loss: 1816.9803414890678\n",
      "loss: 1811.051512048397\n",
      "loss: 1808.715423228837\n",
      "loss: 1815.4901299647834\n",
      "loss: 1810.724473066976\n",
      "loss: 1807.0910408164814\n",
      "loss: 1810.3406221109456\n",
      "loss: 1806.2102745434456\n",
      "loss: 1808.2833735614179\n",
      "loss: 1813.7811178964353\n",
      "loss: 1816.3154401953802\n",
      "loss: 1805.1575431663816\n",
      "loss: 1820.2234001684692\n",
      "loss: 1807.876675464696\n",
      "loss: 1816.444659339563\n",
      "loss: 1810.1036879891471\n",
      "loss: 1804.8480522357593\n",
      "loss: 1813.9005188490023\n",
      "loss: 1803.8822446068614\n",
      "loss: 1803.3585947825877\n",
      "loss: 1804.527704599554\n",
      "loss: 1800.73128122285\n",
      "loss: 1809.263546519846\n",
      "loss: 1819.9801200464228\n",
      "loss: 1808.8427826127954\n",
      "loss: 1799.283110792701\n",
      "loss: 1799.809424009897\n",
      "loss: 1817.9449463403027\n",
      "loss: 1808.7672890451904\n",
      "loss: 1809.4591296849821\n",
      "loss: 1798.954346482557\n",
      "loss: 1808.613983853641\n",
      "loss: 1813.0407779307811\n",
      "loss: 1806.650749521501\n",
      "loss: 1804.550556270556\n",
      "loss: 1810.2486749337788\n",
      "loss: 1810.9623307265858\n",
      "loss: 1813.4550725866538\n",
      "loss: 1808.9505404308566\n",
      "loss: 1807.8862446896044\n",
      "loss: 1808.0515726721183\n",
      "loss: 1803.9975762721147\n",
      "loss: 1795.5401235807776\n",
      "loss: 1806.8554301037834\n",
      "loss: 1811.9198392490655\n",
      "loss: 1800.6121267148335\n",
      "loss: 1816.7333176906866\n",
      "loss: 1802.951368347222\n",
      "loss: 1800.7081664901166\n",
      "loss: 1799.865293002368\n",
      "loss: 1805.7916624458067\n",
      "loss: 1808.8680881390885\n",
      "loss: 1798.6976194685928\n",
      "loss: 1803.310891993742\n",
      "loss: 1794.5273127756138\n",
      "loss: 1804.4118472217829\n",
      "loss: 1810.4672517378926\n",
      "loss: 1808.1370529705644\n",
      "loss: 1807.237812291162\n",
      "loss: 1799.188421315926\n",
      "loss: 1792.2650235229842\n",
      "loss: 1804.0933335362058\n",
      "loss: 1807.7697308622862\n",
      "loss: 1806.342382838373\n",
      "loss: 1806.7320698125818\n",
      "loss: 1796.3048127135464\n",
      "loss: 1807.7806148713996\n",
      "loss: 1795.100930960652\n",
      "loss: 1803.539310468671\n",
      "loss: 1803.6066370984822\n",
      "loss: 1806.1618507625767\n",
      "loss: 1805.2480765511223\n",
      "loss: 1795.2303027280075\n",
      "loss: 1800.1794628825394\n",
      "loss: 1804.5102229451963\n",
      "loss: 1794.4322318439793\n",
      "loss: 1805.858466530171\n",
      "loss: 1797.5214505874928\n",
      "loss: 1803.4894342526727\n",
      "loss: 1803.6252307744637\n",
      "loss: 1798.2976515524506\n",
      "loss: 1812.0973683038405\n",
      "loss: 1798.8262321747245\n",
      "loss: 1790.3978298370364\n",
      "loss: 1801.4249574129262\n",
      "loss: 1800.0555109008587\n",
      "loss: 1801.1021384354722\n",
      "loss: 1807.8724671971192\n",
      "loss: 1803.0262878667727\n",
      "loss: 1801.0297603419567\n",
      "loss: 1799.6870486859045\n",
      "loss: 1796.5345701736796\n",
      "loss: 1808.9020110038075\n",
      "loss: 1801.9558610033532\n",
      "loss: 1800.9974273324335\n",
      "loss: 1799.4067956678734\n",
      "loss: 1805.5991022908993\n",
      "loss: 1792.1672501992991\n",
      "loss: 1796.1281462005747\n",
      "loss: 1796.2098917897324\n",
      "loss: 1799.1365938308404\n",
      "loss: 1802.8034915869252\n",
      "loss: 1798.158043465079\n",
      "loss: 1798.0428895278121\n",
      "loss: 1805.0188112410344\n",
      "loss: 1802.9379629662217\n",
      "loss: 1807.4714289077508\n",
      "loss: 1804.5488325392882\n",
      "loss: 1795.596291454483\n",
      "loss: 1793.4759472404965\n",
      "loss: 1798.0725678093536\n",
      "loss: 1802.1477260332267\n",
      "loss: 1793.7438363876606\n",
      "loss: 1811.1815488822567\n",
      "loss: 1791.6613591969094\n",
      "loss: 1801.7338127569785\n",
      "loss: 1797.4634435998278\n",
      "loss: 1809.7796257366651\n",
      "loss: 1793.7606756632513\n",
      "loss: 1797.045926556938\n",
      "loss: 1800.6790447957806\n",
      "loss: 1791.5144407866192\n",
      "loss: 1795.6295906460532\n",
      "loss: 1799.0848999395769\n",
      "loss: 1795.7261602082683\n",
      "loss: 1797.0370421700936\n",
      "loss: 1792.8110141777067\n",
      "loss: 1795.1815785681051\n",
      "loss: 1796.800604896893\n",
      "loss: 1791.3806263365825\n",
      "loss: 1786.0747601837966\n",
      "loss: 1800.384480697688\n",
      "loss: 1787.1096766996995\n",
      "loss: 1800.8746293700856\n",
      "loss: 1786.5834931238448\n",
      "loss: 1791.2170630640435\n",
      "loss: 1793.3475723696515\n",
      "loss: 1789.1343088149683\n",
      "loss: 1799.0377495297644\n",
      "loss: 1788.053138706094\n",
      "loss: 1787.1705169981897\n",
      "loss: 1787.5900917385325\n",
      "loss: 1789.333076201709\n",
      "loss: 1790.8855894375313\n",
      "loss: 1795.4916120790604\n",
      "loss: 1790.1298181102081\n",
      "loss: 1792.2040410818774\n",
      "loss: 1794.41746982368\n",
      "loss: 1793.8912127455985\n",
      "loss: 1797.7134506769603\n",
      "loss: 1801.1569782894217\n",
      "loss: 1802.0179081712345\n",
      "loss: 1798.3819738180605\n",
      "loss: 1796.8476861895904\n",
      "loss: 1794.2782656234447\n",
      "loss: 1799.1870131033038\n",
      "loss: 1787.8211288014506\n",
      "loss: 1793.657887984007\n",
      "loss: 1796.7575883355707\n",
      "loss: 1798.1782966482135\n",
      "loss: 1790.3408603651783\n",
      "loss: 1801.4673772653578\n",
      "loss: 1790.207248526458\n",
      "loss: 1799.8985325559029\n",
      "loss: 1791.8480626928806\n",
      "loss: 1798.1424907277762\n",
      "loss: 1790.6978802275933\n",
      "loss: 1788.011380387119\n",
      "loss: 1785.942158567529\n",
      "loss: 1791.0161194529862\n",
      "loss: 1797.7992327836484\n",
      "loss: 1784.9960082617151\n",
      "loss: 1794.0880594977075\n",
      "loss: 1785.411859900958\n",
      "loss: 1790.387298724097\n",
      "loss: 1783.5681223568745\n",
      "loss: 1804.632600535147\n",
      "loss: 1794.352589054784\n",
      "loss: 1790.5327865054405\n",
      "loss: 1790.2716610702669\n",
      "loss: 1788.2336887329307\n",
      "loss: 1793.578583571821\n",
      "loss: 1792.9621378679965\n",
      "loss: 1787.513213771731\n",
      "loss: 1784.480965824413\n",
      "loss: 1792.4118008892306\n",
      "loss: 1785.4602678003894\n",
      "loss: 1771.948286642853\n",
      "loss: 1785.912728542338\n",
      "loss: 1788.928208122496\n",
      "loss: 1782.8060840061896\n",
      "loss: 1779.1531414381243\n",
      "loss: 1785.6442591837542\n",
      "loss: 1792.6184331233617\n",
      "loss: 1795.4602761010422\n",
      "loss: 1777.4446395958737\n",
      "loss: 1786.2516167757212\n",
      "loss: 1779.2634714670544\n",
      "loss: 1788.5832749746087\n",
      "loss: 1789.5911337117002\n",
      "loss: 1783.7913083955439\n",
      "loss: 1786.7462047481627\n",
      "loss: 1793.262699219331\n",
      "loss: 1782.0777179786546\n",
      "loss: 1790.0731623027923\n",
      "loss: 1791.766053306982\n",
      "loss: 1784.3517451742046\n",
      "loss: 1786.544920977423\n",
      "loss: 1780.7669415025553\n",
      "loss: 1777.6296619973527\n",
      "loss: 1780.9648304661875\n",
      "loss: 1790.682966682346\n",
      "loss: 1776.5251137272003\n",
      "loss: 1800.3690591501322\n",
      "loss: 1781.514982521968\n",
      "loss: 1790.3331894298776\n",
      "loss: 1785.5670390454763\n",
      "loss: 1781.8394178021556\n",
      "loss: 1790.736292520747\n",
      "loss: 1784.49464561265\n",
      "loss: 1770.2139450119746\n",
      "loss: 1778.5383406334056\n",
      "loss: 1787.693803010323\n",
      "loss: 1780.4079397346868\n",
      "loss: 1789.536071199574\n",
      "loss: 1784.374921118159\n",
      "loss: 1789.8488189569896\n",
      "loss: 1792.6413801592942\n",
      "loss: 1790.3305042122938\n",
      "loss: 1791.9496447837507\n",
      "loss: 1787.3121704480639\n",
      "loss: 1778.1029148072125\n",
      "loss: 1784.727812669924\n",
      "loss: 1777.355920680783\n",
      "loss: 1787.7944681276679\n",
      "loss: 1786.0095629294021\n",
      "loss: 1801.1256432599305\n",
      "loss: 1785.4947109758045\n",
      "loss: 1788.759921950142\n",
      "loss: 1780.790790607955\n",
      "loss: 1798.577686283987\n",
      "loss: 1782.4099048328399\n",
      "loss: 1774.4571075641725\n",
      "loss: 1791.4248800123034\n",
      "loss: 1783.7209123646994\n",
      "loss: 1775.2470858281768\n",
      "loss: 1789.3481365229716\n",
      "loss: 1780.6941187852638\n",
      "loss: 1791.236975638333\n",
      "loss: 1783.684037178543\n",
      "loss: 1781.3967497077497\n",
      "loss: 1795.1396113084788\n",
      "loss: 1772.8091100417803\n",
      "loss: 1779.8155673543622\n",
      "loss: 1792.7851845419855\n",
      "loss: 1785.7934418073207\n",
      "loss: 1796.9359961304854\n",
      "loss: 1791.1556248581999\n",
      "loss: 1785.420852639386\n",
      "loss: 1786.3380552431192\n",
      "loss: 1778.5891710613685\n",
      "loss: 1780.6421490224923\n",
      "loss: 1790.68612262856\n",
      "loss: 1785.417357990581\n",
      "loss: 1787.6239196400759\n",
      "loss: 1792.8129646033085\n",
      "loss: 1776.342816767233\n",
      "loss: 1775.9496274992666\n",
      "loss: 1787.2600600319624\n",
      "loss: 1783.7089178186473\n",
      "loss: 1767.7242781334844\n",
      "loss: 1786.8206058090666\n",
      "loss: 1782.0869317499448\n",
      "loss: 1779.6431916906058\n",
      "loss: 1781.7776380640862\n",
      "loss: 1782.8694444495702\n",
      "loss: 1786.499928111138\n",
      "loss: 1785.1600307591523\n",
      "loss: 1784.7346542133553\n",
      "loss: 1791.7194001230432\n",
      "loss: 1770.6208336332097\n",
      "loss: 1777.767131818361\n",
      "loss: 1787.4412753779325\n",
      "loss: 1782.2299692596955\n",
      "loss: 1772.6794267311925\n",
      "loss: 1769.240665449618\n",
      "loss: 1780.9518211326413\n",
      "loss: 1778.452551029891\n",
      "loss: 1781.1285002277832\n",
      "loss: 1782.154659544086\n",
      "loss: 1769.2046338572222\n",
      "loss: 1788.1047091797677\n",
      "loss: 1765.6212760829283\n",
      "loss: 1773.247563961884\n",
      "loss: 1780.074879734571\n",
      "loss: 1770.392102456463\n",
      "loss: 1772.2800759184263\n",
      "loss: 1781.0514159063569\n",
      "loss: 1779.0934092763798\n",
      "loss: 1788.9265989335856\n",
      "loss: 1792.435354992006\n",
      "loss: 1787.0069093644822\n",
      "loss: 1775.6612250075843\n",
      "loss: 1767.0788735734968\n",
      "loss: 1782.1677299690746\n",
      "loss: 1781.1287301809516\n",
      "loss: 1778.9131115053447\n",
      "loss: 1781.5823254841448\n",
      "loss: 1784.1903604210781\n",
      "loss: 1780.2623402944678\n",
      "loss: 1771.4353560120426\n",
      "loss: 1773.3724823622647\n",
      "loss: 1773.8671299097268\n",
      "loss: 1766.9414780073346\n",
      "loss: 1774.457210316238\n",
      "loss: 1777.0042034496619\n",
      "loss: 1767.3942998556552\n",
      "loss: 1768.581128727194\n",
      "loss: 1777.6915076779596\n",
      "loss: 1779.3925589998078\n",
      "loss: 1786.5140106823828\n",
      "loss: 1759.2445075694934\n",
      "loss: 1780.2862710142485\n",
      "loss: 1775.7156047516778\n",
      "loss: 1772.0430899301114\n",
      "loss: 1780.1468952403184\n",
      "loss: 1776.2580869004607\n",
      "loss: 1792.1346716642877\n",
      "loss: 1767.1615924192893\n",
      "loss: 1761.3846668968836\n",
      "loss: 1770.7175765880006\n",
      "loss: 1784.8681183858762\n",
      "loss: 1770.827632682899\n",
      "loss: 1769.186561307163\n",
      "loss: 1774.0215670435266\n",
      "loss: 1776.0136946925431\n",
      "loss: 1776.0925853350836\n",
      "loss: 1772.713589749499\n",
      "loss: 1780.4717378124867\n",
      "loss: 1777.2137168840807\n",
      "loss: 1777.7202634766577\n",
      "loss: 1764.7992862662072\n",
      "loss: 1760.7418164907294\n",
      "loss: 1776.1113400859972\n",
      "loss: 1781.4064468893037\n",
      "loss: 1761.0750590732353\n",
      "loss: 1771.1707868298588\n",
      "loss: 1761.5630335884512\n",
      "loss: 1770.4624698827956\n",
      "loss: 1766.9198099718183\n",
      "loss: 1765.628890313101\n",
      "loss: 1766.929559677167\n",
      "loss: 1796.920806705119\n",
      "loss: 1770.2904605227432\n",
      "loss: 1782.5706475350082\n",
      "loss: 1779.0754407642655\n",
      "loss: 1750.7544984955482\n",
      "loss: 1781.547718647305\n",
      "loss: 1771.655007886716\n",
      "loss: 1767.7933878039446\n",
      "loss: 1767.9513661427582\n",
      "loss: 1770.3666101907854\n",
      "loss: 1768.961690018783\n",
      "loss: 1782.5428416858726\n",
      "loss: 1782.3884393599824\n",
      "loss: 1783.248421068812\n",
      "loss: 1768.0841051106872\n",
      "loss: 1770.1869080860715\n",
      "loss: 1759.5046299265998\n",
      "loss: 1764.5769167333676\n",
      "loss: 1771.6251582397254\n",
      "loss: 1757.014393098115\n",
      "loss: 1765.922903651743\n",
      "loss: 1781.3175388955713\n",
      "loss: 1766.6888847769571\n",
      "loss: 1777.4505419384786\n",
      "loss: 1772.5703177307646\n",
      "loss: 1766.4923954465453\n",
      "loss: 1780.9344439669874\n",
      "loss: 1764.1011341198264\n",
      "loss: 1782.0748398341864\n",
      "loss: 1780.9594352926642\n",
      "loss: 1764.915927974411\n",
      "loss: 1772.3295611209276\n",
      "loss: 1778.8510280966027\n",
      "loss: 1771.3947693285331\n",
      "loss: 1765.5385850098266\n",
      "loss: 1778.207865695486\n",
      "loss: 1773.150160738682\n",
      "loss: 1760.744810935589\n",
      "loss: 1781.3725023353293\n",
      "loss: 1774.8702061356553\n",
      "loss: 1785.4416413555696\n",
      "loss: 1768.8320190283407\n",
      "loss: 1769.6705863799073\n",
      "loss: 1759.0539632962286\n",
      "loss: 1768.8617638300693\n",
      "loss: 1781.909623208937\n",
      "loss: 1765.7392355213083\n",
      "loss: 1775.454687600605\n",
      "loss: 1762.2758318464114\n",
      "loss: 1774.2081322977344\n",
      "loss: 1782.8334351152869\n",
      "loss: 1769.3151780710543\n",
      "loss: 1760.7195511000348\n",
      "loss: 1771.5672169191557\n",
      "loss: 1757.3677806918329\n",
      "loss: 1784.3530650787297\n",
      "loss: 1772.4027167192658\n",
      "loss: 1763.061583387111\n",
      "loss: 1770.663536106252\n",
      "loss: 1758.057933056602\n",
      "loss: 1744.6027455059798\n",
      "loss: 1766.72509149573\n",
      "loss: 1760.878336700509\n",
      "loss: 1757.1278473334773\n",
      "loss: 1770.1565171193708\n",
      "loss: 1770.616527568722\n",
      "loss: 1764.298648173635\n",
      "loss: 1760.2020163733655\n",
      "loss: 1771.714941377574\n",
      "loss: 1752.0918632968112\n",
      "loss: 1768.0096438698556\n",
      "loss: 1770.0842399569165\n",
      "loss: 1762.8569079227432\n",
      "loss: 1759.9505533641814\n",
      "loss: 1758.3576878370031\n",
      "loss: 1759.192019743707\n",
      "loss: 1755.6309981228917\n",
      "loss: 1757.050899977383\n",
      "loss: 1757.9691903741696\n",
      "loss: 1783.48142860784\n",
      "loss: 1771.052179430158\n",
      "loss: 1776.8533167887706\n",
      "loss: 1750.0435134531385\n",
      "loss: 1759.1925575140485\n",
      "loss: 1759.6273034820426\n",
      "loss: 1764.6189352903143\n",
      "loss: 1779.003651974484\n",
      "loss: 1752.3408561528008\n",
      "loss: 1763.3800507652102\n",
      "loss: 1749.559860200078\n",
      "loss: 1759.0357363398482\n",
      "loss: 1765.2739029555842\n",
      "loss: 1759.5682611650873\n",
      "loss: 1745.047130880638\n",
      "loss: 1773.9724910540867\n",
      "loss: 1745.851566505102\n",
      "loss: 1776.924415694769\n",
      "loss: 1764.7374800355105\n",
      "loss: 1764.6654201441438\n",
      "loss: 1754.2534206790103\n",
      "loss: 1759.3503819576993\n",
      "loss: 1763.7966018635457\n",
      "loss: 1770.9104191322695\n",
      "loss: 1751.6526299659895\n",
      "loss: 1767.3497933670735\n",
      "loss: 1769.0386601472574\n",
      "loss: 1768.5343473995451\n",
      "loss: 1767.7395725500992\n",
      "loss: 1755.4177251675742\n",
      "loss: 1767.058790193429\n",
      "loss: 1776.1354074769333\n",
      "loss: 1763.2741081583488\n",
      "loss: 1765.217024246119\n",
      "loss: 1757.5452015003948\n",
      "loss: 1746.6174175549738\n",
      "loss: 1762.984262092922\n",
      "loss: 1754.218319402416\n",
      "loss: 1762.5119698877788\n",
      "loss: 1764.3714762727873\n",
      "loss: 1758.9832634787879\n",
      "loss: 1766.9368718766982\n",
      "loss: 1760.7881578097451\n",
      "loss: 1759.8255022722594\n",
      "loss: 1760.7157664585036\n",
      "loss: 1758.5136582542889\n",
      "loss: 1763.69150542809\n",
      "loss: 1769.6179984454438\n",
      "loss: 1754.5485429689788\n",
      "loss: 1773.851205103655\n",
      "loss: 1737.982516228398\n",
      "loss: 1759.3694818870117\n",
      "loss: 1772.5001449409692\n",
      "loss: 1767.3079944836945\n",
      "loss: 1755.9940353852596\n",
      "loss: 1741.9688103527433\n",
      "loss: 1758.6498538930425\n",
      "loss: 1783.533074371848\n",
      "loss: 1758.886905852045\n",
      "loss: 1748.650070749933\n",
      "loss: 1756.3128365345137\n",
      "loss: 1738.2864016652177\n",
      "loss: 1781.7657326674594\n",
      "loss: 1746.4179298514068\n",
      "loss: 1774.3461647632837\n",
      "loss: 1769.0008400104252\n",
      "loss: 1759.3453367480993\n",
      "loss: 1754.6950228751607\n",
      "loss: 1767.4897013883344\n",
      "loss: 1769.1137487181657\n",
      "loss: 1752.2633080001358\n",
      "loss: 1767.8354781315443\n",
      "loss: 1758.3457206977528\n",
      "loss: 1754.809620797068\n",
      "loss: 1753.0682490558338\n",
      "loss: 1756.7170541621308\n",
      "loss: 1756.5834399495564\n",
      "loss: 1758.1526097032065\n",
      "loss: 1766.382143134123\n",
      "loss: 1760.5216737577364\n",
      "loss: 1770.6541983405577\n",
      "loss: 1766.1510400665177\n",
      "loss: 1760.2446992920245\n",
      "loss: 1740.1836625918056\n",
      "loss: 1761.18418830725\n",
      "loss: 1765.4459153642372\n",
      "loss: 1741.4776712922376\n",
      "loss: 1760.5748444677972\n",
      "loss: 1762.8655515207752\n",
      "loss: 1764.2363151410343\n",
      "loss: 1761.6185529375148\n",
      "loss: 1760.7712962421172\n",
      "loss: 1774.6880418008138\n",
      "loss: 1762.8677563664576\n",
      "loss: 1769.2781421324578\n",
      "loss: 1759.4670666349855\n",
      "loss: 1750.4006381208617\n",
      "loss: 1751.971448718513\n",
      "loss: 1770.7209364955695\n",
      "loss: 1743.3694912836836\n",
      "loss: 1753.672212607633\n",
      "loss: 1755.53960372574\n",
      "loss: 1759.6085094079215\n",
      "loss: 1754.3491402554007\n",
      "loss: 1768.1305525195855\n",
      "loss: 1753.2485663131258\n",
      "loss: 1762.4664498816926\n",
      "loss: 1760.9721447625943\n",
      "loss: 1762.6662089785584\n",
      "loss: 1757.3364910573341\n",
      "loss: 1763.966135875223\n",
      "loss: 1755.261913490984\n",
      "loss: 1755.9337057722928\n",
      "loss: 1757.791688668156\n",
      "loss: 1740.7777131636096\n",
      "loss: 1760.1296620617813\n",
      "loss: 1758.4869770828564\n",
      "loss: 1756.9040076244619\n",
      "loss: 1744.6856131420172\n",
      "loss: 1748.2515153228946\n",
      "loss: 1777.800844451888\n",
      "loss: 1760.8549402919618\n",
      "loss: 1742.8740816258428\n",
      "loss: 1746.4837315159268\n",
      "loss: 1761.369967697452\n",
      "loss: 1748.3586772260926\n",
      "loss: 1754.98230578714\n",
      "loss: 1766.5849049243836\n",
      "loss: 1768.9129159554434\n",
      "loss: 1758.8527009709358\n",
      "loss: 1755.6852695830767\n",
      "loss: 1759.981141074246\n",
      "loss: 1753.9787610491721\n",
      "loss: 1736.067615107678\n",
      "loss: 1760.152419482734\n",
      "loss: 1743.3158258218475\n",
      "loss: 1745.8181915887983\n",
      "loss: 1762.0407122347888\n",
      "loss: 1770.2050746555876\n",
      "loss: 1742.1200033961352\n",
      "loss: 1748.1485953969236\n",
      "loss: 1751.6512189320508\n",
      "loss: 1749.9471738107252\n",
      "loss: 1741.4635078784775\n",
      "loss: 1756.610368586841\n",
      "loss: 1749.2558949619215\n",
      "loss: 1774.9890392940692\n",
      "loss: 1768.9625230332929\n",
      "loss: 1769.9272633898763\n",
      "loss: 1766.3553987231505\n",
      "loss: 1747.0625047481274\n",
      "loss: 1754.7442772216705\n",
      "loss: 1740.5921086521748\n",
      "loss: 1749.9270745894196\n",
      "loss: 1742.0237057632473\n",
      "loss: 1736.6146956906243\n",
      "loss: 1748.6894880588666\n",
      "loss: 1766.933895790333\n",
      "loss: 1768.3215142044094\n",
      "loss: 1748.408958803217\n",
      "loss: 1756.537468809111\n",
      "loss: 1743.1050704526353\n",
      "loss: 1766.2955714291452\n",
      "loss: 1754.0785959717962\n",
      "loss: 1768.4150043592529\n",
      "loss: 1751.832120674507\n",
      "loss: 1762.7977972499746\n",
      "loss: 1744.9174118588987\n",
      "loss: 1752.555441046\n",
      "loss: 1743.0161016562597\n",
      "loss: 1740.851173076983\n",
      "loss: 1748.9359439637537\n",
      "loss: 1754.0817905128933\n",
      "loss: 1743.1040226466469\n",
      "loss: 1758.0074697790626\n",
      "loss: 1759.4930926603254\n",
      "loss: 1748.9774627617687\n",
      "loss: 1734.9908841597583\n",
      "loss: 1753.5537110094297\n",
      "loss: 1745.2346829570083\n",
      "loss: 1744.3132654686463\n",
      "loss: 1757.3343799880756\n",
      "loss: 1741.8210592789871\n",
      "loss: 1761.3443500136066\n",
      "loss: 1764.1858590782706\n",
      "loss: 1744.2495175940949\n",
      "loss: 1750.1347011186538\n",
      "loss: 1760.9276409972135\n",
      "loss: 1760.3227151692934\n",
      "loss: 1764.9358926086384\n",
      "loss: 1754.4727908336959\n",
      "loss: 1759.0730563068835\n",
      "loss: 1739.1539431486067\n",
      "loss: 1735.882428698249\n",
      "loss: 1742.33250131389\n",
      "loss: 1746.3203762739017\n",
      "loss: 1762.4496463516357\n",
      "loss: 1759.8410597788456\n",
      "loss: 1733.9180008851993\n",
      "loss: 1741.5890053569528\n",
      "loss: 1743.0047597567236\n",
      "loss: 1756.4022344657474\n",
      "loss: 1763.2274527136308\n",
      "loss: 1758.2548061374891\n",
      "loss: 1775.1859002608933\n",
      "loss: 1752.175670136603\n",
      "loss: 1761.4343109564516\n",
      "loss: 1736.71044006424\n",
      "loss: 1762.2799116767544\n",
      "loss: 1745.6780841472894\n",
      "loss: 1725.2294533380095\n",
      "loss: 1749.9608088404661\n",
      "loss: 1750.1612261895484\n",
      "loss: 1742.5545528658747\n",
      "loss: 1749.7177573055246\n",
      "loss: 1757.91811847113\n",
      "loss: 1750.2915461322264\n",
      "loss: 1756.1070726135101\n",
      "loss: 1735.5805259662452\n",
      "loss: 1754.1636847365685\n",
      "loss: 1756.6057988566242\n",
      "loss: 1750.0559843191268\n",
      "loss: 1751.8692629668308\n",
      "loss: 1751.1633664772219\n",
      "loss: 1727.3055549754329\n",
      "loss: 1742.5744384725608\n",
      "loss: 1758.082573226629\n",
      "loss: 1750.750383559031\n",
      "loss: 1752.4713355705758\n",
      "loss: 1763.9795088863023\n",
      "loss: 1765.509759724359\n",
      "loss: 1732.314275738874\n",
      "loss: 1756.849886344062\n",
      "loss: 1751.9314809262373\n",
      "loss: 1761.560049646718\n",
      "loss: 1759.7075162656965\n",
      "loss: 1751.2460337138964\n",
      "loss: 1752.3436927183816\n",
      "loss: 1758.5047875116516\n",
      "loss: 1763.4807266491796\n",
      "loss: 1744.8205163279945\n",
      "loss: 1742.170453305845\n",
      "loss: 1749.4936859730296\n",
      "loss: 1743.64746148053\n",
      "loss: 1754.5069930172062\n",
      "loss: 1751.5959201955395\n",
      "loss: 1740.4467181976725\n",
      "loss: 1748.4952228890081\n",
      "loss: 1750.4855599694931\n",
      "loss: 1750.374713229787\n",
      "loss: 1762.9689990246138\n",
      "loss: 1743.9234566096507\n",
      "loss: 1760.5786900170197\n",
      "loss: 1752.1570528972995\n",
      "loss: 1734.0289367099044\n",
      "loss: 1743.6029394141312\n",
      "loss: 1756.1625330039722\n",
      "loss: 1732.4368439248906\n",
      "loss: 1738.8040718424143\n",
      "loss: 1722.9798885270125\n",
      "loss: 1758.0411943019922\n",
      "loss: 1751.1133572858191\n",
      "loss: 1736.7625801558784\n",
      "loss: 1750.1576822958039\n",
      "loss: 1768.8900222111076\n",
      "loss: 1729.763074600547\n",
      "loss: 1749.7004026268526\n",
      "loss: 1760.2010568646842\n",
      "loss: 1747.5646709503421\n",
      "loss: 1726.9803802555232\n",
      "loss: 1744.2129598587155\n",
      "loss: 1759.8802714045692\n",
      "loss: 1744.0975238017318\n",
      "loss: 1740.9279332018953\n",
      "loss: 1736.873079134909\n",
      "loss: 1741.054972616325\n",
      "loss: 1752.0225485830476\n",
      "loss: 1768.8330141034278\n",
      "loss: 1747.743944532322\n",
      "loss: 1741.5121821726812\n",
      "loss: 1768.799932756029\n",
      "loss: 1725.0681075128937\n",
      "loss: 1728.9296426709775\n",
      "loss: 1760.8928673382939\n",
      "loss: 1756.6650080360937\n",
      "loss: 1728.804324347056\n",
      "loss: 1768.7743796546836\n",
      "loss: 1735.6240185117858\n",
      "loss: 1755.5982892883144\n",
      "loss: 1773.560586148223\n",
      "loss: 1758.5472117191641\n",
      "loss: 1750.916935450583\n",
      "loss: 1753.674078779923\n",
      "loss: 1754.8847511680722\n",
      "loss: 1742.7334427647047\n",
      "loss: 1769.9335529661328\n",
      "loss: 1735.2900863391178\n",
      "loss: 1741.8322988272814\n",
      "loss: 1737.873514319548\n",
      "loss: 1744.3063221936604\n",
      "loss: 1739.9505084249313\n",
      "loss: 1744.8292809793516\n",
      "loss: 1749.6720430921741\n",
      "loss: 1748.029938195591\n",
      "loss: 1721.6798242001444\n",
      "loss: 1734.0951106931227\n",
      "loss: 1746.2703612437545\n",
      "loss: 1742.0651146612981\n",
      "loss: 1749.091879687319\n",
      "loss: 1731.7536841543908\n",
      "loss: 1733.9447656087768\n",
      "loss: 1740.2430048998226\n",
      "loss: 1759.2553654966491\n",
      "loss: 1734.159295461829\n",
      "loss: 1748.2896606075183\n",
      "loss: 1743.6609965433959\n",
      "loss: 1737.027505287216\n",
      "loss: 1733.6743270327752\n",
      "loss: 1730.8121022629218\n",
      "loss: 1726.0166409107035\n",
      "loss: 1742.083211200966\n",
      "loss: 1731.0201451987143\n",
      "loss: 1766.4576407417558\n",
      "loss: 1747.9342846130314\n",
      "loss: 1750.8986410362995\n",
      "loss: 1758.5603081019187\n",
      "loss: 1757.1850484726456\n",
      "loss: 1777.6833913411033\n",
      "loss: 1754.020206829153\n",
      "loss: 1754.3876811430187\n",
      "loss: 1752.0964449351966\n",
      "loss: 1730.806726430292\n",
      "loss: 1728.4367296793248\n",
      "loss: 1708.0935730527335\n",
      "loss: 1733.708642040983\n",
      "loss: 1726.4729217256988\n",
      "loss: 1756.641432588732\n",
      "loss: 1735.5147231423612\n",
      "loss: 1743.1644198366098\n",
      "loss: 1733.0593259752877\n",
      "loss: 1745.3098549284095\n",
      "loss: 1748.8819506521131\n",
      "loss: 1744.2859127531417\n",
      "loss: 1752.7685602883032\n",
      "loss: 1724.3009446679193\n",
      "loss: 1745.0647459326806\n",
      "loss: 1729.2479689910108\n",
      "loss: 1723.482712887946\n",
      "loss: 1738.6478213327857\n",
      "loss: 1741.0024043486524\n",
      "loss: 1738.4370587035194\n",
      "loss: 1729.7124657686168\n",
      "loss: 1747.0401710700637\n",
      "loss: 1738.4200063893045\n",
      "loss: 1724.1881315440078\n",
      "loss: 1742.0057023814477\n",
      "loss: 1755.8372866877717\n",
      "loss: 1734.9457156162025\n",
      "loss: 1741.2005748214297\n",
      "loss: 1746.5383105519236\n",
      "loss: 1739.9387091814456\n",
      "loss: 1728.6446728655396\n",
      "loss: 1746.917411189166\n",
      "loss: 1754.69623106651\n",
      "loss: 1737.7889552350846\n",
      "loss: 1750.2742555414134\n",
      "loss: 1737.3106472764161\n",
      "loss: 1712.8907826657646\n",
      "loss: 1747.0291905160143\n",
      "loss: 1740.748820912348\n",
      "loss: 1737.0507492455806\n",
      "loss: 1727.826581241304\n",
      "loss: 1754.6896791930317\n",
      "loss: 1746.9911146925656\n",
      "loss: 1722.4816117261594\n",
      "loss: 1748.2542890640966\n",
      "loss: 1736.6074331118027\n",
      "loss: 1747.4288526367166\n",
      "loss: 1766.6387327873592\n",
      "loss: 1723.549696825706\n",
      "loss: 1754.391313915819\n",
      "loss: 1733.6864115533704\n",
      "loss: 1734.9873301019065\n",
      "loss: 1735.9456388878514\n",
      "loss: 1759.733459759946\n",
      "loss: 1745.3986849268101\n",
      "loss: 1722.5094310197865\n",
      "loss: 1745.0264324953976\n",
      "loss: 1736.6085016475567\n",
      "loss: 1743.206653063692\n",
      "loss: 1746.9379407229562\n",
      "loss: 1723.3465620724007\n",
      "loss: 1739.1883442246226\n",
      "loss: 1748.0593617996192\n",
      "loss: 1750.6593410225528\n",
      "loss: 1736.5790120957117\n",
      "loss: 1737.3273172176262\n",
      "loss: 1719.4127226342976\n",
      "loss: 1736.2625905066432\n",
      "loss: 1735.4911479734562\n",
      "loss: 1743.975590603633\n",
      "loss: 1720.6475580383237\n",
      "loss: 1740.8342445565822\n",
      "loss: 1729.5238766747975\n",
      "loss: 1741.2817074174932\n",
      "loss: 1741.1991830041136\n",
      "loss: 1741.99881802094\n",
      "loss: 1737.8650557970075\n",
      "loss: 1749.5618231532833\n",
      "loss: 1744.9373858480653\n",
      "loss: 1735.131651179267\n",
      "loss: 1748.2561895306885\n",
      "loss: 1748.2553322036997\n",
      "loss: 1756.1210416191338\n",
      "loss: 1736.4733808064257\n",
      "loss: 1720.4773314429974\n",
      "loss: 1742.6823631698032\n",
      "loss: 1727.5745145002033\n",
      "loss: 1751.5846515016674\n",
      "loss: 1747.554786369057\n",
      "loss: 1719.9526310177146\n",
      "loss: 1727.7774365321986\n",
      "loss: 1749.4575728085415\n",
      "loss: 1737.0574597657915\n",
      "loss: 1729.928063249122\n",
      "loss: 1716.9708015055876\n",
      "loss: 1727.7582758496376\n",
      "loss: 1740.1552263760543\n",
      "loss: 1747.8957903385804\n",
      "loss: 1716.6631293879827\n",
      "loss: 1728.1633308533453\n",
      "loss: 1744.1220481415512\n",
      "loss: 1767.8043306920035\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

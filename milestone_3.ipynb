{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary imports\n",
    "import numpy as np\n",
    "from numpy_ml.neural_nets.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing \n",
    "    # load encoded train/validation/test sets\n",
    "    # chunken und batchen\n",
    "\n",
    "batch_size = 1\n",
    "chunk_size = 1\n",
    "\n",
    "def get_batch(input, batch_size):\n",
    "    return np.array([[1,2,3], [3,4,7]]), np.array([[1,2,3], [3,4,7]]) # pls adapt. obv. :))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_embedding:\n",
    "    def __init__(self, vocab_size):\n",
    "        # they call nn.Embedding, need to look up what that is, exactly\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding_table = np.random.rand(vocab_size, vocab_size)\n",
    "\n",
    "    def calculate_softmax(self, x):\n",
    "        \"\"\"Takes input array x and returns softmax.\"\"\"\n",
    "        soft_x = np.exp(x - np.max(x))\n",
    "        softer_x = soft_x / np.sum(soft_x)\n",
    "        return softer_x\n",
    "    \n",
    "\n",
    "    def calculate_cross_entropy(self, y_hatless, y_hat):\n",
    "        \"\"\"\n",
    "        Takes target (y_hatless) and prediction (y_hat) and computes cross entropy loss.\n",
    "        \"\"\"\n",
    "        # get vocab_size\n",
    "        _, _, vocab_size = y_hat.shape        \n",
    "        y_hat = y_hat.reshape(y_hat.shape[0]*y_hat.shape[1], y_hat.shape[2])\n",
    "        y_hatless_flat = y_hatless.reshape(-1)\n",
    "        # one-hot encode targets\n",
    "        y_hatless_hot = np.eye(vocab_size)[y_hatless_flat]\n",
    "       \n",
    "        y_hat = self.calculate_softmax(y_hat)\n",
    "    \n",
    "        return -np.sum(y_hatless_hot*np.log(y_hat))\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Implements forward pass with an unnecessary logitte function \n",
    "        which i only did not delete because now I'm emotionally attached.\n",
    "        Args:\n",
    "            idx(np.array): (B,T) numpy array of integers\n",
    "            targets(np.array): (B,T) numpy array of integers\n",
    "        Returns:\n",
    "            input_logits(np.array)\n",
    "            sometimes also: targets(np.array)\n",
    "        \"\"\"\n",
    "        batch_size, chunk_size = idx.shape\n",
    "        logits = np.zeros((batch_size, chunk_size, (self.token_embedding_table[0].size)))\n",
    "\n",
    "        def logitte(batch_size, chunk_size, input):\n",
    "            for batch in range(batch_size):\n",
    "                for chunk in range(chunk_size):\n",
    "                    # (B,T,C) b=batch_size, t=\"time\"=chunk_size, c=vocab_size\n",
    "                    logits[batch][chunk] = self.token_embedding_table[input[batch][chunk]]\n",
    "                    \n",
    "            return logits\n",
    "\n",
    "        input_logits = logitte(batch_size, chunk_size, idx)\n",
    "        \n",
    "        if targets is not None:\n",
    "            loss = self.calculate_cross_entropy(targets, input_logits)\n",
    "\n",
    "            return input_logits, loss\n",
    "\n",
    "        return input_logits\n",
    "\n",
    "    \n",
    "    def backward(self, targets, input_logits):\n",
    "        # need to do the same reshaping as we did for cross entropy, apaprently\n",
    "        targets_flat = targets.reshape(-1)\n",
    "        one_hot_targets = np.eye(self.vocab_size)[targets_flat]\n",
    "\n",
    "        # shape after: ((batch_size*chunk_size), vocab_size)\n",
    "        input_logits_2d = input_logits.reshape(input_logits.shape[0]*input_logits.shape[1], input_logits.shape[2])\n",
    "\n",
    "        # somehow this is supposedly the combiantion of the derivative of softmax with the derivative of the CCE\n",
    "        delta = one_hot_targets - input_logits_2d\n",
    "        # compute gradient for weight matrix: dot product between the transpose of the to layer and delta vector computed above\n",
    "        # conversations have resulted in might only need to multiply this with the part of token embedding table \n",
    "        # part that is actually used should the have size (6,80) so np.dot((80,6),(6,80)) should then work?\n",
    "        # or maybe i'm just supposed to add them? We'll see tomorrow.\n",
    "        gradient = np.dot(delta.transpose(),self.token_embedding_table)\n",
    "        # would then need to fill up the rest with gradients of 0 so that I get 80,80 in the end?\n",
    "    \n",
    "        return gradient\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently do not use loss at all, so something is probably very wrong\n",
    "\n",
    "def train(model, optimiser):\n",
    "    batch_size=32\n",
    "    for steps in range(2): # TODO: please increase\n",
    "        # sample batch of data\n",
    "        xb, yb = get_batch('train', batch_size) # TODO: pls adapt to above fct\n",
    "\n",
    "        # get logits and loss\n",
    "        logits, loss = model.forward(xb, yb)\n",
    "        print(f\"loss: {loss}\")\n",
    "\n",
    "        gradient = model.backward(yb, logits)\n",
    "        optimiser.update(param=model.token_embedding_table, param_grad=gradient, param_name=\"gradient\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 36.03990560393165\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (80,6) and (80,80) not aligned: 6 (dim 1) != 80 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m my_neural_embedding \u001b[38;5;241m=\u001b[39m neural_embedding(vocab_size)\n\u001b[0;32m      4\u001b[0m optimiser \u001b[38;5;241m=\u001b[39m Adam(my_neural_embedding\u001b[38;5;241m.\u001b[39mtoken_embedding_table)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_neural_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimiser)\u001b[0m\n\u001b[0;32m     10\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(xb, yb)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m gradient \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43myb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mupdate(param\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtoken_embedding_table, param_grad\u001b[38;5;241m=\u001b[39mgradient, param_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[30], line 71\u001b[0m, in \u001b[0;36mneural_embedding.backward\u001b[1;34m(self, targets, input_logits)\u001b[0m\n\u001b[0;32m     68\u001b[0m delta \u001b[38;5;241m=\u001b[39m one_hot_targets \u001b[38;5;241m-\u001b[39m input_logits_2d\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# compute gradient for weight matrix: dot product between the transpose of the to layer and delta vector computed above\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# transposed so that it fit, might not be right way to do it and indicative of larger issue\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m gradient \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gradient\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (80,6) and (80,80) not aligned: 6 (dim 1) != 80 (dim 0)"
     ]
    }
   ],
   "source": [
    "vocab_size = 80\n",
    "\n",
    "my_neural_embedding = neural_embedding(vocab_size)\n",
    "optimiser = Adam(my_neural_embedding.token_embedding_table)\n",
    "train(my_neural_embedding, optimiser)\n",
    "#uglies_array_youve_ever_seeeeeen = np.array([[1,2,3], [3,4,7]])\n",
    "#my_input_logits, my_loss = my_neural_embedding.forward(uglies_array_youve_ever_seeeeeen, uglies_array_youve_ever_seeeeeen)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing \n",
    "    # load encoded train/validation/test sets\n",
    "    # chunken und batchen\n",
    "\n",
    "batch_size = 1\n",
    "chunk_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "# token embedding table, outputs: logits\n",
    "class neural_embedding:\n",
    "    def __init__(self, vocab_size):\n",
    "        # they call nn.Embedding, need to look up what that is, exactly\n",
    "        self.token_embedding_table = np.random.rand(vocab_size, vocab_size)\n",
    "\n",
    "    def calculate_softmax(x):\n",
    "        \n",
    "\n",
    "    def calculate_cross_entropy(y_hatless, y_hat):\n",
    "        return -np.sum(target*np.log)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets both (B,T) tensor of integers\n",
    "        batch_size, chunk_size = idx.shape\n",
    "        print(f\"Shape: {idx.shape}, batches: {batch_size}, chunks: {chunk_size}\")\n",
    "        print(f\"Vocab size, hopefully: {self.token_embedding_table[0].size}\")\n",
    "        logits = np.zeros((batch_size, chunk_size, (self.token_embedding_table[0].size)))\n",
    "        \n",
    "\n",
    "        def logitte(batch_size, chunk_size, input):\n",
    "            for batch in range(batch_size):\n",
    "                for chunk in range(chunk_size):\n",
    "                    logits[batch][chunk] = self.token_embedding_table[input[batch][chunk]]\n",
    "                    \n",
    "                    #logits = self.token_embedding_table(idx) # (B,T,C) b=batch_size, t=\"time\"=chunk_size, c=vocab_size\n",
    "            print(f\"Shape: {logits.shape}, \\n logits: {logits}\")\n",
    "            return logits\n",
    "\n",
    "        input_logits = logitte(batch_size, chunk_size, idx)\n",
    "        \n",
    "        if targets:\n",
    "            target_logits = logitte(batch_size, chunk_size, targets)\n",
    "            loss = calculate_cross_entropy()\n",
    "\n",
    "            return input_logits, loss\n",
    "            \n",
    "\n",
    "        return input_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2, 3), batches: 2, chunks: 3\n",
      "Vocab size, hopefully: 80\n",
      "Shape: (2, 3, 80), \n",
      " logits: [[[0.73201479 0.27030392 0.70973625 0.8197704  0.49323783 0.70820592\n",
      "   0.70160224 0.23461736 0.1546716  0.1411906  0.86090586 0.60720078\n",
      "   0.68506617 0.02877644 0.61030857 0.47854969 0.90801611 0.70123056\n",
      "   0.85905851 0.77717179 0.40035914 0.31185009 0.07976589 0.52401908\n",
      "   0.15742186 0.06471174 0.40236957 0.39744331 0.24373007 0.41771146\n",
      "   0.73077545 0.29062708 0.85914515 0.35683093 0.78658311 0.38198216\n",
      "   0.34613823 0.56917591 0.17282977 0.23941929 0.26234193 0.39351056\n",
      "   0.55175297 0.87191324 0.66894636 0.63985496 0.66368258 0.15972155\n",
      "   0.05346324 0.22215219 0.36270614 0.02455648 0.15011015 0.26961892\n",
      "   0.79539584 0.45567954 0.91028726 0.8064391  0.71077523 0.06678244\n",
      "   0.8087665  0.85420102 0.63255339 0.28759491 0.89662583 0.03562966\n",
      "   0.28124392 0.50750589 0.94335716 0.7818028  0.94849249 0.68535708\n",
      "   0.03858723 0.1957973  0.10355168 0.44363704 0.4512732  0.20961761\n",
      "   0.16374579 0.70512073]\n",
      "  [0.41267181 0.72021233 0.52340326 0.54528458 0.89357959 0.6831087\n",
      "   0.3471458  0.50501776 0.75946018 0.28327786 0.48902836 0.11692688\n",
      "   0.24843551 0.4116074  0.59001266 0.22524609 0.64912059 0.45867194\n",
      "   0.19848255 0.27124077 0.11692768 0.20851647 0.96427388 0.59168135\n",
      "   0.50470048 0.3648748  0.73042861 0.14973898 0.80144584 0.81808174\n",
      "   0.15533208 0.59121549 0.04694291 0.77539941 0.22161902 0.71873262\n",
      "   0.93717127 0.80266333 0.53889631 0.91513284 0.03375376 0.99193608\n",
      "   0.50913942 0.55946365 0.24076581 0.21551532 0.74108549 0.26468084\n",
      "   0.36114337 0.69506153 0.47347415 0.79223538 0.89019116 0.97964006\n",
      "   0.72966268 0.90505689 0.26728155 0.53923531 0.81269583 0.6750384\n",
      "   0.12070909 0.09238885 0.98874675 0.99079004 0.18861248 0.43461326\n",
      "   0.77575118 0.39369746 0.49200537 0.95314512 0.38335823 0.91576601\n",
      "   0.84340855 0.3427902  0.02110104 0.63233661 0.16893493 0.13872824\n",
      "   0.23529717 0.3929642 ]\n",
      "  [0.61443185 0.44412397 0.78526971 0.88092682 0.59201113 0.67317924\n",
      "   0.4794113  0.11156101 0.23597484 0.01917745 0.09926326 0.48013114\n",
      "   0.46916906 0.33381891 0.91642647 0.14793231 0.19274688 0.58671524\n",
      "   0.59791947 0.91973859 0.3787072  0.51636115 0.50807314 0.99457489\n",
      "   0.54998982 0.15021048 0.08509601 0.64622112 0.05740697 0.37889174\n",
      "   0.70972437 0.04389016 0.94770728 0.82659799 0.86289468 0.93563403\n",
      "   0.75520129 0.79321518 0.79417328 0.62313985 0.67328798 0.70657984\n",
      "   0.85474264 0.60969828 0.82927011 0.44401081 0.7164998  0.26559625\n",
      "   0.93903625 0.17445672 0.79578069 0.60432766 0.65563733 0.76822582\n",
      "   0.9952421  0.0539835  0.42472911 0.59380977 0.27028717 0.64877958\n",
      "   0.88687037 0.37866935 0.50198554 0.3762647  0.76456    0.66977564\n",
      "   0.35418405 0.56585014 0.27208801 0.57979319 0.64845589 0.82731186\n",
      "   0.11054046 0.1712858  0.0301999  0.69674071 0.68907809 0.38678755\n",
      "   0.60444693 0.26314475]]\n",
      "\n",
      " [[0.61443185 0.44412397 0.78526971 0.88092682 0.59201113 0.67317924\n",
      "   0.4794113  0.11156101 0.23597484 0.01917745 0.09926326 0.48013114\n",
      "   0.46916906 0.33381891 0.91642647 0.14793231 0.19274688 0.58671524\n",
      "   0.59791947 0.91973859 0.3787072  0.51636115 0.50807314 0.99457489\n",
      "   0.54998982 0.15021048 0.08509601 0.64622112 0.05740697 0.37889174\n",
      "   0.70972437 0.04389016 0.94770728 0.82659799 0.86289468 0.93563403\n",
      "   0.75520129 0.79321518 0.79417328 0.62313985 0.67328798 0.70657984\n",
      "   0.85474264 0.60969828 0.82927011 0.44401081 0.7164998  0.26559625\n",
      "   0.93903625 0.17445672 0.79578069 0.60432766 0.65563733 0.76822582\n",
      "   0.9952421  0.0539835  0.42472911 0.59380977 0.27028717 0.64877958\n",
      "   0.88687037 0.37866935 0.50198554 0.3762647  0.76456    0.66977564\n",
      "   0.35418405 0.56585014 0.27208801 0.57979319 0.64845589 0.82731186\n",
      "   0.11054046 0.1712858  0.0301999  0.69674071 0.68907809 0.38678755\n",
      "   0.60444693 0.26314475]\n",
      "  [0.23401542 0.42228472 0.50531825 0.20279606 0.91995903 0.30634672\n",
      "   0.13007429 0.35477087 0.92853568 0.58362545 0.8104809  0.22018967\n",
      "   0.09817607 0.1615762  0.42439855 0.33351668 0.01070409 0.54940112\n",
      "   0.35227026 0.24254914 0.52740838 0.33677253 0.03791711 0.26449697\n",
      "   0.42199834 0.17486374 0.91344914 0.02789081 0.17450939 0.47205213\n",
      "   0.83930921 0.83335801 0.93705023 0.68273353 0.94968175 0.77043128\n",
      "   0.70228052 0.64085037 0.01431701 0.12356787 0.20890835 0.45276279\n",
      "   0.89631949 0.98689522 0.8829223  0.47160068 0.18647887 0.52251373\n",
      "   0.77082322 0.03786571 0.59163723 0.228689   0.36981411 0.54970785\n",
      "   0.1014722  0.06935169 0.19940019 0.42960072 0.59440889 0.03776726\n",
      "   0.83108463 0.68862039 0.40916244 0.17667897 0.75068394 0.18240149\n",
      "   0.85089597 0.02562843 0.71446713 0.2953268  0.49767685 0.56521435\n",
      "   0.53442202 0.54912732 0.0220368  0.2515639  0.52124359 0.75125315\n",
      "   0.9110543  0.59122115]\n",
      "  [0.97059938 0.1327552  0.62867616 0.01706857 0.32675411 0.26267652\n",
      "   0.92978491 0.00992601 0.29642302 0.71136528 0.09837593 0.15380882\n",
      "   0.67414041 0.33524869 0.65364439 0.41695333 0.03227059 0.24286589\n",
      "   0.9259109  0.56632081 0.87028135 0.97963949 0.70948917 0.30234046\n",
      "   0.74561555 0.29757493 0.91627068 0.4410485  0.14070611 0.07719135\n",
      "   0.83763449 0.30424557 0.42040079 0.33083098 0.91438861 0.59751195\n",
      "   0.60015829 0.79527315 0.90614369 0.45376402 0.04864328 0.09522023\n",
      "   0.66523086 0.44772576 0.10146404 0.26235095 0.81850172 0.4142242\n",
      "   0.1477107  0.09222143 0.63756343 0.38376604 0.53197884 0.87579038\n",
      "   0.3659653  0.30704461 0.8212839  0.50985272 0.12377823 0.61401564\n",
      "   0.59280495 0.72694394 0.45545196 0.15971853 0.54394807 0.92849072\n",
      "   0.9076696  0.03349698 0.16478941 0.42160836 0.58587108 0.34884393\n",
      "   0.27817103 0.09133674 0.71147111 0.21840769 0.55738317 0.17578473\n",
      "   0.69028396 0.97723154]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.73201479, 0.27030392, 0.70973625, 0.8197704 , 0.49323783,\n",
       "         0.70820592, 0.70160224, 0.23461736, 0.1546716 , 0.1411906 ,\n",
       "         0.86090586, 0.60720078, 0.68506617, 0.02877644, 0.61030857,\n",
       "         0.47854969, 0.90801611, 0.70123056, 0.85905851, 0.77717179,\n",
       "         0.40035914, 0.31185009, 0.07976589, 0.52401908, 0.15742186,\n",
       "         0.06471174, 0.40236957, 0.39744331, 0.24373007, 0.41771146,\n",
       "         0.73077545, 0.29062708, 0.85914515, 0.35683093, 0.78658311,\n",
       "         0.38198216, 0.34613823, 0.56917591, 0.17282977, 0.23941929,\n",
       "         0.26234193, 0.39351056, 0.55175297, 0.87191324, 0.66894636,\n",
       "         0.63985496, 0.66368258, 0.15972155, 0.05346324, 0.22215219,\n",
       "         0.36270614, 0.02455648, 0.15011015, 0.26961892, 0.79539584,\n",
       "         0.45567954, 0.91028726, 0.8064391 , 0.71077523, 0.06678244,\n",
       "         0.8087665 , 0.85420102, 0.63255339, 0.28759491, 0.89662583,\n",
       "         0.03562966, 0.28124392, 0.50750589, 0.94335716, 0.7818028 ,\n",
       "         0.94849249, 0.68535708, 0.03858723, 0.1957973 , 0.10355168,\n",
       "         0.44363704, 0.4512732 , 0.20961761, 0.16374579, 0.70512073],\n",
       "        [0.41267181, 0.72021233, 0.52340326, 0.54528458, 0.89357959,\n",
       "         0.6831087 , 0.3471458 , 0.50501776, 0.75946018, 0.28327786,\n",
       "         0.48902836, 0.11692688, 0.24843551, 0.4116074 , 0.59001266,\n",
       "         0.22524609, 0.64912059, 0.45867194, 0.19848255, 0.27124077,\n",
       "         0.11692768, 0.20851647, 0.96427388, 0.59168135, 0.50470048,\n",
       "         0.3648748 , 0.73042861, 0.14973898, 0.80144584, 0.81808174,\n",
       "         0.15533208, 0.59121549, 0.04694291, 0.77539941, 0.22161902,\n",
       "         0.71873262, 0.93717127, 0.80266333, 0.53889631, 0.91513284,\n",
       "         0.03375376, 0.99193608, 0.50913942, 0.55946365, 0.24076581,\n",
       "         0.21551532, 0.74108549, 0.26468084, 0.36114337, 0.69506153,\n",
       "         0.47347415, 0.79223538, 0.89019116, 0.97964006, 0.72966268,\n",
       "         0.90505689, 0.26728155, 0.53923531, 0.81269583, 0.6750384 ,\n",
       "         0.12070909, 0.09238885, 0.98874675, 0.99079004, 0.18861248,\n",
       "         0.43461326, 0.77575118, 0.39369746, 0.49200537, 0.95314512,\n",
       "         0.38335823, 0.91576601, 0.84340855, 0.3427902 , 0.02110104,\n",
       "         0.63233661, 0.16893493, 0.13872824, 0.23529717, 0.3929642 ],\n",
       "        [0.61443185, 0.44412397, 0.78526971, 0.88092682, 0.59201113,\n",
       "         0.67317924, 0.4794113 , 0.11156101, 0.23597484, 0.01917745,\n",
       "         0.09926326, 0.48013114, 0.46916906, 0.33381891, 0.91642647,\n",
       "         0.14793231, 0.19274688, 0.58671524, 0.59791947, 0.91973859,\n",
       "         0.3787072 , 0.51636115, 0.50807314, 0.99457489, 0.54998982,\n",
       "         0.15021048, 0.08509601, 0.64622112, 0.05740697, 0.37889174,\n",
       "         0.70972437, 0.04389016, 0.94770728, 0.82659799, 0.86289468,\n",
       "         0.93563403, 0.75520129, 0.79321518, 0.79417328, 0.62313985,\n",
       "         0.67328798, 0.70657984, 0.85474264, 0.60969828, 0.82927011,\n",
       "         0.44401081, 0.7164998 , 0.26559625, 0.93903625, 0.17445672,\n",
       "         0.79578069, 0.60432766, 0.65563733, 0.76822582, 0.9952421 ,\n",
       "         0.0539835 , 0.42472911, 0.59380977, 0.27028717, 0.64877958,\n",
       "         0.88687037, 0.37866935, 0.50198554, 0.3762647 , 0.76456   ,\n",
       "         0.66977564, 0.35418405, 0.56585014, 0.27208801, 0.57979319,\n",
       "         0.64845589, 0.82731186, 0.11054046, 0.1712858 , 0.0301999 ,\n",
       "         0.69674071, 0.68907809, 0.38678755, 0.60444693, 0.26314475]],\n",
       "\n",
       "       [[0.61443185, 0.44412397, 0.78526971, 0.88092682, 0.59201113,\n",
       "         0.67317924, 0.4794113 , 0.11156101, 0.23597484, 0.01917745,\n",
       "         0.09926326, 0.48013114, 0.46916906, 0.33381891, 0.91642647,\n",
       "         0.14793231, 0.19274688, 0.58671524, 0.59791947, 0.91973859,\n",
       "         0.3787072 , 0.51636115, 0.50807314, 0.99457489, 0.54998982,\n",
       "         0.15021048, 0.08509601, 0.64622112, 0.05740697, 0.37889174,\n",
       "         0.70972437, 0.04389016, 0.94770728, 0.82659799, 0.86289468,\n",
       "         0.93563403, 0.75520129, 0.79321518, 0.79417328, 0.62313985,\n",
       "         0.67328798, 0.70657984, 0.85474264, 0.60969828, 0.82927011,\n",
       "         0.44401081, 0.7164998 , 0.26559625, 0.93903625, 0.17445672,\n",
       "         0.79578069, 0.60432766, 0.65563733, 0.76822582, 0.9952421 ,\n",
       "         0.0539835 , 0.42472911, 0.59380977, 0.27028717, 0.64877958,\n",
       "         0.88687037, 0.37866935, 0.50198554, 0.3762647 , 0.76456   ,\n",
       "         0.66977564, 0.35418405, 0.56585014, 0.27208801, 0.57979319,\n",
       "         0.64845589, 0.82731186, 0.11054046, 0.1712858 , 0.0301999 ,\n",
       "         0.69674071, 0.68907809, 0.38678755, 0.60444693, 0.26314475],\n",
       "        [0.23401542, 0.42228472, 0.50531825, 0.20279606, 0.91995903,\n",
       "         0.30634672, 0.13007429, 0.35477087, 0.92853568, 0.58362545,\n",
       "         0.8104809 , 0.22018967, 0.09817607, 0.1615762 , 0.42439855,\n",
       "         0.33351668, 0.01070409, 0.54940112, 0.35227026, 0.24254914,\n",
       "         0.52740838, 0.33677253, 0.03791711, 0.26449697, 0.42199834,\n",
       "         0.17486374, 0.91344914, 0.02789081, 0.17450939, 0.47205213,\n",
       "         0.83930921, 0.83335801, 0.93705023, 0.68273353, 0.94968175,\n",
       "         0.77043128, 0.70228052, 0.64085037, 0.01431701, 0.12356787,\n",
       "         0.20890835, 0.45276279, 0.89631949, 0.98689522, 0.8829223 ,\n",
       "         0.47160068, 0.18647887, 0.52251373, 0.77082322, 0.03786571,\n",
       "         0.59163723, 0.228689  , 0.36981411, 0.54970785, 0.1014722 ,\n",
       "         0.06935169, 0.19940019, 0.42960072, 0.59440889, 0.03776726,\n",
       "         0.83108463, 0.68862039, 0.40916244, 0.17667897, 0.75068394,\n",
       "         0.18240149, 0.85089597, 0.02562843, 0.71446713, 0.2953268 ,\n",
       "         0.49767685, 0.56521435, 0.53442202, 0.54912732, 0.0220368 ,\n",
       "         0.2515639 , 0.52124359, 0.75125315, 0.9110543 , 0.59122115],\n",
       "        [0.97059938, 0.1327552 , 0.62867616, 0.01706857, 0.32675411,\n",
       "         0.26267652, 0.92978491, 0.00992601, 0.29642302, 0.71136528,\n",
       "         0.09837593, 0.15380882, 0.67414041, 0.33524869, 0.65364439,\n",
       "         0.41695333, 0.03227059, 0.24286589, 0.9259109 , 0.56632081,\n",
       "         0.87028135, 0.97963949, 0.70948917, 0.30234046, 0.74561555,\n",
       "         0.29757493, 0.91627068, 0.4410485 , 0.14070611, 0.07719135,\n",
       "         0.83763449, 0.30424557, 0.42040079, 0.33083098, 0.91438861,\n",
       "         0.59751195, 0.60015829, 0.79527315, 0.90614369, 0.45376402,\n",
       "         0.04864328, 0.09522023, 0.66523086, 0.44772576, 0.10146404,\n",
       "         0.26235095, 0.81850172, 0.4142242 , 0.1477107 , 0.09222143,\n",
       "         0.63756343, 0.38376604, 0.53197884, 0.87579038, 0.3659653 ,\n",
       "         0.30704461, 0.8212839 , 0.50985272, 0.12377823, 0.61401564,\n",
       "         0.59280495, 0.72694394, 0.45545196, 0.15971853, 0.54394807,\n",
       "         0.92849072, 0.9076696 , 0.03349698, 0.16478941, 0.42160836,\n",
       "         0.58587108, 0.34884393, 0.27817103, 0.09133674, 0.71147111,\n",
       "         0.21840769, 0.55738317, 0.17578473, 0.69028396, 0.97723154]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 80\n",
    "\n",
    "my_neural_embedding = neural_embedding(vocab_size)\n",
    "#print(my_neural_embedding.token_embedding_table)\n",
    "uglies_array_youve_ever_seeeeeen = np.array([[1,2,3], [3,4,7]])\n",
    "my_neural_embedding.forward(uglies_array_youve_ever_seeeeeen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

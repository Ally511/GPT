{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:50:53.809092Z",
     "start_time": "2025-07-30T12:50:53.788786Z"
    }
   },
   "source": [
    "# import necessary imports\n",
    "import numpy as np\n",
    "#from numpy_ml.neural_nets.optimizers import Adam\n",
    "import torch\n",
    "from torch import nn, optim"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:50:55.710947Z",
     "start_time": "2025-07-30T12:50:55.693133Z"
    }
   },
   "source": [
    "# # data preprocessing \n",
    "#     # load encoded train/validation/test sets\n",
    "#     # chunken und batchen\n",
    "# \n",
    "# batch_size = 1\n",
    "# chunk_size = 1\n",
    "# \n",
    "# def get_batch(input, batch_size):\n",
    "#     return np.array([[1,2,3], [3,4,7]]), np.array([[2,3,3], [4,7,7]]) # pls adapt. obv. :))))))"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:10:17.670507Z",
     "start_time": "2025-07-30T13:10:17.647515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_batch(input, batch_size,chunk_size):\n",
    "\n",
    "    input_batch = []\n",
    "    print(type(input_batch))\n",
    "    target_batch = []\n",
    "    idx = np.random.randint(0,len(input)-(chunk_size+1),size=batch_size)\n",
    "    for i in range(0,len(idx)-1):\n",
    "        input_batch.append(input[idx[i]:idx[i]+chunk_size])\n",
    "        target_batch.append(input[idx[i]+1:idx[i]+(chunk_size+1)])\n",
    "    \n",
    "    input_batch = np.array(input_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    return input_batch, target_batch"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:10:22.690210Z",
     "start_time": "2025-07-30T13:10:20.643894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "  indices_text = eval(f.read())\n",
    "  \n",
    "x,y = get_batch(indices_text,4,8)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(y.shape)\n",
    "print(y)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(3, 8)\n",
      "[[1263  791  190  360  608  127 1263 1285]\n",
      " [ 438 1170 1539 1459  462  622 1401  966]\n",
      " [1285  335 1540 1336   14 1285  485 1274]]\n",
      "(3, 8)\n",
      "[[ 791  190  360  608  127 1263 1285  413]\n",
      " [1170 1539 1459  462  622 1401  966  797]\n",
      " [ 335 1540 1336   14 1285  485 1274 1386]]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:51:07.699419Z",
     "start_time": "2025-07-30T12:51:07.672241Z"
    }
   },
   "source": [
    "class neural_embedding:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding_table = np.random.rand(vocab_size, vocab_size)\n",
    "\n",
    "    def calculate_softmax(self, x):\n",
    "        \"\"\"Takes input array x and returns softmax.\"\"\"\n",
    "        soft_x = np.exp(x - np.max(x))\n",
    "        softer_x = soft_x / np.sum(soft_x)\n",
    "        return softer_x\n",
    "\n",
    "    def calculate_cross_entropy(self, y_hatless, y_hat):\n",
    "        \"\"\"\n",
    "        Takes target (y_hatless) and prediction (y_hat) and computes cross entropy loss.\n",
    "        \"\"\"\n",
    "        # get vocab_size\n",
    "        _, _, vocab_size = y_hat.shape        \n",
    "        y_hat = y_hat.reshape(y_hat.shape[0]*y_hat.shape[1], y_hat.shape[2])\n",
    "        y_hatless_flat = y_hatless.reshape(-1)\n",
    "        # one-hot encode targets\n",
    "        y_hatless_hot = np.eye(vocab_size)[y_hatless_flat]\n",
    "       \n",
    "        y_hat = self.calculate_softmax(y_hat)\n",
    "    \n",
    "        return -np.sum(y_hatless_hot*np.log(y_hat))\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Implements forward pass with an unnecessary logitte function \n",
    "        which i only did not delete because now I'm emotionally attached.\n",
    "        Args:\n",
    "            idx(np.array): (B,T) numpy array of integers\n",
    "            targets(np.array): (B,T) numpy array of integers\n",
    "        Returns:\n",
    "            input_logits(np.array)\n",
    "            sometimes also: targets(np.array)\n",
    "        \"\"\"\n",
    "        batch_size, chunk_size = idx.shape\n",
    "        logits = np.zeros((batch_size, chunk_size, (self.token_embedding_table[0].size)))\n",
    "\n",
    "        def logitte(batch_size, chunk_size, input):\n",
    "            for batch in range(batch_size):\n",
    "                for chunk in range(chunk_size):\n",
    "                    # (B,T,C) b=batch_size, t=\"time\"=chunk_size, c=vocab_size\n",
    "                    logits[batch][chunk] = self.token_embedding_table[input[batch][chunk]]\n",
    "                    \n",
    "            return logits\n",
    "\n",
    "        input_logits = logitte(batch_size, chunk_size, idx)\n",
    "        \n",
    "        if targets is not None:\n",
    "            loss = self.calculate_cross_entropy(targets, input_logits)\n",
    "\n",
    "            return input_logits, loss\n",
    "\n",
    "        return input_logits\n",
    "\n",
    "    \n",
    "    def backward(self, targets, input_logits):\n",
    "        # need to do the same reshaping as we did for cross entropy, apparently\n",
    "        targets_flat = targets.reshape(-1)\n",
    "        one_hot_targets = np.eye(self.vocab_size)[targets_flat]\n",
    "\n",
    "        # shape after: ((batch_size*chunk_size), vocab_size)\n",
    "        input_logits_2d = input_logits.reshape(input_logits.shape[0]*input_logits.shape[1], input_logits.shape[2])\n",
    "\n",
    "        # somehow this is supposedly the combiantion of the derivative of softmax with the derivative of the CCE\n",
    "        delta = one_hot_targets - input_logits_2d\n",
    "\n",
    "        # want shape (80,80) for matrix multiplication, but with correct indices (use one-hot targets for that)\n",
    "        delta_indexed = np.dot(one_hot_targets.transpose(),delta)\n",
    "\n",
    "        # compute gradient for weight matrix: dot product between the transpose of the to layer and delta vector computed above\n",
    "        gradient = (self.token_embedding_table @ delta_indexed) \n",
    "\n",
    "    \n",
    "        return gradient\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:17:17.599927Z",
     "start_time": "2025-07-30T13:17:17.559373Z"
    }
   },
   "source": [
    "# currently do not use loss at all, so something is probably very wrong\n",
    "\n",
    "def train(model, text, optimiser, token_embedding_table):\n",
    "    batch_size=32\n",
    "    chunk_size = 8\n",
    "    for steps in range(10): # TODO: please increase\n",
    "        # sample batch of data\n",
    "        xb, yb = get_batch(text, batch_size, chunk_size) # TODO: pls adapt to above fct\n",
    "        optimiser.zero_grad()  # reset gradients\n",
    "        # get logits and loss\n",
    "        logits, loss = model.forward(xb, yb)\n",
    "        print(f\"loss: {loss}\")\n",
    "\n",
    "        gradient = model.backward(yb, logits)\n",
    "        token_embedding_table.grad = torch.tensor(gradient, dtype=torch.float32)\n",
    "        optimiser.step()  # apply gradients to parameters\n",
    "        diff = model.token_embedding_table - np.array(token_embedding_table)\n",
    "        print(f\"Difference in weights before - afte Adam: {diff}\")\n",
    "        model.token_embedding_table = np.array(token_embedding_table)\n",
    "        "
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:22:27.375726Z",
     "start_time": "2025-07-30T13:22:26.172166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "    indices_text = eval(f.read())\n",
    "with open(r\"vocab_train.txt\", 'r') as f:\n",
    "    vocab_train = eval(f.read())"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:22:31.375266Z",
     "start_time": "2025-07-30T13:22:28.964492Z"
    }
   },
   "source": [
    "vocab_size = len(vocab_train)\n",
    "\n",
    "my_neural_embedding = neural_embedding(vocab_size)\n",
    "param_dict = {\"weight\": my_neural_embedding.token_embedding_table} # one entry, key is weight and value is my_neural\n",
    "\n",
    "tensor = torch.tensor(my_neural_embedding.token_embedding_table, dtype=torch.float32)\n",
    "optimiser = optim.Adam([tensor], lr=0.3)\n",
    "train(my_neural_embedding, indices_text, optimiser, tensor)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "loss: 3202.6472473793574\n",
      "Difference in weights before - afte Adam: [[-0.29999998 -0.29999998 -0.29999997 ... -0.29999999 -0.29999999\n",
      "  -0.29999995]\n",
      " [-0.29999997 -0.30000003 -0.29999998 ... -0.30000001 -0.29999997\n",
      "  -0.29999998]\n",
      " [-0.29999997 -0.29999999 -0.29999998 ... -0.29999993 -0.3\n",
      "  -0.30000001]\n",
      " ...\n",
      " [-0.29999999 -0.30000001 -0.29999993 ... -0.29999996 -0.29999994\n",
      "  -0.29999997]\n",
      " [-0.29999998 -0.29999987 -0.29999993 ... -0.29999996 -0.29999999\n",
      "  -0.29999995]\n",
      " [-0.30000003 -0.30000001 -0.3        ... -0.29999998 -0.29999995\n",
      "  -0.30000001]]\n",
      "<class 'list'>\n",
      "loss: 3197.2258681386966\n",
      "Difference in weights before - afte Adam: [[-0.28314358 -0.27951425 -0.2813983  ... -0.28145868 -0.28104\n",
      "  -0.27603263]\n",
      " [-0.2811556  -0.27822626 -0.27827388 ... -0.2795955  -0.27734727\n",
      "  -0.27823392]\n",
      " [-0.28287196 -0.2783522  -0.27953547 ... -0.27827358 -0.27961063\n",
      "  -0.27764198]\n",
      " ...\n",
      " [-0.2839327  -0.281659   -0.28297532 ... -0.28117943 -0.28143716\n",
      "  -0.2798254 ]\n",
      " [-0.2848431  -0.28276253 -0.2828678  ... -0.28162384 -0.2835757\n",
      "  -0.2810866 ]\n",
      " [-0.282481   -0.28066826 -0.2809552  ... -0.28020912 -0.28032136\n",
      "  -0.27760255]]\n",
      "<class 'list'>\n",
      "loss: 3205.6715607289398\n",
      "Difference in weights before - afte Adam: [[-0.27513945 -0.27418715 -0.2744056  ... -0.2743237  -0.27502656\n",
      "  -0.27318573]\n",
      " [-0.27355254 -0.27364552 -0.27382135 ... -0.27355826 -0.272992\n",
      "  -0.27284825]\n",
      " [-0.27425665 -0.2724951  -0.27269763 ... -0.2720034  -0.2723816\n",
      "  -0.27156997]\n",
      " ...\n",
      " [-0.27522898 -0.2754382  -0.27525258 ... -0.27400625 -0.27455544\n",
      "  -0.27301216]\n",
      " [-0.27529716 -0.2738731  -0.27428222 ... -0.2732392  -0.27439898\n",
      "  -0.27338672]\n",
      " [-0.27495056 -0.2752366  -0.2740327  ... -0.27351725 -0.27401543\n",
      "  -0.2726102 ]]\n",
      "<class 'list'>\n",
      "loss: 3195.4881986716523\n",
      "Difference in weights before - afte Adam: [[-0.27254486 -0.27110142 -0.2744137  ... -0.27268636 -0.27396846\n",
      "  -0.2739265 ]\n",
      " [-0.27255642 -0.27158618 -0.2749697  ... -0.27285016 -0.27326393\n",
      "  -0.27425408]\n",
      " [-0.27299768 -0.27114177 -0.27429664 ... -0.27232707 -0.27311796\n",
      "  -0.27350736]\n",
      " ...\n",
      " [-0.27269685 -0.27176678 -0.27516365 ... -0.27248526 -0.27364445\n",
      "  -0.27354884]\n",
      " [-0.27307796 -0.2713039  -0.27499723 ... -0.27278185 -0.27400303\n",
      "  -0.2744925 ]\n",
      " [-0.2722823  -0.27140296 -0.2741816  ... -0.27208507 -0.27345002\n",
      "  -0.27382028]]\n",
      "<class 'list'>\n",
      "loss: 3196.885484996036\n",
      "Difference in weights before - afte Adam: [[-0.27472484 -0.2744825  -0.27605176 ... -0.27493966 -0.27507234\n",
      "  -0.2752396 ]\n",
      " [-0.27490234 -0.27464366 -0.2762792  ... -0.27532697 -0.2749548\n",
      "  -0.27538157]\n",
      " [-0.27460885 -0.27392042 -0.27527988 ... -0.27435887 -0.27419627\n",
      "  -0.27454495]\n",
      " ...\n",
      " [-0.2752179  -0.27498317 -0.27675295 ... -0.2753936  -0.2754693\n",
      "  -0.27523947]\n",
      " [-0.273772   -0.2731018  -0.27499843 ... -0.27404702 -0.2738706\n",
      "  -0.27413213]\n",
      " [-0.27520514 -0.27486753 -0.2761265  ... -0.27501726 -0.27526116\n",
      "  -0.27552795]]\n",
      "<class 'list'>\n",
      "loss: 3194.2050770455994\n",
      "Difference in weights before - afte Adam: [[-0.27731156 -0.27678728 -0.27814853 ... -0.27681553 -0.27746034\n",
      "  -0.27701473]\n",
      " [-0.27688324 -0.27600884 -0.27763295 ... -0.27623022 -0.2767539\n",
      "  -0.27657747]\n",
      " [-0.2777053  -0.27674937 -0.27812827 ... -0.2767179  -0.2773292\n",
      "  -0.2772665 ]\n",
      " ...\n",
      " [-0.27743554 -0.2766857  -0.27822113 ... -0.2766781  -0.27741647\n",
      "  -0.27676988]\n",
      " [-0.27649927 -0.27553248 -0.27715015 ... -0.27592146 -0.2764014\n",
      "  -0.27624488]\n",
      " [-0.2774564  -0.27685726 -0.27795553 ... -0.27663147 -0.2774135\n",
      "  -0.27709377]]\n",
      "<class 'list'>\n",
      "loss: 3201.088657075178\n",
      "Difference in weights before - afte Adam: [[-0.2803173  -0.28022552 -0.28114676 ... -0.28048468 -0.2807181\n",
      "  -0.2805922 ]\n",
      " [-0.27939177 -0.27893782 -0.2800629  ... -0.27942276 -0.27943754\n",
      "  -0.27958035]\n",
      " [-0.28041708 -0.27997482 -0.2810185  ... -0.2802229  -0.28038776\n",
      "  -0.28059208]\n",
      " ...\n",
      " [-0.2801609  -0.27975512 -0.2808349  ... -0.2800758  -0.28020144\n",
      "  -0.2800839 ]\n",
      " [-0.27929735 -0.27882838 -0.28007793 ... -0.27943492 -0.27946615\n",
      "  -0.27959907]\n",
      " [-0.28029788 -0.28014398 -0.28096032 ... -0.28029943 -0.28049672\n",
      "  -0.280514  ]]\n",
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1541 is out of bounds for axis 0 with size 1541",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(my_neural_embedding\u001B[38;5;241m.\u001B[39mtoken_embedding_table, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m      7\u001B[0m optimiser \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam([tensor], lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmy_neural_embedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[11], line 11\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, text, optimiser, token_embedding_table)\u001B[0m\n\u001B[0;32m      9\u001B[0m optimiser\u001B[38;5;241m.\u001B[39mzero_grad()  \u001B[38;5;66;03m# reset gradients\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# get logits and loss\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m logits, loss \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43myb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     14\u001B[0m gradient \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mbackward(yb, logits)\n",
      "Cell \u001B[1;32mIn[4], line 49\u001B[0m, in \u001B[0;36mneural_embedding.forward\u001B[1;34m(self, idx, targets)\u001B[0m\n\u001B[0;32m     45\u001B[0m             logits[batch][chunk] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken_embedding_table[\u001B[38;5;28minput\u001B[39m[batch][chunk]]\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m logits\n\u001B[1;32m---> 49\u001B[0m input_logits \u001B[38;5;241m=\u001B[39m \u001B[43mlogitte\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m targets \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     52\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcalculate_cross_entropy(targets, input_logits)\n",
      "Cell \u001B[1;32mIn[4], line 45\u001B[0m, in \u001B[0;36mneural_embedding.forward.<locals>.logitte\u001B[1;34m(batch_size, chunk_size, input)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(batch_size):\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(chunk_size):\n\u001B[0;32m     44\u001B[0m         \u001B[38;5;66;03m# (B,T,C) b=batch_size, t=\"time\"=chunk_size, c=vocab_size\u001B[39;00m\n\u001B[1;32m---> 45\u001B[0m         logits[batch][chunk] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoken_embedding_table\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m logits\n",
      "\u001B[1;31mIndexError\u001B[0m: index 1541 is out of bounds for axis 0 with size 1541"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

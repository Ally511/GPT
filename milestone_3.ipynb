{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary imports\n",
    "import numpy as np\n",
    "#from numpy_ml.neural_nets.optimizers import Adam\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing \n",
    "    # load encoded train/validation/test sets\n",
    "    # chunken und batchen\n",
    "\n",
    "batch_size = 1\n",
    "chunk_size = 1\n",
    "\n",
    "def get_batch(input, batch_size):\n",
    "    return np.array([[1,2,3], [3,4,7]]), np.array([[2,3,3], [4,7,7]]) # pls adapt. obv. :))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_embedding:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding_table = np.random.rand(vocab_size, vocab_size)\n",
    "\n",
    "    def calculate_softmax(self, x):\n",
    "        \"\"\"Takes input array x and returns softmax.\"\"\"\n",
    "        soft_x = np.exp(x - np.max(x))\n",
    "        softer_x = soft_x / np.sum(soft_x)\n",
    "        return softer_x\n",
    "\n",
    "    def calculate_cross_entropy(self, y_hatless, y_hat):\n",
    "        \"\"\"\n",
    "        Takes target (y_hatless) and prediction (y_hat) and computes cross entropy loss.\n",
    "        \"\"\"\n",
    "        # get vocab_size\n",
    "        _, _, vocab_size = y_hat.shape        \n",
    "        y_hat = y_hat.reshape(y_hat.shape[0]*y_hat.shape[1], y_hat.shape[2])\n",
    "        y_hatless_flat = y_hatless.reshape(-1)\n",
    "        # one-hot encode targets\n",
    "        y_hatless_hot = np.eye(vocab_size)[y_hatless_flat]\n",
    "       \n",
    "        y_hat = self.calculate_softmax(y_hat)\n",
    "    \n",
    "        return -np.sum(y_hatless_hot*np.log(y_hat))\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Implements forward pass with an unnecessary logitte function \n",
    "        which i only did not delete because now I'm emotionally attached.\n",
    "        Args:\n",
    "            idx(np.array): (B,T) numpy array of integers\n",
    "            targets(np.array): (B,T) numpy array of integers\n",
    "        Returns:\n",
    "            input_logits(np.array)\n",
    "            sometimes also: targets(np.array)\n",
    "        \"\"\"\n",
    "        batch_size, chunk_size = idx.shape\n",
    "        logits = np.zeros((batch_size, chunk_size, (self.token_embedding_table[0].size)))\n",
    "\n",
    "        def logitte(batch_size, chunk_size, input):\n",
    "            for batch in range(batch_size):\n",
    "                for chunk in range(chunk_size):\n",
    "                    # (B,T,C) b=batch_size, t=\"time\"=chunk_size, c=vocab_size\n",
    "                    logits[batch][chunk] = self.token_embedding_table[input[batch][chunk]]\n",
    "                    \n",
    "            return logits\n",
    "\n",
    "        input_logits = logitte(batch_size, chunk_size, idx)\n",
    "        \n",
    "        if targets is not None:\n",
    "            loss = self.calculate_cross_entropy(targets, input_logits)\n",
    "\n",
    "            return input_logits, loss\n",
    "\n",
    "        return input_logits\n",
    "\n",
    "    \n",
    "    def backward(self, targets, input_logits):\n",
    "        # need to do the same reshaping as we did for cross entropy, apparently\n",
    "        targets_flat = targets.reshape(-1)\n",
    "        one_hot_targets = np.eye(self.vocab_size)[targets_flat]\n",
    "\n",
    "        # shape after: ((batch_size*chunk_size), vocab_size)\n",
    "        input_logits_2d = input_logits.reshape(input_logits.shape[0]*input_logits.shape[1], input_logits.shape[2])\n",
    "\n",
    "        # somehow this is supposedly the combiantion of the derivative of softmax with the derivative of the CCE\n",
    "        delta = one_hot_targets - input_logits_2d\n",
    "\n",
    "        # want shape (80,80) for matrix multiplication, but with correct indices (use one-hot targets for that)\n",
    "        delta_indexed = np.dot(one_hot_targets.transpose(),delta)\n",
    "\n",
    "        # compute gradient for weight matrix: dot product between the transpose of the to layer and delta vector computed above\n",
    "        gradient = (self.token_embedding_table @ delta_indexed) \n",
    "\n",
    "    \n",
    "        return gradient\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "    \"\"\"\n",
    "    Initializes v and s as two python dictionaries with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters[\"W\" + str(l)] = Wl\n",
    "                    parameters[\"b\" + str(l)] = bl\n",
    "    \n",
    "    Returns: \n",
    "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
    "                    v[\"dW\" + str(l)] = ...\n",
    "                    v[\"db\" + str(l)] = ...\n",
    "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
    "                    s[\"dW\" + str(l)] = ...\n",
    "                    s[\"db\" + str(l)] = ...\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialization of v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)]).shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)]).shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)]).shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)]).shape)\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \n",
    "    # In the adam Paper by default beta1 is taken as 0.9 and beta2 as 0.999 and the epsilon as 10^(-8)\n",
    "    \n",
    "    \"\"\"\n",
    "    Update parameters using Adam\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)]\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] /(1 - beta1 ** t)\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] /(1 - beta1 ** t)\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * (np.square(grads[\"dW\" + str(l+1)]) )\n",
    "        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * (np.square(grads[\"db\" + str(l+1)]) )\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] /(1 - beta2 ** t)\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] /(1 - beta2 ** t)\n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate *  v_corrected[\"dW\" + str(l+1)] /(np.sqrt(s_corrected[\"dW\" + str(l+1)]) + epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / (np.sqrt(s_corrected[\"db\" + str(l+1)]) + epsilon)\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently do not use loss at all, so something is probably very wrong\n",
    "\n",
    "def train(model, optimiser, token_embedding_table):\n",
    "    batch_size=32\n",
    "    for steps in range(10): # TODO: please increase\n",
    "        # sample batch of data\n",
    "        xb, yb = get_batch('train', batch_size) # TODO: pls adapt to above fct\n",
    "        optimiser.zero_grad()  # reset gradients\n",
    "        # get logits and loss\n",
    "        logits, loss = model.forward(xb, yb)\n",
    "        print(f\"loss: {loss}\")\n",
    "\n",
    "        gradient = model.backward(yb, logits)\n",
    "        token_embedding_table.grad = torch.tensor(gradient, dtype=torch.float32)\n",
    "        optimiser.step()  # apply gradients to parameters\n",
    "        diff = model.token_embedding_table - np.array(token_embedding_table)\n",
    "        print(f\"Difference in weights before - afte Adam: {diff}\")\n",
    "        model.token_embedding_table = np.array(token_embedding_table)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 36.64729368813076\n",
      "Difference in weights before - after Adam: [[-0.29999993 -0.29999993 -0.30000002 ... -0.29999996 -0.29999998\n",
      "  -0.30000001]\n",
      " [-0.29999996 -0.30000003 -0.29999999 ... -0.29999997 -0.29999997\n",
      "  -0.29999997]\n",
      " [-0.29999995 -0.3        -0.29999996 ... -0.30000005 -0.29999996\n",
      "  -0.29999997]\n",
      " ...\n",
      " [-0.30000003 -0.29999998 -0.29999997 ... -0.3        -0.29999998\n",
      "  -0.30000002]\n",
      " [-0.30000001 -0.29999999 -0.29999995 ... -0.29999993 -0.29999995\n",
      "  -0.29999997]\n",
      " [-0.30000003 -0.29999994 -0.29999997 ... -0.30000004 -0.30000003\n",
      "  -0.29999998]]\n",
      "loss: 36.64729382526401\n",
      "Difference in weights before - after Adam: [[-0.2822063  -0.27971232 -0.26941466 ... -0.27572685 -0.27336824\n",
      "  -0.27926672]\n",
      " [-0.28566408 -0.28463483 -0.27554727 ... -0.28230542 -0.27712432\n",
      "  -0.28205895]\n",
      " [-0.28234565 -0.28217876 -0.26900077 ... -0.27793837 -0.27181703\n",
      "  -0.2796803 ]\n",
      " ...\n",
      " [-0.29399294 -0.2975868  -0.28512794 ... -0.29538053 -0.2817943\n",
      "  -0.29099357]\n",
      " [-0.27659386 -0.27931553 -0.27403176 ... -0.27673483 -0.2676729\n",
      "  -0.27018154]\n",
      " [-0.28539062 -0.28575522 -0.27392972 ... -0.28136373 -0.27477682\n",
      "  -0.2823426 ]]\n",
      "loss: 36.71310549703043\n",
      "Difference in weights before - after Adam: [[-0.27701938 -0.2746774  -0.2659279  ... -0.2713486  -0.26908898\n",
      "  -0.2741611 ]\n",
      " [-0.28044558 -0.27928543 -0.2708447  ... -0.27706248 -0.272407\n",
      "  -0.27686763]\n",
      " [-0.2770753  -0.27673602 -0.26548755 ... -0.27298343 -0.267839\n",
      "  -0.2744186 ]\n",
      " ...\n",
      " [-0.28091377 -0.2839707  -0.2670927  ... -0.28017867 -0.268399\n",
      "  -0.27795935]\n",
      " [-0.27245235 -0.2744388  -0.268557   ... -0.272061   -0.26466143\n",
      "  -0.2672338 ]\n",
      " [-0.2801602  -0.28036225 -0.26960778 ... -0.27621257 -0.27056158\n",
      "  -0.27708137]]\n",
      "loss: 36.76585249496301\n",
      "Difference in weights before - after Adam: [[-0.27583277 -0.27388632 -0.26656222 ... -0.27119577 -0.2690451\n",
      "  -0.27329814]\n",
      " [-0.27887464 -0.27781296 -0.27049947 ... -0.2758609  -0.27187508\n",
      "  -0.27571428]\n",
      " [-0.27583325 -0.27543855 -0.26617312 ... -0.2723472  -0.26808214\n",
      "  -0.27343547]\n",
      " ...\n",
      " [-0.2754463  -0.27697003 -0.2631954  ... -0.27366817 -0.2655406\n",
      "  -0.27311838]\n",
      " [-0.27191222 -0.27324152 -0.26757193 ... -0.27117813 -0.26524806\n",
      "  -0.26772988]\n",
      " [-0.27862    -0.27868342 -0.26956928 ... -0.27516973 -0.27049017\n",
      "  -0.27585828]]\n",
      "loss: 36.808121048534815\n",
      "Difference in weights before - after Adam: [[-0.2766857  -0.2750981  -0.26886797 ... -0.27288544 -0.27085257\n",
      "  -0.27446258]\n",
      " [-0.2793305  -0.27840734 -0.27209234 ... -0.27670717 -0.27327788\n",
      "  -0.2765913 ]\n",
      " [-0.27664745 -0.27624917 -0.26852715 ... -0.2736975  -0.27009487\n",
      "  -0.274518  ]\n",
      " ...\n",
      " [-0.27453578 -0.27514863 -0.26437843 ... -0.2725023  -0.26670778\n",
      "  -0.27264977]\n",
      " [-0.2732277  -0.27410376 -0.26888418 ... -0.27232373 -0.26749015\n",
      "  -0.26981652]\n",
      " [-0.27910697 -0.27908063 -0.2713722  ... -0.27614713 -0.2722211\n",
      "  -0.27668166]]\n",
      "loss: 36.842795421864835\n",
      "Difference in weights before - after Adam: [[-0.2787602  -0.2774608  -0.27208328 ... -0.27560616 -0.27370024\n",
      "  -0.2768047 ]\n",
      " [-0.28105426 -0.280262   -0.27477562 ... -0.2787757  -0.27579665\n",
      "  -0.27867913]\n",
      " [-0.27869463 -0.27831388 -0.27178097 ... -0.27618134 -0.27308762\n",
      "  -0.2768047 ]\n",
      " ...\n",
      " [-0.27596927 -0.27610803 -0.26754212 ... -0.27397788 -0.26961458\n",
      "  -0.27437568]\n",
      " [-0.2756765  -0.27625072 -0.27152348 ... -0.2747072  -0.27069068\n",
      "  -0.27284002]\n",
      " [-0.2808571  -0.2807759  -0.27420163 ... -0.27831602 -0.27497125\n",
      "  -0.27872992]]\n",
      "loss: 36.871816995359694\n",
      "Difference in weights before - after Adam: [[-0.28163362 -0.2805605  -0.27586675 ... -0.2789812  -0.27720642\n",
      "  -0.27990437]\n",
      " [-0.28362918 -0.2829497  -0.27814937 ... -0.28164315 -0.27903295\n",
      "  -0.2815597 ]\n",
      " [-0.2815497  -0.2811947  -0.2755952  ... -0.27938926 -0.2766992\n",
      "  -0.27986765]\n",
      " ...\n",
      " [-0.2786932  -0.2785921  -0.2716143  ... -0.2768438  -0.27340198\n",
      "  -0.27730536]\n",
      " [-0.2788701  -0.27924204 -0.27498484 ... -0.27789426 -0.2745006\n",
      "  -0.2764721 ]\n",
      " [-0.2834537  -0.28334045 -0.2776811  ... -0.28126    -0.2783742\n",
      "  -0.28158188]]\n",
      "loss: 36.89648945325675\n",
      "Difference in weights before - after Adam: [[-0.28506112 -0.28416634 -0.28003526 ... -0.28280473 -0.28116035\n",
      "  -0.28352427]\n",
      " [-0.28680468 -0.28621984 -0.28199363 ... -0.28506517 -0.28276277\n",
      "  -0.28499126]\n",
      " [-0.28496575 -0.28463912 -0.27978945 ... -0.28309345 -0.28073263\n",
      "  -0.28346372]\n",
      " ...\n",
      " [-0.28218913 -0.2819705  -0.27616477 ... -0.2805078  -0.27769446\n",
      "  -0.28095675]\n",
      " [-0.28257775 -0.28281307 -0.2789843  ... -0.28162837 -0.2787218\n",
      "  -0.2805233 ]\n",
      " [-0.28664708 -0.28651667 -0.2816043  ... -0.28474188 -0.28222728\n",
      "  -0.28499293]]\n",
      "loss: 36.91771707134655\n",
      "Difference in weights before - after Adam: [[-0.28888965 -0.28813672 -0.2844782  ... -0.28695178 -0.28543425\n",
      "  -0.2875185 ]\n",
      " [-0.2904191  -0.28991365 -0.28617406 ... -0.28888917 -0.2868476\n",
      "  -0.2888224 ]\n",
      " [-0.28878784 -0.28848934 -0.28425455 ... -0.28715396 -0.28506804\n",
      "  -0.28744292]\n",
      " ...\n",
      " [-0.2861805  -0.28590965 -0.28099632 ... -0.28466582 -0.282305\n",
      "  -0.28507352]\n",
      " [-0.2866547  -0.28679633 -0.2833519  ... -0.28574872 -0.2832334\n",
      "  -0.28487587]\n",
      " [-0.290277   -0.29013896 -0.28584576 ... -0.28861284 -0.28640556\n",
      "  -0.28880954]]\n",
      "loss: 36.9361490252376\n",
      "Difference in weights before - after Adam: [[-0.29301715 -0.29237866 -0.28912425 ... -0.2913401  -0.28994417\n",
      "  -0.29179025]\n",
      " [-0.29436398 -0.29392505 -0.2906034  ... -0.29301286 -0.29119587\n",
      "  -0.2929523 ]\n",
      " [-0.2929132  -0.29264092 -0.2889204  ... -0.2914791  -0.28962684\n",
      "  -0.29170585]\n",
      " ...\n",
      " [-0.29050803 -0.29022002 -0.28600645 ... -0.28914857 -0.2871294\n",
      "  -0.28950667]\n",
      " [-0.29100394 -0.29108095 -0.28798032 ... -0.29015064 -0.28795576\n",
      "  -0.28945136]\n",
      " [-0.29423547 -0.29409552 -0.29032326 ... -0.29277492 -0.29082632\n",
      "  -0.29292965]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38971/4108722637.py:25: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  diff = model.token_embedding_table - np.array(token_embedding_table)\n",
      "/tmp/ipykernel_38971/4108722637.py:27: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  model.token_embedding_table = np.array(token_embedding_table)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 80\n",
    "\n",
    "my_neural_embedding = neural_embedding(vocab_size)\n",
    "param_dict = {\"weight\": my_neural_embedding.token_embedding_table} # one entry, key is weight and value is my_neural\n",
    "\n",
    "tensor = torch.tensor(my_neural_embedding.token_embedding_table, dtype=torch.float32)\n",
    "optimiser = optim.Adam([tensor], lr=0.3)\n",
    "train(my_neural_embedding, optimiser, tensor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

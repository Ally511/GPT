{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:50:53.809092Z",
     "start_time": "2025-07-30T12:50:53.788786Z"
    }
   },
   "source": [
    "# import necessary imports\n",
    "import numpy as np\n",
    "#from numpy_ml.neural_nets.optimizers import Adam\n",
    "import torch\n",
    "from torch import nn, optim"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:29:35.898826Z",
     "start_time": "2025-07-30T13:29:35.883687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_batch(input, batch_size,chunk_size):\n",
    "\n",
    "    input_batch = []\n",
    "    # print(type(input_batch))\n",
    "    target_batch = []\n",
    "    idx = np.random.randint(0,len(input)-(chunk_size+1),size=batch_size)\n",
    "    for i in range(0,len(idx)-1):\n",
    "        input_batch.append(input[idx[i]:idx[i]+chunk_size])\n",
    "        target_batch.append(input[idx[i]+1:idx[i]+(chunk_size+1)])\n",
    "    \n",
    "    input_batch = np.array(input_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    return input_batch, target_batch"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:10:22.690210Z",
     "start_time": "2025-07-30T13:10:20.643894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "  indices_text = eval(f.read())\n",
    "  \n",
    "x,y = get_batch(indices_text,4,8)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(y.shape)\n",
    "print(y)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(3, 8)\n",
      "[[1263  791  190  360  608  127 1263 1285]\n",
      " [ 438 1170 1539 1459  462  622 1401  966]\n",
      " [1285  335 1540 1336   14 1285  485 1274]]\n",
      "(3, 8)\n",
      "[[ 791  190  360  608  127 1263 1285  413]\n",
      " [1170 1539 1459  462  622 1401  966  797]\n",
      " [ 335 1540 1336   14 1285  485 1274 1386]]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:51:07.699419Z",
     "start_time": "2025-07-30T12:51:07.672241Z"
    }
   },
   "source": [
    "class neural_embedding:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding_table = np.random.rand(vocab_size, vocab_size)\n",
    "\n",
    "    def calculate_softmax(self, x):\n",
    "        \"\"\"Takes input array x and returns softmax.\"\"\"\n",
    "        soft_x = np.exp(x - np.max(x))\n",
    "        softer_x = soft_x / np.sum(soft_x)\n",
    "        return softer_x\n",
    "\n",
    "    def calculate_cross_entropy(self, y_hatless, y_hat):\n",
    "        \"\"\"\n",
    "        Takes target (y_hatless) and prediction (y_hat) and computes cross entropy loss.\n",
    "        \"\"\"\n",
    "        # get vocab_size\n",
    "        _, _, vocab_size = y_hat.shape        \n",
    "        y_hat = y_hat.reshape(y_hat.shape[0]*y_hat.shape[1], y_hat.shape[2])\n",
    "        y_hatless_flat = y_hatless.reshape(-1)\n",
    "        # one-hot encode targets\n",
    "        y_hatless_hot = np.eye(vocab_size)[y_hatless_flat]\n",
    "       \n",
    "        y_hat = self.calculate_softmax(y_hat)\n",
    "    \n",
    "        return -np.sum(y_hatless_hot*np.log(y_hat))\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Implements forward pass with an unnecessary logitte function \n",
    "        which i only did not delete because now I'm emotionally attached.\n",
    "        Args:\n",
    "            idx(np.array): (B,T) numpy array of integers\n",
    "            targets(np.array): (B,T) numpy array of integers\n",
    "        Returns:\n",
    "            input_logits(np.array)\n",
    "            sometimes also: targets(np.array)\n",
    "        \"\"\"\n",
    "        batch_size, chunk_size = idx.shape\n",
    "        logits = np.zeros((batch_size, chunk_size, (self.token_embedding_table[0].size)))\n",
    "\n",
    "        def logitte(batch_size, chunk_size, input):\n",
    "            for batch in range(batch_size):\n",
    "                for chunk in range(chunk_size):\n",
    "                    # (B,T,C) b=batch_size, t=\"time\"=chunk_size, c=vocab_size\n",
    "                    logits[batch][chunk] = self.token_embedding_table[input[batch][chunk]]\n",
    "                    \n",
    "            return logits\n",
    "\n",
    "        input_logits = logitte(batch_size, chunk_size, idx)\n",
    "        \n",
    "        if targets is not None:\n",
    "            loss = self.calculate_cross_entropy(targets, input_logits)\n",
    "\n",
    "            return input_logits, loss\n",
    "\n",
    "        return input_logits\n",
    "\n",
    "    \n",
    "    def backward(self, targets, input_logits):\n",
    "        # need to do the same reshaping as we did for cross entropy, apparently\n",
    "        targets_flat = targets.reshape(-1)\n",
    "        one_hot_targets = np.eye(self.vocab_size)[targets_flat]\n",
    "\n",
    "        # shape after: ((batch_size*chunk_size), vocab_size)\n",
    "        input_logits_2d = input_logits.reshape(input_logits.shape[0]*input_logits.shape[1], input_logits.shape[2])\n",
    "\n",
    "        # somehow this is supposedly the combiantion of the derivative of softmax with the derivative of the CCE\n",
    "        delta = one_hot_targets - input_logits_2d\n",
    "\n",
    "        # want shape (80,80) for matrix multiplication, but with correct indices (use one-hot targets for that)\n",
    "        delta_indexed = np.dot(one_hot_targets.transpose(),delta)\n",
    "\n",
    "        # compute gradient for weight matrix: dot product between the transpose of the to layer and delta vector computed above\n",
    "        gradient = (self.token_embedding_table @ delta_indexed) \n",
    "\n",
    "    \n",
    "        return gradient\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:33:52.622952Z",
     "start_time": "2025-07-30T13:33:52.599953Z"
    }
   },
   "source": [
    "# currently do not use loss at all, so something is probably very wrong\n",
    "\n",
    "def train(model, text, optimiser, token_embedding_table, train_step):\n",
    "    batch_size=2\n",
    "    chunk_size = 8\n",
    "    for steps in range(train_step): # TODO: please increase\n",
    "        # sample batch of data\n",
    "        xb, yb = get_batch(text, batch_size, chunk_size) # TODO: pls adapt to above fct\n",
    "        optimiser.zero_grad()  # reset gradients\n",
    "        # get logits and loss\n",
    "        logits, loss = model.forward(xb, yb)\n",
    "        print(f\"loss: {loss}\")\n",
    "\n",
    "        gradient = model.backward(yb, logits)\n",
    "        token_embedding_table.grad = torch.tensor(gradient, dtype=torch.float32)\n",
    "        optimiser.step()  # apply gradients to parameters\n",
    "        # diff = model.token_embedding_table - np.array(token_embedding_table)\n",
    "        # print(f\"Difference in weights before - afte Adam: {diff}\")\n",
    "        model.token_embedding_table = np.array(token_embedding_table)\n",
    "        "
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:28:11.641787Z",
     "start_time": "2025-07-30T13:28:10.773748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "    indices_text = eval(f.read())\n",
    "with open(r\"vocab_train.txt\", 'r') as f:\n",
    "    vocab_train = eval(f.read())"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:37:57.602335Z",
     "start_time": "2025-07-30T13:37:46.059846Z"
    }
   },
   "source": [
    "vocab_size = len(vocab_train)\n",
    "\n",
    "my_neural_embedding = neural_embedding(vocab_size)\n",
    "param_dict = {\"weight\": my_neural_embedding.token_embedding_table} # one entry, key is weight and value is my_neural\n",
    "\n",
    "tensor = torch.tensor(my_neural_embedding.token_embedding_table, dtype=torch.float32)\n",
    "optimiser = optim.Adam([tensor], lr=0.05)\n",
    "train(my_neural_embedding, indices_text, optimiser, tensor, 100)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 75.8971398075822\n",
      "loss: 74.21115756332456\n",
      "loss: 76.28120404501644\n",
      "loss: 76.53743491862558\n",
      "loss: 77.96347814556802\n",
      "loss: 76.09861277798572\n",
      "loss: 75.86626607099015\n",
      "loss: 74.91210292200792\n",
      "loss: 74.46984672435244\n",
      "loss: 75.39001330928426\n",
      "loss: 75.39218656981764\n",
      "loss: 76.38491124819178\n",
      "loss: 75.64124816762077\n",
      "loss: 75.5814019052456\n",
      "loss: 74.93620534412004\n",
      "loss: 74.83211692704603\n",
      "loss: 74.4820652698238\n",
      "loss: 76.12528508275724\n",
      "loss: 76.05929364408374\n",
      "loss: 74.7113047906242\n",
      "loss: 74.80671984790442\n",
      "loss: 74.79763427067118\n",
      "loss: 76.21755590484298\n",
      "loss: 75.08532508382578\n",
      "loss: 76.55844865578308\n",
      "loss: 74.83867916369022\n",
      "loss: 76.23922502355687\n",
      "loss: 74.5855698866618\n",
      "loss: 74.81315973602985\n",
      "loss: 74.92499563179449\n",
      "loss: 75.93937820893562\n",
      "loss: 74.06197910964494\n",
      "loss: 74.75281577857575\n",
      "loss: 75.52885857484458\n",
      "loss: 75.6422897042865\n",
      "loss: 76.74882794952148\n",
      "loss: 75.21015302452456\n",
      "loss: 76.16576384817489\n",
      "loss: 75.75960706307542\n",
      "loss: 76.11343031099238\n",
      "loss: 76.01663669639909\n",
      "loss: 76.87645446666352\n",
      "loss: 77.19319029457293\n",
      "loss: 77.27847992685463\n",
      "loss: 76.0644208522433\n",
      "loss: 73.70625318863401\n",
      "loss: 73.93791390058132\n",
      "loss: 76.29596381434312\n",
      "loss: 74.679733451423\n",
      "loss: 75.43541990857548\n",
      "loss: 74.41725951628612\n",
      "loss: 76.16545967458256\n",
      "loss: 73.68572688462316\n",
      "loss: 75.75726731056142\n",
      "loss: 76.22877125878055\n",
      "loss: 74.47407395568521\n",
      "loss: 75.37240183670744\n",
      "loss: 76.6971361965725\n",
      "loss: 75.6465320612145\n",
      "loss: 74.47284038886836\n",
      "loss: 75.14388110653695\n",
      "loss: 75.37953960495284\n",
      "loss: 76.90652975236209\n",
      "loss: 74.3804269226288\n",
      "loss: 75.29396466744788\n",
      "loss: 76.25883112877618\n",
      "loss: 75.43292192646743\n",
      "loss: 75.5833159963133\n",
      "loss: 75.62383075089772\n",
      "loss: 76.8433288199713\n",
      "loss: 76.00851641166908\n",
      "loss: 74.22540970397112\n",
      "loss: 75.84925417922923\n",
      "loss: 76.03496491596457\n",
      "loss: 75.54726650762754\n",
      "loss: 75.96533450737941\n",
      "loss: 75.45671340357053\n",
      "loss: 75.23046866543068\n",
      "loss: 75.11284705752118\n",
      "loss: 75.24251589932702\n",
      "loss: 76.21971921934696\n",
      "loss: 74.12295166049606\n",
      "loss: 74.95167343153494\n",
      "loss: 76.95092021871842\n",
      "loss: 77.5235792928882\n",
      "loss: 75.56429215348115\n",
      "loss: 76.01946506193781\n",
      "loss: 74.77626326561133\n",
      "loss: 76.07235377073843\n",
      "loss: 76.6805638941731\n",
      "loss: 74.72211173330608\n",
      "loss: 76.3088060199365\n",
      "loss: 75.35389524223994\n",
      "loss: 75.4937370322228\n",
      "loss: 74.046069367858\n",
      "loss: 74.74816948373635\n",
      "loss: 75.36988766664965\n",
      "loss: 75.60587533263165\n",
      "loss: 76.58871633790568\n",
      "loss: 74.75482155697694\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

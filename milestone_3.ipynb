{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Hashable' from 'collections' (C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\collections\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# import necessary imports\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy_ml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mneural_nets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy_ml\\__init__.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m\"\"\"Common ML and ML-adjacent algorithms implemented in NumPy\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gmm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy_ml\\utils\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data_structures\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distance_metrics\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m kernels\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy_ml\\utils\\data_structures.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mheapq\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcopy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m copy\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Hashable\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistance_metrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m euclidean\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'Hashable' from 'collections' (C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\collections\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# import necessary imports\n",
    "import numpy as np\n",
    "from numpy_ml.neural_nets.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(input, batch_size,chunk_size):\n",
    "\n",
    "    input_batch = []\n",
    "    print(type(input_batch))\n",
    "    target_batch = []\n",
    "    idx = np.random.randint(0,len(input)-(chunk_size+1),size=batch_size)\n",
    "    for i in range(0,len(idx)-1):\n",
    "        input_batch.append(input[idx[i]:idx[i]+chunk_size])\n",
    "        target_batch.append(input[idx[i]+1:idx[i]+(chunk_size+1)])\n",
    "    \n",
    "    input_batch = np.array(input_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (r\"Shakespeare_byte.txt\", 'r') as f:\n",
    "  shakespeare_byte_train = eval(f.read())\n",
    "\n",
    "with open (r\"vocab_train.txt\", 'r') as f:\n",
    "  vocab_train = eval(f.read())\n",
    "\n",
    "vocab = vocab_train\n",
    "indices = np.arange(1,len(vocab)+1,1)\n",
    "inidces = indices.astype(int)\n",
    "indices = indices.tolist()\n",
    "key_byte = dict(zip(vocab, indices))\n",
    "value_byte = dict(zip(indices,vocab))\n",
    "\n",
    "# Map each token in shakespeare_byte_train to its index using key_byte\n",
    "indices_translation = [key_byte[token] for token in shakespeare_byte_train if token in key_byte]\n",
    "\n",
    "with open('indices_text.txt', 'w') as indices_text:\n",
    "    indices_text.write(str(indices_translation))\n",
    "\n",
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "  indices_text = eval(f.read())\n",
    "\n",
    "bytes_translation = [value_byte[token] for token in indices_text if token in value_byte]\n",
    "\n",
    "with open('bytes_text.txt', 'w') as bytes_text:\n",
    "    bytes_text.write(str(bytes_translation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(31, 8)\n",
      "[[ 177  698 1261  445  488  992  807 1273]\n",
      " [ 377  875  808 1263  808 1263  150  446]\n",
      " [ 439  640 1262   39 1273 1166   98 1263]\n",
      " [1534  498  270 1263  207   54 1170 1380]\n",
      " [   3  207   54  508 1263  440  240  791]\n",
      " [ 618  799  459  324  787 1419  649 1259]\n",
      " [1401 1484 1540 1263  804 1285  215  796]\n",
      " [1534  498 1284  398  256 1293  480  458]\n",
      " [1090  793  607  990 1261  766  517  438]\n",
      " [ 785 1273 1285  304  440 1263  102  785]\n",
      " [ 643  791  733 1457 1273  934 1389 1121]\n",
      " [ 188  453 1263  800  198 1273   13 1057]\n",
      " [ 797  796 1287 1391 1397 1098 1267 1263]\n",
      " [1423 1522 1207  800 1087 1396 1261  806]\n",
      " [ 177   38  181  445  122 1263  449  307]\n",
      " [ 438  863  777  797   77  868 1260 1261]\n",
      " [1293  445  828 1522 1365 1259  113 1263]\n",
      " [ 802 1273 1534 1510  171 1263  841 1065]\n",
      " [1259  105  438   85  797  976 1512  761]\n",
      " [ 307  797  445  745  442 1273   17  193]\n",
      " [1383 1531 1521 1261  439 1421 1300  462]\n",
      " [ 400  792  791 1169  805 1416 1510 1259]\n",
      " [ 867  929  802  439  894 1381 1311 1504]\n",
      " [ 402 1263 1432  807  233  186 1273   62]\n",
      " [ 840  222 1263  796  485  441 1285 1273]\n",
      " [1540  440 1263   19 1263  844  177  440]\n",
      " [ 464 1293  180 1424  796  797 1169 1309]\n",
      " [1430 1540 1293 1284  995  815  208  177]\n",
      " [1284 1326  934  462  797  213  852 1262]\n",
      " [ 470 1284  428  783  361 1296   82  831]\n",
      " [1263  510  787 1284 1417 1246 1457  580]]\n",
      "(31, 8)\n",
      "[[ 698 1261  445  488  992  807 1273  193]\n",
      " [ 875  808 1263  808 1263  150  446  382]\n",
      " [ 640 1262   39 1273 1166   98 1263  608]\n",
      " [ 498  270 1263  207   54 1170 1380  798]\n",
      " [ 207   54  508 1263  440  240  791  980]\n",
      " [ 799  459  324  787 1419  649 1259 1296]\n",
      " [1484 1540 1263  804 1285  215  796 1273]\n",
      " [ 498 1284  398  256 1293  480  458 1284]\n",
      " [ 793  607  990 1261  766  517  438 1200]\n",
      " [1273 1285  304  440 1263  102  785  799]\n",
      " [ 791  733 1457 1273  934 1389 1121 1242]\n",
      " [ 453 1263  800  198 1273   13 1057 1272]\n",
      " [ 796 1287 1391 1397 1098 1267 1263   75]\n",
      " [1522 1207  800 1087 1396 1261  806  667]\n",
      " [  38  181  445  122 1263  449  307 1263]\n",
      " [ 863  777  797   77  868 1260 1261 1312]\n",
      " [ 445  828 1522 1365 1259  113 1263 1291]\n",
      " [1273 1534 1510  171 1263  841 1065 1262]\n",
      " [ 105  438   85  797  976 1512  761 1273]\n",
      " [ 797  445  745  442 1273   17  193 1263]\n",
      " [1531 1521 1261  439 1421 1300  462 1490]\n",
      " [ 792  791 1169  805 1416 1510 1259 1273]\n",
      " [ 929  802  439  894 1381 1311 1504  191]\n",
      " [1263 1432  807  233  186 1273   62 1074]\n",
      " [ 222 1263  796  485  441 1285 1273   56]\n",
      " [ 440 1263   19 1263  844  177  440   26]\n",
      " [1293  180 1424  796  797 1169 1309 1340]\n",
      " [1540 1293 1284  995  815  208  177  309]\n",
      " [1326  934  462  797  213  852 1262 1296]\n",
      " [1284  428  783  361 1296   82  831  202]\n",
      " [ 510  787 1284 1417 1246 1457  580 1262]]\n"
     ]
    }
   ],
   "source": [
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "  indices_text = eval(f.read())\n",
    "  \n",
    "x,y = get_batch(indices_text,32,8)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_characters(input):\n",
    " #Decodes a list of indices back to their corresponding characters\n",
    " #given the abive defined vocabulary\n",
    "    decoded = [] #given the input, we will decode it back to characters\n",
    "    for i in range(0,len(input)):\n",
    "        decoded.append(value_byte[input[i]])#using the translation dctionary: value_byte\n",
    "#make its prettier by joining list to actual words and replacing underscores with spaces\n",
    "    decoded = ''.join(decoded)\n",
    "    decoded = decoded.replace('_', ' ')\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tragedy of antony and cleopatra dramatis personae mark antony octavius caesar m \n"
     ]
    }
   ],
   "source": [
    "text1 = [438, 1006, 1434, 853, 797, 27, 439, 1, 1024, 1130, 781, 886, 1184, 1519, 1259, 221, 27, 5, 25, 1294]\n",
    "\n",
    "decoded = decode_characters(text1)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_embedding:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding_table = np.random.rand(vocab_size, vocab_size)\n",
    "\n",
    "    def calculate_softmax(self, x):\n",
    "        \"\"\"Takes input array x and returns softmax.\"\"\"\n",
    "        soft_x = np.exp(x - np.max(x))\n",
    "        softer_x = soft_x / np.sum(soft_x)\n",
    "        return softer_x\n",
    "\n",
    "    def calculate_cross_entropy(self, y_hatless, y_hat):\n",
    "        \"\"\"\n",
    "        Takes target (y_hatless) and prediction (y_hat) and computes cross entropy loss.\n",
    "        \"\"\"\n",
    "        # get vocab_size\n",
    "        _, _, vocab_size = y_hat.shape        \n",
    "        y_hat = y_hat.reshape(y_hat.shape[0]*y_hat.shape[1], y_hat.shape[2])\n",
    "        y_hatless_flat = y_hatless.reshape(-1)\n",
    "        # one-hot encode targets\n",
    "        y_hatless_hot = np.eye(vocab_size)[y_hatless_flat]\n",
    "       \n",
    "        y_hat = self.calculate_softmax(y_hat)\n",
    "    \n",
    "        return -np.sum(y_hatless_hot*np.log(y_hat))\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Implements forward pass with an unnecessary logitte function \n",
    "        which i only did not delete because now I'm emotionally attached.\n",
    "        Args:\n",
    "            idx(np.array): (B,T) numpy array of integers\n",
    "            targets(np.array): (B,T) numpy array of integers\n",
    "        Returns:\n",
    "            input_logits(np.array)\n",
    "            sometimes also: targets(np.array)\n",
    "        \"\"\"\n",
    "        batch_size, chunk_size = idx.shape\n",
    "        logits = np.zeros((batch_size, chunk_size, (self.token_embedding_table[0].size)))\n",
    "\n",
    "        def logitte(batch_size, chunk_size, input):\n",
    "            for batch in range(batch_size):\n",
    "                for chunk in range(chunk_size):\n",
    "                    # (B,T,C) b=batch_size, t=\"time\"=chunk_size, c=vocab_size\n",
    "                    logits[batch][chunk] = self.token_embedding_table[input[batch][chunk]]\n",
    "                    \n",
    "            return logits\n",
    "\n",
    "        input_logits = logitte(batch_size, chunk_size, idx)\n",
    "        \n",
    "        if targets is not None:\n",
    "            loss = self.calculate_cross_entropy(targets, input_logits)\n",
    "\n",
    "            return input_logits, loss\n",
    "\n",
    "        return input_logits\n",
    "\n",
    "    \n",
    "    def backward(self, targets, input_logits):\n",
    "        # need to do the same reshaping as we did for cross entropy, apparently\n",
    "        targets_flat = targets.reshape(-1)\n",
    "        one_hot_targets = np.eye(self.vocab_size)[targets_flat]\n",
    "\n",
    "        # shape after: ((batch_size*chunk_size), vocab_size)\n",
    "        input_logits_2d = input_logits.reshape(input_logits.shape[0]*input_logits.shape[1], input_logits.shape[2])\n",
    "\n",
    "        # somehow this is supposedly the combiantion of the derivative of softmax with the derivative of the CCE\n",
    "        delta = one_hot_targets - input_logits_2d\n",
    "\n",
    "        # want shape (80,80) for matrix multiplication, but with correct indices (use one-hot targets for that)\n",
    "        delta_indexed = np.dot(one_hot_targets.transpose(),delta)\n",
    "\n",
    "        # compute gradient for weight matrix: dot product between the transpose of the to layer and delta vector computed above\n",
    "        gradient = (self.token_embedding_table @ delta_indexed) \n",
    "\n",
    "    \n",
    "        return gradient\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "    \"\"\"\n",
    "    Initializes v and s as two python dictionaries with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters[\"W\" + str(l)] = Wl\n",
    "                    parameters[\"b\" + str(l)] = bl\n",
    "    \n",
    "    Returns: \n",
    "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
    "                    v[\"dW\" + str(l)] = ...\n",
    "                    v[\"db\" + str(l)] = ...\n",
    "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
    "                    s[\"dW\" + str(l)] = ...\n",
    "                    s[\"db\" + str(l)] = ...\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialization of v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)]).shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)]).shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)]).shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)]).shape)\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \n",
    "    # In the adam Paper by default beta1 is taken as 0.9 and beta2 as 0.999 and the epsilon as 10^(-8)\n",
    "    \n",
    "    \"\"\"\n",
    "    Update parameters using Adam\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)]\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] /(1 - beta1 ** t)\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] /(1 - beta1 ** t)\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * (np.square(grads[\"dW\" + str(l+1)]) )\n",
    "        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * (np.square(grads[\"db\" + str(l+1)]) )\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] /(1 - beta2 ** t)\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] /(1 - beta2 ** t)\n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate *  v_corrected[\"dW\" + str(l+1)] /(np.sqrt(s_corrected[\"dW\" + str(l+1)]) + epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / (np.sqrt(s_corrected[\"db\" + str(l+1)]) + epsilon)\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently do not use loss at all, so something is probably very wrong\n",
    "\n",
    "def train(model, optimiser):\n",
    "    batch_size=32\n",
    "    for steps in range(2): # TODO: please increase\n",
    "        # sample batch of data\n",
    "        xb, yb = get_batch('train', batch_size) # TODO: pls adapt to above fct\n",
    "\n",
    "        # get logits and loss\n",
    "        logits, loss = model.forward(xb, yb)\n",
    "        print(f\"loss: {loss}\")\n",
    "\n",
    "        gradient = model.backward(yb, logits)\n",
    "        before_adam = model.token_embedding_table.copy()\n",
    "        #print(f\"before Adam: {model.token_embedding_table}\")\n",
    "        \n",
    "        param_dict = {\"weight\": model.token_embedding_table}\n",
    "        param_dict, v, s = update_parameters_with_adam(parameters=param_dict, grads=gradient, v=optimiser[0], s=optimiser[1], t=5)\n",
    "        #optimiser.update(param=model.token_embedding_table, param_grad=gradient, param_name=\"gradient\", cur_loss = loss)\n",
    "        #optimiser.step()\n",
    "        #print(f\"after Adam: {model.token_embedding_table}\")\n",
    "        model.token_embedding_table = param_dict['weight']\n",
    "        diff = before_adam - model.token_embedding_table.copy()\n",
    "        print(f\"Difference in weights before - after Adam: {diff}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimiser v: {}, optimiser s: {}\n",
      "loss: 37.439745740279065\n",
      "Difference in weights before - after Adam: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "loss: 37.439745740279065\n",
      "Difference in weights before - after Adam: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 80\n",
    "\n",
    "my_neural_embedding = neural_embedding(vocab_size)\n",
    "param_dict = {\"weight\": my_neural_embedding.token_embedding_table} # one entry, key is weight and value is my_neural\n",
    "#optimiser = Adam(params = my_neural_embedding.token_embedding_table, lr=0.3)\n",
    "optimiser_v, optimiser_s = initialize_adam(param_dict)\n",
    "print(f\"Optimiser v: {optimiser_v}, optimiser s: {optimiser_s}\")\n",
    "optimiser = [optimiser_v, optimiser_s]\n",
    "train(my_neural_embedding, optimiser)\n",
    "#uglies_array_youve_ever_seeeeeen = np.array([[1,2,3], [3,4,7]])\n",
    "#my_input_logits, my_loss = my_neural_embedding.forward(uglies_array_youve_ever_seeeeeen, uglies_array_youve_ever_seeeeeen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

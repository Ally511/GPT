{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:50:53.809092Z",
     "start_time": "2025-07-30T12:50:53.788786Z"
    }
   },
   "source": [
    "# import necessary imports\n",
    "import numpy as np\n",
    "#from numpy_ml.neural_nets.optimizers import Adam\n",
    "import torch\n",
    "from torch import nn, optim"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:29:35.898826Z",
     "start_time": "2025-07-30T13:29:35.883687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_batch(input, batch_size,chunk_size):\n",
    "\n",
    "    input_batch = []\n",
    "    # print(type(input_batch))\n",
    "    target_batch = []\n",
    "    idx = np.random.randint(0,len(input)-(chunk_size+1),size=batch_size)\n",
    "    for i in range(0,len(idx)-1):\n",
    "        input_batch.append(input[idx[i]:idx[i]+chunk_size])\n",
    "        target_batch.append(input[idx[i]+1:idx[i]+(chunk_size+1)])\n",
    "    \n",
    "    input_batch = np.array(input_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    return input_batch, target_batch"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:10:22.690210Z",
     "start_time": "2025-07-30T13:10:20.643894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "  indices_text = eval(f.read())\n",
    "  \n",
    "x,y = get_batch(indices_text,4,8)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(y.shape)\n",
    "print(y)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(3, 8)\n",
      "[[1263  791  190  360  608  127 1263 1285]\n",
      " [ 438 1170 1539 1459  462  622 1401  966]\n",
      " [1285  335 1540 1336   14 1285  485 1274]]\n",
      "(3, 8)\n",
      "[[ 791  190  360  608  127 1263 1285  413]\n",
      " [1170 1539 1459  462  622 1401  966  797]\n",
      " [ 335 1540 1336   14 1285  485 1274 1386]]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T14:53:58.977262Z",
     "start_time": "2025-07-30T14:53:58.947253Z"
    }
   },
   "source": [
    "class neural_embedding:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding_table = np.random.rand(vocab_size, vocab_size)\n",
    "\n",
    "    def calculate_softmax(self, x):\n",
    "        \"\"\"Takes input array x and returns softmax.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # Numerical stability\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "    def calculate_cross_entropy(self, y_hatless, y_hat):\n",
    "        \"\"\"\n",
    "        Takes target (y_hatless) and prediction (y_hat) and computes cross entropy loss.\n",
    "        \"\"\"\n",
    "        # get vocab_size\n",
    "        _, _, vocab_size = y_hat.shape        \n",
    "        y_hat = y_hat.reshape(y_hat.shape[0]*y_hat.shape[1], y_hat.shape[2])\n",
    "        y_hatless_flat = y_hatless.reshape(-1)\n",
    "        # one-hot encode targets\n",
    "        y_hatless_hot = np.eye(vocab_size)[y_hatless_flat]\n",
    "       \n",
    "        y_hat = self.calculate_softmax(y_hat)\n",
    "        \n",
    "        # Clip predictions to avoid log(0)\n",
    "        y_hat = np.clip(y_hat, 1e-15, 1.0) \n",
    "        return -np.sum(y_hatless_hot*np.log(y_hat))\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Implements forward pass with an unnecessary logitte function \n",
    "        which i only did not delete because now I'm emotionally attached.\n",
    "        Args:\n",
    "            idx(np.array): (B,T) numpy array of integers\n",
    "            targets(np.array): (B,T) numpy array of integers\n",
    "        Returns:\n",
    "            input_logits(np.array)\n",
    "            sometimes also: targets(np.array)\n",
    "        \"\"\"\n",
    "        batch_size, chunk_size = idx.shape\n",
    "        logits = np.zeros((batch_size, chunk_size, (self.token_embedding_table[0].size)))\n",
    "\n",
    "        def logitte(batch_size, chunk_size, input):\n",
    "            for batch in range(batch_size):\n",
    "                for chunk in range(chunk_size):\n",
    "                    # (B,T,C) b=batch_size, t=\"time\"=chunk_size, c=vocab_size\n",
    "                    logits[batch][chunk] = self.token_embedding_table[input[batch][chunk]]\n",
    "                    \n",
    "            return logits\n",
    "\n",
    "        input_logits = logitte(batch_size, chunk_size, idx)\n",
    "        \n",
    "        if targets is not None:\n",
    "            loss = self.calculate_cross_entropy(targets, input_logits)\n",
    "\n",
    "            return input_logits, loss\n",
    "\n",
    "        return input_logits\n",
    "\n",
    "    \n",
    "    def backward(self, targets, input_logits):\n",
    "        batch_size = 32\n",
    "        chunk_size = 8\n",
    "        # need to do the same reshaping as we did for cross entropy, apparently\n",
    "        targets_flat = targets.reshape(-1)\n",
    "        one_hot_targets = np.eye(self.vocab_size)[targets_flat]\n",
    "\n",
    "        # shape after: ((batch_size*chunk_size), vocab_size)\n",
    "        input_logits_2d = input_logits.reshape(input_logits.shape[0]*input_logits.shape[1], input_logits.shape[2])\n",
    "        \n",
    "        soft_input = self.calculate_softmax(input_logits_2d)\n",
    "        # somehow this is supposedly the combiantion of the derivative of softmax with the derivative of the CCE\n",
    "        delta = one_hot_targets - soft_input\n",
    "\n",
    "        # want shape (80,80) for matrix multiplication, but with correct indices (use one-hot targets for that)\n",
    "        delta_indexed = np.dot(one_hot_targets.transpose(),delta)\n",
    "\n",
    "        # compute gradient for weight matrix: dot product between the transpose of the to layer and delta vector computed above\n",
    "        gradient = (self.token_embedding_table.T @ delta_indexed) \n",
    "        \n",
    "        # Calculate gradients for the embedding table\n",
    "        # gradient = np.zeros_like(self.token_embedding_table)\n",
    "        # for batch in range(batch_size):\n",
    "        #     for chunk in range(chunk_size):\n",
    "        #         gradient[idx[batch][chunk]] += delta[batch][chunk]\n",
    "    \n",
    "        return gradient\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T14:16:31.308038Z",
     "start_time": "2025-07-30T14:16:31.290415Z"
    }
   },
   "source": [
    "# currently do not use loss at all, so something is probably very wrong\n",
    "\n",
    "def train(model, text, optimiser, param_tensor, train_step):\n",
    "    batch_size=32\n",
    "    chunk_size = 8\n",
    "    for steps in range(train_step): # TODO: please increase\n",
    "        # sample batch of data\n",
    "        xb, yb = get_batch(text, batch_size, chunk_size) # TODO: pls adapt to above fct\n",
    "        optimiser.zero_grad()  # reset gradients\n",
    "        # get logits and loss\n",
    "        logits, loss = model.forward(xb, yb)\n",
    "        print(f\"loss: {loss}\")\n",
    "\n",
    "        gradient = model.backward(yb, logits)\n",
    "        param_tensor.grad = torch.tensor(gradient, dtype=torch.float32)\n",
    "        optimiser.step()  # apply gradients to parameters\n",
    "        # diff = model.token_embedding_table - np.array(token_embedding_table)\n",
    "        # print(f\"Difference in weights before - afte Adam: {diff}\")\n",
    "        #/model.token_embedding_table = np.array(token_embedding_table)\n",
    "        model.token_embedding_table = param_tensor.detach().numpy()\n",
    "        "
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T14:16:37.357596Z",
     "start_time": "2025-07-30T14:16:36.425371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "    indices_text = eval(f.read())\n",
    "with open(r\"vocab_train.txt\", 'r') as f:\n",
    "    vocab_train = eval(f.read())"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-30T14:54:05.991578Z"
    }
   },
   "source": [
    "vocab_size = len(vocab_train)\n",
    "\n",
    "my_neural_embedding = neural_embedding(vocab_size)\n",
    "param_dict = {\"weight\": my_neural_embedding.token_embedding_table} # one entry, key is weight and value is my_neural\n",
    "\n",
    "tensor = torch.tensor(my_neural_embedding.token_embedding_table, dtype=torch.float32, requires_grad=True)\n",
    "optimiser = optim.Adam([tensor], lr=0.05)\n",
    "train(my_neural_embedding, indices_text, optimiser, tensor, 1000)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1832.4722530829679\n",
      "loss: 1837.0142048636314\n",
      "loss: 1847.799516567781\n",
      "loss: 1850.9609506189688\n",
      "loss: 1866.2934735257836\n",
      "loss: 1870.8128625034346\n",
      "loss: 1888.6521947337974\n",
      "loss: 1890.2126208849595\n",
      "loss: 1905.5559745540522\n",
      "loss: 1903.7713983514702\n",
      "loss: 1907.5464550347444\n",
      "loss: 1927.697449601592\n",
      "loss: 1936.0507341627228\n",
      "loss: 1940.2029639287855\n",
      "loss: 1952.751254632537\n",
      "loss: 1953.9549965922954\n",
      "loss: 1984.148269735761\n",
      "loss: 1975.2200473542734\n",
      "loss: 1976.7226462351243\n",
      "loss: 1995.4704832340651\n",
      "loss: 1975.9882073939207\n",
      "loss: 2003.8953578192788\n",
      "loss: 1989.9240277695312\n",
      "loss: 2039.4205432643407\n",
      "loss: 2039.8186337409315\n",
      "loss: 2024.945703574818\n",
      "loss: 2039.0823401235036\n",
      "loss: 2028.5040154793755\n",
      "loss: 2047.3226478465479\n",
      "loss: 2054.0369806433537\n",
      "loss: 2056.9368607915562\n",
      "loss: 2082.417943735787\n",
      "loss: 2078.49738712342\n",
      "loss: 2077.173815666717\n",
      "loss: 2080.6102478403354\n",
      "loss: 2092.8925397629423\n",
      "loss: 2102.85166117645\n",
      "loss: 2118.072734776374\n",
      "loss: 2128.130206254763\n",
      "loss: 2121.964388087381\n",
      "loss: 2139.9516412014145\n",
      "loss: 2129.877058419392\n",
      "loss: 2098.8742127234905\n",
      "loss: 2157.384256266832\n",
      "loss: 2181.4675456476575\n",
      "loss: 2169.1176154032655\n",
      "loss: 2161.158534153844\n",
      "loss: 2182.4148491338183\n",
      "loss: 2196.5366212966105\n",
      "loss: 2237.9499439499505\n",
      "loss: 2202.801086814115\n",
      "loss: 2211.774504273175\n",
      "loss: 2232.984164945383\n",
      "loss: 2236.340306310498\n",
      "loss: 2237.4255246902276\n",
      "loss: 2283.723079780324\n",
      "loss: 2290.622290839599\n",
      "loss: 2215.403547726743\n",
      "loss: 2277.1615467783176\n",
      "loss: 2296.3065423585285\n",
      "loss: 2321.282023319362\n",
      "loss: 2299.4089907226908\n",
      "loss: 2329.4300121830415\n",
      "loss: 2329.4694851842587\n",
      "loss: 2301.0454144171135\n",
      "loss: 2322.8927702701953\n",
      "loss: 2300.543170039485\n",
      "loss: 2326.972689556863\n",
      "loss: 2340.225970694783\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

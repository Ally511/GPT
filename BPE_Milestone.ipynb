{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a9a86ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import self-written functions\"\"\"\n",
    "from utility_functions import get_words,performance,compare_to_gpt_encoding\n",
    "from BPE_function import bpe,get_best_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a50e9212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"read training and test data\"\"\"\n",
    "with open (r\"corpora/Shakespeare_clean_train.txt\", 'r') as f:\n",
    "  text_train = f.read()\n",
    "with open (r\"corpora/Shakespeare_clean_valid.txt\", 'r') as f:\n",
    "  text_valid = f.read()\n",
    "with open (r\"corpora/Shakespeare_clean_test.txt\", 'r') as f:\n",
    "  text_test = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7300fbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram split for 1-gram, k = 100: [['the_'], ['t'], ['ra'], ['g'], ['e'], ['d'], ['y_'], ['of_'], ['an'], ['t']]\n",
      "Old perplexity: 100000000.00106137\n",
      "Perplexity: 95.83\n",
      "N-gram split for 2-gram, k = 100: [['the_', 't'], ['t', 'ra'], ['ra', 'g'], ['g', 'e'], ['e', 'd'], ['d', 'y_'], ['y_', 'of_'], ['of_', 'an'], ['an', 't'], ['t', 'on']]\n",
      "Old perplexity: 31.041010729441577\n",
      "Perplexity: 30.48\n",
      "N-gram split for 3-gram, k = 100: [['the_', 't', 'ra'], ['t', 'ra', 'g'], ['ra', 'g', 'e'], ['g', 'e', 'd'], ['e', 'd', 'y_'], ['d', 'y_', 'of_'], ['y_', 'of_', 'an'], ['of_', 'an', 't'], ['an', 't', 'on'], ['t', 'on', 'y_']]\n",
      "Old perplexity: 53.91956844747087\n",
      "Perplexity: 26.83\n",
      "N-gram split for 4-gram, k = 100: [['the_', 't', 'ra', 'g'], ['t', 'ra', 'g', 'e'], ['ra', 'g', 'e', 'd'], ['g', 'e', 'd', 'y_'], ['e', 'd', 'y_', 'of_'], ['d', 'y_', 'of_', 'an'], ['y_', 'of_', 'an', 't'], ['of_', 'an', 't', 'on'], ['an', 't', 'on', 'y_'], ['t', 'on', 'y_', 'and_']]\n",
      "Old perplexity: 1655.7131932966815\n",
      "Perplexity: 79.63\n",
      "N-gram split for 1-gram, k = 200: [['the_'], ['t'], ['ra'], ['g'], ['e'], ['d'], ['y_'], ['of_'], ['anton'], ['y_']]\n",
      "Old perplexity: 100000000.00131468\n",
      "Perplexity: 158.30\n",
      "N-gram split for 2-gram, k = 200: [['the_', 't'], ['t', 'ra'], ['ra', 'g'], ['g', 'e'], ['e', 'd'], ['d', 'y_'], ['y_', 'of_'], ['of_', 'anton'], ['anton', 'y_'], ['y_', 'and_']]\n",
      "Old perplexity: 45.443633975628195\n",
      "Perplexity: 41.85\n",
      "N-gram split for 3-gram, k = 200: [['the_', 't', 'ra'], ['t', 'ra', 'g'], ['ra', 'g', 'e'], ['g', 'e', 'd'], ['e', 'd', 'y_'], ['d', 'y_', 'of_'], ['y_', 'of_', 'anton'], ['of_', 'anton', 'y_'], ['anton', 'y_', 'and_'], ['y_', 'and_', 'c']]\n",
      "Old perplexity: 404.4884327017365\n",
      "Perplexity: 65.17\n",
      "N-gram split for 4-gram, k = 200: [['the_', 't', 'ra', 'g'], ['t', 'ra', 'g', 'e'], ['ra', 'g', 'e', 'd'], ['g', 'e', 'd', 'y_'], ['e', 'd', 'y_', 'of_'], ['d', 'y_', 'of_', 'anton'], ['y_', 'of_', 'anton', 'y_'], ['of_', 'anton', 'y_', 'and_'], ['anton', 'y_', 'and_', 'c'], ['y_', 'and_', 'c', 'le']]\n",
      "Old perplexity: 23302.87804972291\n",
      "Perplexity: 160.78\n",
      "N-gram split for 1-gram, k = 300: [['the_'], ['t'], ['ra'], ['g'], ['e'], ['dy_'], ['of_'], ['antony_'], ['and_'], ['c']]\n",
      "Old perplexity: 100000000.00106244\n",
      "Perplexity: 209.87\n",
      "N-gram split for 2-gram, k = 300: [['the_', 't'], ['t', 'ra'], ['ra', 'g'], ['g', 'e'], ['e', 'dy_'], ['dy_', 'of_'], ['of_', 'antony_'], ['antony_', 'and_'], ['and_', 'c'], ['c', 'le']]\n",
      "Old perplexity: 64.62248649372009\n",
      "Perplexity: 51.44\n",
      "N-gram split for 3-gram, k = 300: [['the_', 't', 'ra'], ['t', 'ra', 'g'], ['ra', 'g', 'e'], ['g', 'e', 'dy_'], ['e', 'dy_', 'of_'], ['dy_', 'of_', 'antony_'], ['of_', 'antony_', 'and_'], ['antony_', 'and_', 'c'], ['and_', 'c', 'le'], ['c', 'le', 'op']]\n",
      "Old perplexity: 1509.4546667103627\n",
      "Perplexity: 107.95\n",
      "N-gram split for 4-gram, k = 300: [['the_', 't', 'ra', 'g'], ['t', 'ra', 'g', 'e'], ['ra', 'g', 'e', 'dy_'], ['g', 'e', 'dy_', 'of_'], ['e', 'dy_', 'of_', 'antony_'], ['dy_', 'of_', 'antony_', 'and_'], ['of_', 'antony_', 'and_', 'c'], ['antony_', 'and_', 'c', 'le'], ['and_', 'c', 'le', 'op'], ['c', 'le', 'op', 'at']]\n",
      "Old perplexity: 80682.91442746914\n",
      "Perplexity: 220.25\n",
      "N-gram split for 1-gram, k = 400: [['the_'], ['t'], ['ra'], ['g'], ['e'], ['dy_'], ['of_'], ['antony_'], ['and_'], ['cleopatra_']]\n",
      "Old perplexity: 100000000.0008713\n",
      "Perplexity: 252.13\n",
      "N-gram split for 2-gram, k = 400: [['the_', 't'], ['t', 'ra'], ['ra', 'g'], ['g', 'e'], ['e', 'dy_'], ['dy_', 'of_'], ['of_', 'antony_'], ['antony_', 'and_'], ['and_', 'cleopatra_'], ['cleopatra_', 'd']]\n",
      "Old perplexity: 88.29872664728587\n",
      "Perplexity: 61.38\n",
      "N-gram split for 3-gram, k = 400: [['the_', 't', 'ra'], ['t', 'ra', 'g'], ['ra', 'g', 'e'], ['g', 'e', 'dy_'], ['e', 'dy_', 'of_'], ['dy_', 'of_', 'antony_'], ['of_', 'antony_', 'and_'], ['antony_', 'and_', 'cleopatra_'], ['and_', 'cleopatra_', 'd'], ['cleopatra_', 'd', 'ra']]\n",
      "Old perplexity: 3757.0822394115635\n",
      "Perplexity: 154.85\n",
      "N-gram split for 4-gram, k = 400: [['the_', 't', 'ra', 'g'], ['t', 'ra', 'g', 'e'], ['ra', 'g', 'e', 'dy_'], ['g', 'e', 'dy_', 'of_'], ['e', 'dy_', 'of_', 'antony_'], ['dy_', 'of_', 'antony_', 'and_'], ['of_', 'antony_', 'and_', 'cleopatra_'], ['antony_', 'and_', 'cleopatra_', 'd'], ['and_', 'cleopatra_', 'd', 'ra'], ['cleopatra_', 'd', 'ra', 'ma']]\n",
      "Old perplexity: 177471.51961496245\n",
      "Perplexity: 279.94\n",
      "N-gram split for 1-gram, k = 500: [['the_'], ['t'], ['ra'], ['g'], ['e'], ['dy_'], ['of_'], ['antony_'], ['and_'], ['cleopatra_']]\n",
      "Old perplexity: 100000000.0007068\n",
      "Perplexity: 288.99\n",
      "N-gram split for 2-gram, k = 500: [['the_', 't'], ['t', 'ra'], ['ra', 'g'], ['g', 'e'], ['e', 'dy_'], ['dy_', 'of_'], ['of_', 'antony_'], ['antony_', 'and_'], ['and_', 'cleopatra_'], ['cleopatra_', 'd']]\n",
      "Old perplexity: 121.45166762420037\n",
      "Perplexity: 72.93\n",
      "N-gram split for 3-gram, k = 500: [['the_', 't', 'ra'], ['t', 'ra', 'g'], ['ra', 'g', 'e'], ['g', 'e', 'dy_'], ['e', 'dy_', 'of_'], ['dy_', 'of_', 'antony_'], ['of_', 'antony_', 'and_'], ['antony_', 'and_', 'cleopatra_'], ['and_', 'cleopatra_', 'd'], ['cleopatra_', 'd', 'ra']]\n",
      "Old perplexity: 7880.039482517996\n",
      "Perplexity: 205.72\n",
      "N-gram split for 4-gram, k = 500: [['the_', 't', 'ra', 'g'], ['t', 'ra', 'g', 'e'], ['ra', 'g', 'e', 'dy_'], ['g', 'e', 'dy_', 'of_'], ['e', 'dy_', 'of_', 'antony_'], ['dy_', 'of_', 'antony_', 'and_'], ['of_', 'antony_', 'and_', 'cleopatra_'], ['antony_', 'and_', 'cleopatra_', 'd'], ['and_', 'cleopatra_', 'd', 'ra'], ['cleopatra_', 'd', 'ra', 'ma']]\n",
      "Old perplexity: 319011.89695302973\n",
      "Perplexity: 335.28\n",
      "N-gram split for 1-gram, k = 600: [['the_'], ['tra'], ['ge'], ['dy_'], ['of_'], ['antony_'], ['and_'], ['cleopatra_'], ['dr'], ['a']]\n",
      "Old perplexity: 100000000.00057927\n",
      "Perplexity: 322.70\n",
      "N-gram split for 2-gram, k = 600: [['the_', 'tra'], ['tra', 'ge'], ['ge', 'dy_'], ['dy_', 'of_'], ['of_', 'antony_'], ['antony_', 'and_'], ['and_', 'cleopatra_'], ['cleopatra_', 'dr'], ['dr', 'a'], ['a', 'ma']]\n",
      "Old perplexity: 162.25140881524266\n",
      "Perplexity: 85.16\n",
      "N-gram split for 3-gram, k = 600: [['the_', 'tra', 'ge'], ['tra', 'ge', 'dy_'], ['ge', 'dy_', 'of_'], ['dy_', 'of_', 'antony_'], ['of_', 'antony_', 'and_'], ['antony_', 'and_', 'cleopatra_'], ['and_', 'cleopatra_', 'dr'], ['cleopatra_', 'dr', 'a'], ['dr', 'a', 'ma'], ['a', 'ma', 'tis_']]\n",
      "Old perplexity: 13603.1418554953\n",
      "Perplexity: 254.31\n",
      "N-gram split for 4-gram, k = 600: [['the_', 'tra', 'ge', 'dy_'], ['tra', 'ge', 'dy_', 'of_'], ['ge', 'dy_', 'of_', 'antony_'], ['dy_', 'of_', 'antony_', 'and_'], ['of_', 'antony_', 'and_', 'cleopatra_'], ['antony_', 'and_', 'cleopatra_', 'dr'], ['and_', 'cleopatra_', 'dr', 'a'], ['cleopatra_', 'dr', 'a', 'ma'], ['dr', 'a', 'ma', 'tis_'], ['a', 'ma', 'tis_', 'per']]\n",
      "Old perplexity: 486475.4220297373\n",
      "Perplexity: 388.53\n",
      "N-gram split for 1-gram, k = 700: [['the_'], ['tra'], ['ge'], ['dy_'], ['of_'], ['antony_'], ['and_'], ['cleopatra_'], ['dra'], ['ma']]\n",
      "Old perplexity: 100000000.00046131\n",
      "Perplexity: 353.02\n",
      "N-gram split for 2-gram, k = 700: [['the_', 'tra'], ['tra', 'ge'], ['ge', 'dy_'], ['dy_', 'of_'], ['of_', 'antony_'], ['antony_', 'and_'], ['and_', 'cleopatra_'], ['cleopatra_', 'dra'], ['dra', 'ma'], ['ma', 'tis_']]\n",
      "Old perplexity: 212.30167818995182\n",
      "Perplexity: 98.55\n",
      "N-gram split for 3-gram, k = 700: [['the_', 'tra', 'ge'], ['tra', 'ge', 'dy_'], ['ge', 'dy_', 'of_'], ['dy_', 'of_', 'antony_'], ['of_', 'antony_', 'and_'], ['antony_', 'and_', 'cleopatra_'], ['and_', 'cleopatra_', 'dra'], ['cleopatra_', 'dra', 'ma'], ['dra', 'ma', 'tis_'], ['ma', 'tis_', 'per']]\n",
      "Old perplexity: 21842.274997184468\n",
      "Perplexity: 306.62\n",
      "N-gram split for 4-gram, k = 700: [['the_', 'tra', 'ge', 'dy_'], ['tra', 'ge', 'dy_', 'of_'], ['ge', 'dy_', 'of_', 'antony_'], ['dy_', 'of_', 'antony_', 'and_'], ['of_', 'antony_', 'and_', 'cleopatra_'], ['antony_', 'and_', 'cleopatra_', 'dra'], ['and_', 'cleopatra_', 'dra', 'ma'], ['cleopatra_', 'dra', 'ma', 'tis_'], ['dra', 'ma', 'tis_', 'per'], ['ma', 'tis_', 'per', 'so']]\n",
      "Old perplexity: 693316.4545818109\n",
      "Perplexity: 437.31\n",
      "N-gram split for 1-gram, k = 800: [['the_'], ['tra'], ['ge'], ['dy_'], ['of_'], ['antony_'], ['and_'], ['cleopatra_'], ['dra'], ['ma']]\n",
      "Old perplexity: 100000000.00037001\n",
      "Perplexity: 377.92\n",
      "N-gram split for 2-gram, k = 800: [['the_', 'tra'], ['tra', 'ge'], ['ge', 'dy_'], ['dy_', 'of_'], ['of_', 'antony_'], ['antony_', 'and_'], ['and_', 'cleopatra_'], ['cleopatra_', 'dra'], ['dra', 'ma'], ['ma', 'tis_']]\n",
      "Old perplexity: 268.95899145096445\n",
      "Perplexity: 112.09\n",
      "N-gram split for 3-gram, k = 800: [['the_', 'tra', 'ge'], ['tra', 'ge', 'dy_'], ['ge', 'dy_', 'of_'], ['dy_', 'of_', 'antony_'], ['of_', 'antony_', 'and_'], ['antony_', 'and_', 'cleopatra_'], ['and_', 'cleopatra_', 'dra'], ['cleopatra_', 'dra', 'ma'], ['dra', 'ma', 'tis_'], ['ma', 'tis_', 'per']]\n",
      "Old perplexity: 30540.916986443233\n",
      "Perplexity: 354.01\n",
      "N-gram split for 4-gram, k = 800: [['the_', 'tra', 'ge', 'dy_'], ['tra', 'ge', 'dy_', 'of_'], ['ge', 'dy_', 'of_', 'antony_'], ['dy_', 'of_', 'antony_', 'and_'], ['of_', 'antony_', 'and_', 'cleopatra_'], ['antony_', 'and_', 'cleopatra_', 'dra'], ['and_', 'cleopatra_', 'dra', 'ma'], ['cleopatra_', 'dra', 'ma', 'tis_'], ['dra', 'ma', 'tis_', 'per'], ['ma', 'tis_', 'per', 'so']]\n",
      "Old perplexity: 863517.314609638\n",
      "Perplexity: 481.45\n",
      "N-gram split for 1-gram, k = 900: [['the_'], ['tra'], ['ge'], ['dy_'], ['of_'], ['antony_'], ['and_'], ['cleopatra_'], ['dra'], ['ma']]\n",
      "Old perplexity: 100000000.00030002\n",
      "Perplexity: 399.78\n",
      "N-gram split for 2-gram, k = 900: [['the_', 'tra'], ['tra', 'ge'], ['ge', 'dy_'], ['dy_', 'of_'], ['of_', 'antony_'], ['antony_', 'and_'], ['and_', 'cleopatra_'], ['cleopatra_', 'dra'], ['dra', 'ma'], ['ma', 'tis_']]\n",
      "Old perplexity: 326.6586386889279\n",
      "Perplexity: 124.95\n",
      "N-gram split for 3-gram, k = 900: [['the_', 'tra', 'ge'], ['tra', 'ge', 'dy_'], ['ge', 'dy_', 'of_'], ['dy_', 'of_', 'antony_'], ['of_', 'antony_', 'and_'], ['antony_', 'and_', 'cleopatra_'], ['and_', 'cleopatra_', 'dra'], ['cleopatra_', 'dra', 'ma'], ['dra', 'ma', 'tis_'], ['ma', 'tis_', 'per']]\n",
      "Old perplexity: 39695.96872024733\n",
      "Perplexity: 399.87\n",
      "N-gram split for 4-gram, k = 900: [['the_', 'tra', 'ge', 'dy_'], ['tra', 'ge', 'dy_', 'of_'], ['ge', 'dy_', 'of_', 'antony_'], ['dy_', 'of_', 'antony_', 'and_'], ['of_', 'antony_', 'and_', 'cleopatra_'], ['antony_', 'and_', 'cleopatra_', 'dra'], ['and_', 'cleopatra_', 'dra', 'ma'], ['cleopatra_', 'dra', 'ma', 'tis_'], ['dra', 'ma', 'tis_', 'per'], ['ma', 'tis_', 'per', 'so']]\n",
      "Old perplexity: 1040109.1982249514\n",
      "Perplexity: 525.73\n"
     ]
    }
   ],
   "source": [
    "with open (r\"corpora/Shakespeare_clean_valid.txt\", 'r') as f:\n",
    "  text_valid = f.read()\n",
    "  \n",
    "dict_train = get_words(text_train)\n",
    "dict_valid = get_words(text_valid)\n",
    "dict_test = get_words(text_test)\n",
    "\n",
    "best_k,best_perplexity,best_n_gram,second_best_k,second_best_perplexity,second_best_n_gram,third_best_k,third_best_perplexity,third_best_n_gram, ks, n_gram_num, perplexities = get_best_merges(dict_train,text_train,dict_valid,text_valid,1000,100)\n",
    "result = [\"best k:\",best_k,\"best n gram\", best_n_gram,\"best k perplexity:\",best_perplexity,\"2nd best k:\",second_best_k,\"2nd best n gram\", second_best_n_gram,\"2nd best k perplexity:\",second_best_perplexity,\"3rd best k:\",third_best_k,\"3rd best n gram\",third_best_n_gram,\"3rd best k perplexity:\",third_best_perplexity]\n",
    "with open(\"k_merges_n_gram.txt\", \"w\") as k_merges_n_gram:\n",
    "  for item in result:\n",
    "     k_merges_n_gram.write(f\"{item}\\n\")\n",
    "\n",
    "with open(\"k_merges_n_gram.txt\", \"r\") as f:\n",
    "    k_merges_n_gram = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6494fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train a dictionary of byte pairs and their respective counts on training and test data\"\"\"\n",
    "#extract words from the text\n",
    "dict_train = get_words(text_train)\n",
    "dict_test = get_words(text_test)\n",
    "\n",
    "best_k = int(k_merges_n_gram[1])\n",
    "second_best_k = int(k_merges_n_gram[7])\n",
    "third_best_k = int(k_merges_n_gram[13])\n",
    "\n",
    "\n",
    "#builds the byte pair vocabulary and the sorted token frequency\n",
    "vocab_train_best_k, sorted_token_freq_train_best_k, dict_matrix_train_best_k = bpe(dict_train,best_k)\n",
    "vocab_train_2nd_best_k, sorted_token_freq_train_2nd_best_k, dict_matrix_train_2nd_best_k = bpe(dict_train,second_best_k)\n",
    "vocab_train_3rd_best_k, sorted_token_freq_train_3rd_best_k, dict_matrix_train_3rd_best_k = bpe(dict_train,third_best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb2ba50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy best k:  7.3999999999999995\n",
      "train accuracy 2nd best k:  7.3999999999999995\n",
      "train accuracy 3rd best k:  13.200000000000001\n",
      "test accuracy best k:  7.199999999999999\n",
      "test accuracy 2nd best k:  7.199999999999999\n",
      "test accuracy 3rd best k:  12.8\n"
     ]
    }
   ],
   "source": [
    "\"\"\"test and print performance of training and test set\"\"\"\n",
    "train_accuracy_best_k = performance(dict_train, vocab_train_best_k, 500)\n",
    "train_accuracy_2nd_best_k = performance(dict_train, vocab_train_2nd_best_k, 500)\n",
    "train_accuracy_3rd_best_k = performance(dict_train, vocab_train_3rd_best_k, 500)\n",
    "\n",
    "test_accuracy_best_k = performance(dict_test, vocab_train_best_k, 500)\n",
    "test_accuracy_2nd_best_k = performance(dict_test, vocab_train_2nd_best_k, 500)\n",
    "test_accuracy_3rd_best_k = performance(dict_test, vocab_train_3rd_best_k, 500)\n",
    "\n",
    "print(\"train accuracy best k: \", train_accuracy_best_k)\n",
    "print(\"train accuracy 2nd best k: \", train_accuracy_2nd_best_k)\n",
    "print(\"train accuracy 3rd best k: \", train_accuracy_3rd_best_k)\n",
    "\n",
    "print(\"test accuracy best k: \", test_accuracy_best_k)\n",
    "print(\"test accuracy 2nd best k: \", test_accuracy_2nd_best_k)\n",
    "print(\"test accuracy 3rd best k: \", test_accuracy_3rd_best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af4ef0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8857142857142857\n",
      "0.8857142857142857\n",
      "0.8074866310160428\n",
      "0.638095238095238\n",
      "0.638095238095238\n",
      "0.49732620320855614\n"
     ]
    }
   ],
   "source": [
    "\"\"\"compare our BPE split to split of another system (in this case gpt 3.5)\"\"\"\n",
    "#with open (file_path, 'r') as f:\n",
    "#  text = f.read()\n",
    "model_name=\"gpt-3.5-turbo\"\n",
    "tokens_train = compare_to_gpt_encoding(text_train, vocab_train_best_k, model_name)\n",
    "tokens_train = compare_to_gpt_encoding(text_train, vocab_train_2nd_best_k, model_name)\n",
    "tokens_train = compare_to_gpt_encoding(text_train, vocab_train_3rd_best_k, model_name)\n",
    "\n",
    "tokens_test = compare_to_gpt_encoding(text_test, vocab_train_best_k, model_name)\n",
    "tokens_test = compare_to_gpt_encoding(text_test, vocab_train_2nd_best_k, model_name)\n",
    "tokens_test = compare_to_gpt_encoding(text_test, vocab_train_3rd_best_k, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b4a8362",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'corpora/friends.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"run BPE split on different (non-shakespearean) test set and evaluate performance\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcorpora/friends.txt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m   friends = f.read()\n\u001b[32m      5\u001b[39m friends_dict = get_words(friends)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'corpora/friends.txt'"
     ]
    }
   ],
   "source": [
    "\"\"\"run BPE split on different (non-shakespearean) test set and evaluate performance\"\"\"\n",
    "with open ('corpora/friends.txt', 'r') as f:\n",
    "  friends = f.read()\n",
    "\n",
    "friends_dict = get_words(friends)\n",
    "friends_accuracy = performance(friends_dict, vocab_train_best_k, 500)\n",
    "friends_accuracy = performance(friends_dict, vocab_train_2nd_best_k, 500)\n",
    "friends_accuracy = performance(friends_dict, vocab_train_3rd_best_k, 500)\n",
    "\n",
    "print(\"Accuracy on Friends: \", friends_accuracy)\n",
    "\n",
    "#also compare to gpt encoding of BPE split\n",
    "tokens_friends = compare_to_gpt_encoding(friends, vocab_train_best_k, model_name)\n",
    "tokens_friends = compare_to_gpt_encoding(friends, vocab_train_2nd_best_k, model_name)\n",
    "tokens_friends = compare_to_gpt_encoding(friends, vocab_train_3rd_best_k, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8032be2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram split for 1-gram, k = 100: [['the_'], ['t'], ['ra'], ['g'], ['e'], ['d'], ['y_'], ['of_'], ['an'], ['t']]\n",
      "Old perplexity: 100000000.00106137\n",
      "Perplexity: 95.83\n",
      "N-gram split for 2-gram, k = 100: [['the_', 't'], ['t', 'ra'], ['ra', 'g'], ['g', 'e'], ['e', 'd'], ['d', 'y_'], ['y_', 'of_'], ['of_', 'an'], ['an', 't'], ['t', 'on']]\n",
      "Old perplexity: 31.041010729441577\n",
      "Perplexity: 30.48\n",
      "N-gram split for 3-gram, k = 100: [['the_', 't', 'ra'], ['t', 'ra', 'g'], ['ra', 'g', 'e'], ['g', 'e', 'd'], ['e', 'd', 'y_'], ['d', 'y_', 'of_'], ['y_', 'of_', 'an'], ['of_', 'an', 't'], ['an', 't', 'on'], ['t', 'on', 'y_']]\n",
      "Old perplexity: 53.91956844747087\n",
      "Perplexity: 26.83\n",
      "N-gram split for 4-gram, k = 100: [['the_', 't', 'ra', 'g'], ['t', 'ra', 'g', 'e'], ['ra', 'g', 'e', 'd'], ['g', 'e', 'd', 'y_'], ['e', 'd', 'y_', 'of_'], ['d', 'y_', 'of_', 'an'], ['y_', 'of_', 'an', 't'], ['of_', 'an', 't', 'on'], ['an', 't', 'on', 'y_'], ['t', 'on', 'y_', 'and_']]\n",
      "Old perplexity: 1655.7131932966815\n",
      "Perplexity: 79.63\n",
      "N-gram split for 1-gram, k = 200: [['the_'], ['t'], ['ra'], ['g'], ['e'], ['d'], ['y_'], ['of_'], ['anton'], ['y_']]\n",
      "Old perplexity: 100000000.00131468\n",
      "Perplexity: 158.30\n",
      "N-gram split for 2-gram, k = 200: [['the_', 't'], ['t', 'ra'], ['ra', 'g'], ['g', 'e'], ['e', 'd'], ['d', 'y_'], ['y_', 'of_'], ['of_', 'anton'], ['anton', 'y_'], ['y_', 'and_']]\n",
      "Old perplexity: 45.443633975628195\n",
      "Perplexity: 41.85\n",
      "N-gram split for 3-gram, k = 200: [['the_', 't', 'ra'], ['t', 'ra', 'g'], ['ra', 'g', 'e'], ['g', 'e', 'd'], ['e', 'd', 'y_'], ['d', 'y_', 'of_'], ['y_', 'of_', 'anton'], ['of_', 'anton', 'y_'], ['anton', 'y_', 'and_'], ['y_', 'and_', 'c']]\n",
      "Old perplexity: 404.4884327017365\n",
      "Perplexity: 65.17\n",
      "N-gram split for 4-gram, k = 200: [['the_', 't', 'ra', 'g'], ['t', 'ra', 'g', 'e'], ['ra', 'g', 'e', 'd'], ['g', 'e', 'd', 'y_'], ['e', 'd', 'y_', 'of_'], ['d', 'y_', 'of_', 'anton'], ['y_', 'of_', 'anton', 'y_'], ['of_', 'anton', 'y_', 'and_'], ['anton', 'y_', 'and_', 'c'], ['y_', 'and_', 'c', 'le']]\n",
      "Old perplexity: 23302.87804972291\n",
      "Perplexity: 160.78\n",
      "N-gram split for 1-gram, k = 300: [['the_'], ['t'], ['ra'], ['g'], ['e'], ['dy_'], ['of_'], ['antony_'], ['and_'], ['c']]\n",
      "Old perplexity: 100000000.00106244\n",
      "Perplexity: 209.87\n",
      "N-gram split for 2-gram, k = 300: [['the_', 't'], ['t', 'ra'], ['ra', 'g'], ['g', 'e'], ['e', 'dy_'], ['dy_', 'of_'], ['of_', 'antony_'], ['antony_', 'and_'], ['and_', 'c'], ['c', 'le']]\n",
      "Old perplexity: 64.62248649372009\n",
      "Perplexity: 51.44\n",
      "N-gram split for 3-gram, k = 300: [['the_', 't', 'ra'], ['t', 'ra', 'g'], ['ra', 'g', 'e'], ['g', 'e', 'dy_'], ['e', 'dy_', 'of_'], ['dy_', 'of_', 'antony_'], ['of_', 'antony_', 'and_'], ['antony_', 'and_', 'c'], ['and_', 'c', 'le'], ['c', 'le', 'op']]\n",
      "Old perplexity: 1509.4546667103627\n",
      "Perplexity: 107.95\n",
      "N-gram split for 4-gram, k = 300: [['the_', 't', 'ra', 'g'], ['t', 'ra', 'g', 'e'], ['ra', 'g', 'e', 'dy_'], ['g', 'e', 'dy_', 'of_'], ['e', 'dy_', 'of_', 'antony_'], ['dy_', 'of_', 'antony_', 'and_'], ['of_', 'antony_', 'and_', 'c'], ['antony_', 'and_', 'c', 'le'], ['and_', 'c', 'le', 'op'], ['c', 'le', 'op', 'at']]\n",
      "Old perplexity: 80682.91442746914\n",
      "Perplexity: 220.25\n",
      "N-gram split for 1-gram, k = 400: [['the_'], ['t'], ['ra'], ['g'], ['e'], ['dy_'], ['of_'], ['antony_'], ['and_'], ['cleopatra_']]\n",
      "Old perplexity: 100000000.0008713\n",
      "Perplexity: 252.13\n",
      "N-gram split for 2-gram, k = 400: [['the_', 't'], ['t', 'ra'], ['ra', 'g'], ['g', 'e'], ['e', 'dy_'], ['dy_', 'of_'], ['of_', 'antony_'], ['antony_', 'and_'], ['and_', 'cleopatra_'], ['cleopatra_', 'd']]\n",
      "Old perplexity: 88.29872664728587\n",
      "Perplexity: 61.38\n",
      "N-gram split for 3-gram, k = 400: [['the_', 't', 'ra'], ['t', 'ra', 'g'], ['ra', 'g', 'e'], ['g', 'e', 'dy_'], ['e', 'dy_', 'of_'], ['dy_', 'of_', 'antony_'], ['of_', 'antony_', 'and_'], ['antony_', 'and_', 'cleopatra_'], ['and_', 'cleopatra_', 'd'], ['cleopatra_', 'd', 'ra']]\n",
      "Old perplexity: 3757.0822394115635\n",
      "Perplexity: 154.85\n",
      "N-gram split for 4-gram, k = 400: [['the_', 't', 'ra', 'g'], ['t', 'ra', 'g', 'e'], ['ra', 'g', 'e', 'dy_'], ['g', 'e', 'dy_', 'of_'], ['e', 'dy_', 'of_', 'antony_'], ['dy_', 'of_', 'antony_', 'and_'], ['of_', 'antony_', 'and_', 'cleopatra_'], ['antony_', 'and_', 'cleopatra_', 'd'], ['and_', 'cleopatra_', 'd', 'ra'], ['cleopatra_', 'd', 'ra', 'ma']]\n",
      "Old perplexity: 177471.51961496245\n",
      "Perplexity: 279.94\n",
      "N-gram split for 1-gram, k = 500: [['the_'], ['t'], ['ra'], ['g'], ['e'], ['dy_'], ['of_'], ['antony_'], ['and_'], ['cleopatra_']]\n",
      "Old perplexity: 100000000.0007068\n",
      "Perplexity: 288.99\n",
      "N-gram split for 2-gram, k = 500: [['the_', 't'], ['t', 'ra'], ['ra', 'g'], ['g', 'e'], ['e', 'dy_'], ['dy_', 'of_'], ['of_', 'antony_'], ['antony_', 'and_'], ['and_', 'cleopatra_'], ['cleopatra_', 'd']]\n",
      "Old perplexity: 121.45166762420037\n",
      "Perplexity: 72.93\n",
      "N-gram split for 3-gram, k = 500: [['the_', 't', 'ra'], ['t', 'ra', 'g'], ['ra', 'g', 'e'], ['g', 'e', 'dy_'], ['e', 'dy_', 'of_'], ['dy_', 'of_', 'antony_'], ['of_', 'antony_', 'and_'], ['antony_', 'and_', 'cleopatra_'], ['and_', 'cleopatra_', 'd'], ['cleopatra_', 'd', 'ra']]\n",
      "Old perplexity: 7880.039482517996\n",
      "Perplexity: 205.72\n",
      "N-gram split for 4-gram, k = 500: [['the_', 't', 'ra', 'g'], ['t', 'ra', 'g', 'e'], ['ra', 'g', 'e', 'dy_'], ['g', 'e', 'dy_', 'of_'], ['e', 'dy_', 'of_', 'antony_'], ['dy_', 'of_', 'antony_', 'and_'], ['of_', 'antony_', 'and_', 'cleopatra_'], ['antony_', 'and_', 'cleopatra_', 'd'], ['and_', 'cleopatra_', 'd', 'ra'], ['cleopatra_', 'd', 'ra', 'ma']]\n",
      "Old perplexity: 319011.89695302973\n",
      "Perplexity: 335.28\n",
      "N-gram split for 1-gram, k = 600: [['the_'], ['tra'], ['ge'], ['dy_'], ['of_'], ['antony_'], ['and_'], ['cleopatra_'], ['dr'], ['a']]\n",
      "Old perplexity: 100000000.00057927\n",
      "Perplexity: 322.70\n",
      "N-gram split for 2-gram, k = 600: [['the_', 'tra'], ['tra', 'ge'], ['ge', 'dy_'], ['dy_', 'of_'], ['of_', 'antony_'], ['antony_', 'and_'], ['and_', 'cleopatra_'], ['cleopatra_', 'dr'], ['dr', 'a'], ['a', 'ma']]\n",
      "Old perplexity: 162.25140881524266\n",
      "Perplexity: 85.16\n",
      "N-gram split for 3-gram, k = 600: [['the_', 'tra', 'ge'], ['tra', 'ge', 'dy_'], ['ge', 'dy_', 'of_'], ['dy_', 'of_', 'antony_'], ['of_', 'antony_', 'and_'], ['antony_', 'and_', 'cleopatra_'], ['and_', 'cleopatra_', 'dr'], ['cleopatra_', 'dr', 'a'], ['dr', 'a', 'ma'], ['a', 'ma', 'tis_']]\n",
      "Old perplexity: 13603.1418554953\n",
      "Perplexity: 254.31\n",
      "N-gram split for 4-gram, k = 600: [['the_', 'tra', 'ge', 'dy_'], ['tra', 'ge', 'dy_', 'of_'], ['ge', 'dy_', 'of_', 'antony_'], ['dy_', 'of_', 'antony_', 'and_'], ['of_', 'antony_', 'and_', 'cleopatra_'], ['antony_', 'and_', 'cleopatra_', 'dr'], ['and_', 'cleopatra_', 'dr', 'a'], ['cleopatra_', 'dr', 'a', 'ma'], ['dr', 'a', 'ma', 'tis_'], ['a', 'ma', 'tis_', 'per']]\n",
      "Old perplexity: 486475.4220297373\n",
      "Perplexity: 388.53\n",
      "N-gram split for 1-gram, k = 700: [['the_'], ['tra'], ['ge'], ['dy_'], ['of_'], ['antony_'], ['and_'], ['cleopatra_'], ['dra'], ['ma']]\n",
      "Old perplexity: 100000000.00046131\n",
      "Perplexity: 353.02\n",
      "N-gram split for 2-gram, k = 700: [['the_', 'tra'], ['tra', 'ge'], ['ge', 'dy_'], ['dy_', 'of_'], ['of_', 'antony_'], ['antony_', 'and_'], ['and_', 'cleopatra_'], ['cleopatra_', 'dra'], ['dra', 'ma'], ['ma', 'tis_']]\n",
      "Old perplexity: 212.30167818995182\n",
      "Perplexity: 98.55\n",
      "N-gram split for 3-gram, k = 700: [['the_', 'tra', 'ge'], ['tra', 'ge', 'dy_'], ['ge', 'dy_', 'of_'], ['dy_', 'of_', 'antony_'], ['of_', 'antony_', 'and_'], ['antony_', 'and_', 'cleopatra_'], ['and_', 'cleopatra_', 'dra'], ['cleopatra_', 'dra', 'ma'], ['dra', 'ma', 'tis_'], ['ma', 'tis_', 'per']]\n",
      "Old perplexity: 21842.274997184468\n",
      "Perplexity: 306.62\n",
      "N-gram split for 4-gram, k = 700: [['the_', 'tra', 'ge', 'dy_'], ['tra', 'ge', 'dy_', 'of_'], ['ge', 'dy_', 'of_', 'antony_'], ['dy_', 'of_', 'antony_', 'and_'], ['of_', 'antony_', 'and_', 'cleopatra_'], ['antony_', 'and_', 'cleopatra_', 'dra'], ['and_', 'cleopatra_', 'dra', 'ma'], ['cleopatra_', 'dra', 'ma', 'tis_'], ['dra', 'ma', 'tis_', 'per'], ['ma', 'tis_', 'per', 'so']]\n",
      "Old perplexity: 693316.4545818109\n",
      "Perplexity: 437.31\n",
      "N-gram split for 1-gram, k = 800: [['the_'], ['tra'], ['ge'], ['dy_'], ['of_'], ['antony_'], ['and_'], ['cleopatra_'], ['dra'], ['ma']]\n",
      "Old perplexity: 100000000.00037001\n",
      "Perplexity: 377.92\n",
      "N-gram split for 2-gram, k = 800: [['the_', 'tra'], ['tra', 'ge'], ['ge', 'dy_'], ['dy_', 'of_'], ['of_', 'antony_'], ['antony_', 'and_'], ['and_', 'cleopatra_'], ['cleopatra_', 'dra'], ['dra', 'ma'], ['ma', 'tis_']]\n",
      "Old perplexity: 268.95899145096445\n",
      "Perplexity: 112.09\n",
      "N-gram split for 3-gram, k = 800: [['the_', 'tra', 'ge'], ['tra', 'ge', 'dy_'], ['ge', 'dy_', 'of_'], ['dy_', 'of_', 'antony_'], ['of_', 'antony_', 'and_'], ['antony_', 'and_', 'cleopatra_'], ['and_', 'cleopatra_', 'dra'], ['cleopatra_', 'dra', 'ma'], ['dra', 'ma', 'tis_'], ['ma', 'tis_', 'per']]\n",
      "Old perplexity: 30540.916986443233\n",
      "Perplexity: 354.01\n",
      "N-gram split for 4-gram, k = 800: [['the_', 'tra', 'ge', 'dy_'], ['tra', 'ge', 'dy_', 'of_'], ['ge', 'dy_', 'of_', 'antony_'], ['dy_', 'of_', 'antony_', 'and_'], ['of_', 'antony_', 'and_', 'cleopatra_'], ['antony_', 'and_', 'cleopatra_', 'dra'], ['and_', 'cleopatra_', 'dra', 'ma'], ['cleopatra_', 'dra', 'ma', 'tis_'], ['dra', 'ma', 'tis_', 'per'], ['ma', 'tis_', 'per', 'so']]\n",
      "Old perplexity: 863517.314609638\n",
      "Perplexity: 481.45\n",
      "N-gram split for 1-gram, k = 900: [['the_'], ['tra'], ['ge'], ['dy_'], ['of_'], ['antony_'], ['and_'], ['cleopatra_'], ['dra'], ['ma']]\n",
      "Old perplexity: 100000000.00030002\n",
      "Perplexity: 399.78\n",
      "N-gram split for 2-gram, k = 900: [['the_', 'tra'], ['tra', 'ge'], ['ge', 'dy_'], ['dy_', 'of_'], ['of_', 'antony_'], ['antony_', 'and_'], ['and_', 'cleopatra_'], ['cleopatra_', 'dra'], ['dra', 'ma'], ['ma', 'tis_']]\n",
      "Old perplexity: 326.6586386889279\n",
      "Perplexity: 124.95\n",
      "N-gram split for 3-gram, k = 900: [['the_', 'tra', 'ge'], ['tra', 'ge', 'dy_'], ['ge', 'dy_', 'of_'], ['dy_', 'of_', 'antony_'], ['of_', 'antony_', 'and_'], ['antony_', 'and_', 'cleopatra_'], ['and_', 'cleopatra_', 'dra'], ['cleopatra_', 'dra', 'ma'], ['dra', 'ma', 'tis_'], ['ma', 'tis_', 'per']]\n",
      "Old perplexity: 39695.96872024733\n",
      "Perplexity: 399.87\n",
      "N-gram split for 4-gram, k = 900: [['the_', 'tra', 'ge', 'dy_'], ['tra', 'ge', 'dy_', 'of_'], ['ge', 'dy_', 'of_', 'antony_'], ['dy_', 'of_', 'antony_', 'and_'], ['of_', 'antony_', 'and_', 'cleopatra_'], ['antony_', 'and_', 'cleopatra_', 'dra'], ['and_', 'cleopatra_', 'dra', 'ma'], ['cleopatra_', 'dra', 'ma', 'tis_'], ['dra', 'ma', 'tis_', 'per'], ['ma', 'tis_', 'per', 'so']]\n",
      "Old perplexity: 1040109.1982249514\n",
      "Perplexity: 525.73\n"
     ]
    }
   ],
   "source": [
    "with open (r\"corpora/Shakespeare_clean_valid.txt\", 'r') as f:\n",
    "  text_valid = f.read()\n",
    "  \n",
    "dict_train = get_words(text_train)\n",
    "dict_valid = get_words(text_valid)\n",
    "dict_test = get_words(text_test)\n",
    "\n",
    "best_k,best_perplexity,best_n_gram,second_best_k,second_best_perplexity,second_best_n_gram,third_best_k,third_best_perplexity,third_best_n_gram = get_best_merges(dict_train,text_train,dict_valid,text_valid,1000,100)\n",
    "result = [\"best k:\",best_k,\"best n gram\", best_n_gram,\"best k perplexity:\",best_perplexity,\"2nd best k:\",second_best_k,\"2nd best n gram\", second_best_n_gram,\"2nd best k perplexity:\",second_best_perplexity,\"3rd best k:\",third_best_k,\"3rd best n gram\",third_best_n_gram,\"3rd best k perplexity:\",third_best_perplexity]\n",
    "with open(\"k_merges_n_gram.txt\", \"w\") as k_merges_n_gram:\n",
    "  for item in result:\n",
    "     k_merges_n_gram.write(f\"{item}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

"""utility

contains helper functions:
- get words: extracts words from a text and returns a sorted dictionary of word counts
- performance: calculates the percentage of overlap between a sorted dictionary of words and a vocabulary set   
- compare_to_gpt_encoding: compares the vocabulary with the tokens generated by a GPT model
- get_top_bigrams: extracts the top k bigrams from a dictionary of n-gram probabilities
- generate_n_grams: generates n n-grams from a given corpus
- get_batch: returns slices of a given text in shape (batch_size, chunk_size), if needed with a target batch
- decode_characters: returns the decoded output of an NLP model
"""

import nltk
from collections import Counter
import tiktoken
import numpy as np
from itertools import islice
from n_gram import N_gram
from generator import to_byte_pair

def get_words(text):
    """
    Tokenizes text into words, counts their frequencies, and returns a sorted dictionary.

    Uses a custom regular expression tokenizer to split the text into words, abbreviations,
    numbers, punctuation, and special tokens. Converts all tokens to lowercase, counts occurrences,
    and returns a dictionary sorted in descending order by frequency.

    @:param
    text (str): input text to be tokenized
    @:returns
    sorted_dict (dict): a dictionary, keys: unique tokens, values: counts, sorted by frequency in descending order
    """

    pattern = r'''(?x)          # set flag to allow verbose regexps
              (?:[A-Z]\.)+        # abbreviations, e.g. U.S.A.
              | \w+'\w+           # contractions
            | \w+(?:-\w+)*        # words with optional internal hyphens
            | \$?\d+(?:\.\d+)?%?  # currency and percentages, e.g. $12.40, 82%
            | \.\.\.              # ellipsis
            | [][.,;"'?():_`-]    # these are separate tokens; includes ], [
          '''

    vocab = nltk.regexp_tokenize(text, pattern)
    vocab = [word.lower() for word in vocab]
    unique_words = set(vocab)
    c = Counter(vocab)

    sorted_dict = {key: value for key, value in sorted(
    c.items(), key=lambda item: item[1], reverse=True)}

    return sorted_dict

def generate_n_grams(n_gram_corpus, n):
    """
    Generates up to n n-grams from the given corpus.
    For each n from 1 to 'n', creates an N-gram object using the provided corpus.
    @:param
    n_gram_corpus (list or str): the corpus from which to generate n-grams
    n (int): maximum n-grame size
    @:return
    list_of_n_grams (list): a list containing N-gram objects for n in range [1,n]

    """
    list_of_n_grams = []
    for i in range (0,n):
        list_of_n_grams.append(N_gram(n_gram_corpus, i+1))

    return list_of_n_grams
        
   

def performance(sorted_dict, vocab, k=10000):
    """
    Calculates the percentage of overlap between the top-k most frequent words
    and a given vocabulary set.

    @:param
    sorted_dict (dict): A frequency dictionary sorted by frequency (output of `get_words`).
    vocab (set or iterable): The vocabulary set to compare against.
    k (int): Number of top frequent words to consider (default is 10000)

    @:returns
    percentage (float): percentage of overlap between the top-k frequent words and the vocabulary

    """

    def take(n, iterable):
        """Return the first n items of the iterable as a list."""
        return list(islice(iterable, n))


    # extract top k keys
    qualifier = [key for key, _ in take(k, sorted_dict.items())]

    # clean vocab by stripping the _
    cleaned_vocab = {word.rstrip('_') for word in vocab}

    # calculate overlap
    overlap = set(qualifier) & cleaned_vocab
    num_overlap = len(overlap)

    # percentage
    percentage = (num_overlap / len(qualifier)) * 100
    return percentage



def compare_to_gpt_encoding(text, alphabet, model_name="gpt-3.5-turbo"):
    """
    Encodes a text string using tiktoken and prints the tokens along with their IDs.

    @:param
    text (str): The text string to encode.
    alphabet (set or iterable): the vocabulary to compare against
    model_name (str): GPT model used for encoding. Falls back to 'cl100k_base' if unavailable. Default is "gpt-3.5-turbo".

    """
    alphabet = {word.rstrip('_') for word in alphabet}
    try:
        encoding = tiktoken.encoding_for_model(model_name)
    except KeyError:
        print(f"Warning: Model '{model_name}' not found.  Using 'cl100k_base' instead.")
        encoding = tiktoken.get_encoding("cl100k_base")

    tokens = encoding.encode(text)
    tp = 0

    unique_tokens = set(tokens)
    for token in unique_tokens:
      decoded_token = encoding.decode([token])
      #check if the decoded token is in the dict alphabet
      if(decoded_token in alphabet):
        tp = tp+1
    total_len = len(alphabet)
    print(tp/total_len)
    

def get_top_bigrams(n_gram_probs, k=5):
    """
    Prints the top-k most probable bigrams from an n-gram probability dictionary.

    @:param
    n_gram_probs (dict): A dictionary mapping contexts (tuples) to dictionaries of next words and probabilities.
    k (int): Number of bigrams to return, default is 5.
    """
    bigram_list = []
    for context, next_words in n_gram_probs.items():
        for word, prob in next_words.items():
            bigram = context + (word,)
            bigram_list.append((bigram, prob))

    # Sort by probability (descending)
    bigram_list.sort(key=lambda x: x[1], reverse=True)

    # Print top k
    print(f"Top {k} most probable bigrams:")
    for i in range(min(k, len(bigram_list))):
        bigram, prob = bigram_list[i]
        print(f"{bigram} â†’ {prob:.4f}")


def get_batch(input, batch_size, chunk_size):
    """ Creates random batches of sequential text chunks and their shifted targets.
    @:param
    input (list or np.ndarray): tokenized text data as a sequence of integers
    batch_size (int): size of each batch
    chunk_size (int): length of each sequence that is contained in the batch

    @:returns
    input_batch (np.ndarray): input sequences of shape (batch_size, chunk_size)
    target_batch (np.ndarray): target sequences are the input sequences shifted by one position with shape (batch_size, chunk_size)

    """
    input_batch = []
    target_batch = []
    idx = np.random.randint(0, len(input) - (chunk_size + 1), size=batch_size)
    for i in range(0, len(idx)):
        input_batch.append(input[idx[i]:idx[i] + chunk_size])
        target_batch.append(input[idx[i] + 1:idx[i] + (chunk_size + 1)])

    input_batch = np.array(input_batch)
    target_batch = np.array(target_batch)

    return input_batch, target_batch

def decode_characters(input, vocab_train):
    """Decodes a list of indices back to their corresponding characters
    given the above defined vocabulary

    @:param
    input (list or np.ndarray): a list of token IDs to decode
    vocab_train (list): the vocabulary list containing the tokens

    @:returns
    decoded (str): the decoded string
    """
    vocab = vocab_train
    indices = np.arange(0, len(vocab), 1)
    inidces = indices.astype(int)
    indices = indices.tolist()
    key_byte = dict(zip(vocab, indices))
    value_byte = dict(zip(indices, vocab))

    decoded = [] #given the input, we will decode it back to characters
    for i in range(0,len(input)):
        decoded.append(value_byte[input[i]])#using the translation dctionary: value_byte
#make its prettier by joining list to actual words and replacing underscores with spaces
    decoded = ''.join(decoded)
    decoded = decoded.replace('_', ' ')
    return decoded


def find_top_indices(list, n):
    # Create a copy of the list
    sorted_lst = list.copy()
    
    # Sort the list in descending order
    sorted_lst.sort(reverse=False)
    
    # Get the indices of the top N values
    top_indices = [list.index(value) for value in sorted_lst[:n]]
    
    return top_indices


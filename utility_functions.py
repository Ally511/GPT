"""utility

contains helper functions:
- get words: extracts words from a text and returns a sorted dictionary of word counts
- performance: calculates the percentage of overlap between a sorted dictionary of words and a vocabulary set   
- compare_to_gpt_encoding: compares the vocabulary with the tokens generated by a GPT model
- get_top_bigrams: extracts the top k bigrams from a dictionary of n-gram probabilities
- generate_n_grams: generates n n-grams from a given corpus
"""

import nltk
from collections import Counter
import tiktoken
import numpy as np
from itertools import islice
from n_gram import N_gram

def get_words(text):

  pattern = r'''(?x)          # set flag to allow verbose regexps
          (?:[A-Z]\.)+        # abbreviations, e.g. U.S.A.
          | \w+'\w+           # contractions
        | \w+(?:-\w+)*        # words with optional internal hyphens
        | \$?\d+(?:\.\d+)?%?  # currency and percentages, e.g. $12.40, 82%
        | \.\.\.              # ellipsis
        | [][.,;"'?():_`-]    # these are separate tokens; includes ], [
      '''

  vocab = nltk.regexp_tokenize(text, pattern)
  vocab = [word.lower() for word in vocab]
  unique_words = set(vocab)
  c = Counter(vocab)

  sorted_dict = {key: value for key, value in sorted(
      c.items(), key=lambda item: item[1], reverse=True)}

  return sorted_dict

def generate_n_grams(n_gram_corpus, n):
    """
    Generates up to n n-grams from the given corpus.
    """
    list_of_n_grams = []
    for i in range (0,n):
        list_of_n_grams.append(N_gram(n_gram_corpus, i+1))

    return list_of_n_grams
        
   

def performance(sorted_dict, vocab, k=10000):

    def take(n, iterable):
        """Return the first n items of the iterable as a list."""
        return list(islice(iterable, n))


    # extract top k keys
    qualifier = [key for key, _ in take(k, sorted_dict.items())]

    # clean vocab by stripping the _
    cleaned_vocab = {word.rstrip('_') for word in vocab}

    # calculate overlap
    overlap = set(qualifier) & cleaned_vocab
    num_overlap = len(overlap)

    # percentage
    percentage = (num_overlap / len(qualifier)) * 100
    return percentage



def compare_to_gpt_encoding(text, alphabet, model_name="gpt-3.5-turbo"):
    """
    Encodes a text string using tiktoken and prints the tokens along with their IDs.

    Args:
        text: The text string to encode.

    """
    alphabet = {word.rstrip('_') for word in alphabet}
    try:
        encoding = tiktoken.encoding_for_model(model_name)
    except KeyError:
        print(f"Warning: Model '{model_name}' not found.  Using 'cl100k_base' instead.")
        encoding = tiktoken.get_encoding("cl100k_base")

    tokens = encoding.encode(text)
    tp = 0

    unique_tokens = set(tokens)
    for token in unique_tokens:
      decoded_token = encoding.decode([token])
      #check if the decoded token is in the dict alphabet
      if(decoded_token in alphabet):
        tp = tp+1
    total_len = len(alphabet)
    print(tp/total_len)
    

def get_top_bigrams(n_gram_probs, k=5):
    bigram_list = []
    for context, next_words in n_gram_probs.items():
        for word, prob in next_words.items():
            bigram = context + (word,)
            bigram_list.append((bigram, prob))

    # Sort by probability (descending)
    bigram_list.sort(key=lambda x: x[1], reverse=True)

    # Print top k
    print(f"Top {k} most probable bigrams:")
    for i in range(min(k, len(bigram_list))):
        bigram, prob = bigram_list[i]
        print(f"{bigram} â†’ {prob:.4f}")


def get_batch(input, batch_size, chunk_size):
    """ splits the input text into batches of chunks, returns input and target batches"""
    input_batch = []
    target_batch = []
    idx = np.random.randint(0, len(input) - (chunk_size + 1), size=batch_size)
    for i in range(0, len(idx)):
        input_batch.append(input[idx[i]:idx[i] + chunk_size])
        target_batch.append(input[idx[i] + 1:idx[i] + (chunk_size + 1)])

    input_batch = np.array(input_batch)
    target_batch = np.array(target_batch)

    return input_batch, target_batch

def decode_characters(input, vocab_train):
    """Decodes a list of indices back to their corresponding characters
    given the abive defined vocabulary"""
    vocab = vocab_train
    indices = np.arange(0, len(vocab), 1)
    inidces = indices.astype(int)
    indices = indices.tolist()
    key_byte = dict(zip(vocab, indices))
    value_byte = dict(zip(indices, vocab))

    decoded = [] #given the input, we will decode it back to characters
    for i in range(0,len(input)):
        decoded.append(value_byte[input[i]])#using the translation dctionary: value_byte
#make its prettier by joining list to actual words and replacing underscores with spaces
    decoded = ''.join(decoded)
    decoded = decoded.replace('_', ' ')
    return decoded


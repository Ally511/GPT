{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:50:53.809092Z",
     "start_time": "2025-07-30T12:50:53.788786Z"
    }
   },
   "outputs": [],
   "source": [
    "# import necessary imports\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:29:35.898826Z",
     "start_time": "2025-07-30T13:29:35.883687Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(input, batch_size, chunk_size):\n",
    "    \"\"\" splits the input text into batches of chunks, returns input and target batches\"\"\"\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    idx = np.random.randint(0,len(input)-(chunk_size+1),size=batch_size)\n",
    "    for i in range(0,len(idx)-1):\n",
    "        input_batch.append(input[idx[i]:idx[i]+chunk_size])\n",
    "        target_batch.append(input[idx[i]+1:idx[i]+(chunk_size+1)])\n",
    "    \n",
    "    input_batch = np.array(input_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Code for decoding \"\"\"\n",
    "with open (r\"vocab_train.txt\", 'r') as f:\n",
    "   vocab_train = eval(f.read())\n",
    "\n",
    "vocab = vocab_train\n",
    "indices = np.arange(0,len(vocab),1)\n",
    "inidces = indices.astype(int)\n",
    "indices = indices.tolist()\n",
    "key_byte = dict(zip(vocab, indices))\n",
    "value_byte = dict(zip(indices,vocab))\n",
    "\n",
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "  indices_text = eval(f.read())\n",
    "\n",
    "\n",
    "def decode_characters(input):\n",
    "    \"\"\"Decodes a list of indices back to their corresponding characters\n",
    "    given the abive defined vocabulary\"\"\"\n",
    "\n",
    "    decoded = [] #given the input, we will decode it back to characters\n",
    "    for i in range(0,len(input)):\n",
    "        decoded.append(value_byte[input[i]]) #using the translation dctionary: value_byte\n",
    "\n",
    "    #makes it prettier by joining list to actual words and replacing underscores with spaces\n",
    "    decoded = ''.join(decoded)\n",
    "    decoded = decoded.replace('_', ' ')\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T15:02:06.858697Z",
     "start_time": "2025-07-30T15:02:06.828752Z"
    }
   },
   "outputs": [],
   "source": [
    "class neural_embedding:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding_table = np.random.rand(vocab_size, vocab_size) # weights\n",
    "\n",
    "\n",
    "    def calculate_softmax(self, x):\n",
    "        \"\"\"\n",
    "        Takes input array x and returns softmax along the last axis (axis=-1).\n",
    "        Works for arrays of any shape: (a, b, c) will softmax along c.\n",
    "        \"\"\"\n",
    "        softmax = np.zeros_like(x)\n",
    "        dima, dimb, _ = x.shape\n",
    "        for a in range(dima):\n",
    "            for b in range(dimb):\n",
    "                # subtract max to prevent overflow\n",
    "                exps = np.exp(x[a][b] - np.max(x[a][b]))\n",
    "                # calculate softmax\n",
    "                softmax[a][b] = exps / np.sum(exps)\n",
    "        return softmax\n",
    "\n",
    "\n",
    "    def calculate_cross_entropy(self, target, y_hat):\n",
    "        \"\"\"\n",
    "        Takes target (y_hatless) and prediction (y_hat) and computes cross entropy loss.\n",
    "        \"\"\"\n",
    "        # get vocab_size\n",
    "        _, _, vocab_size = y_hat.shape   \n",
    "        y_hat = self.calculate_softmax(y_hat)\n",
    "        y_hat_flat = y_hat.reshape(-1, vocab_size)\n",
    "        target_flat = target.reshape(-1)\n",
    "        # one-hot encode targets\n",
    "        target_hot = np.eye(vocab_size)[target_flat]\n",
    "                \n",
    "        # Clip predictions to avoid log(0)\n",
    "        y_hat_flat = np.clip(y_hat_flat, 1e-15, 1.0) \n",
    "        loss = -np.sum(target_hot*np.log(y_hat_flat))\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Implements forward pass with an unnecessary logitte function \n",
    "        which i only did not delete because now I'm emotionally attached.\n",
    "        Args:\n",
    "            idx(np.array): (B,T) numpy array of integers\n",
    "            targets(np.array): (B,T) numpy array of integers\n",
    "        Returns:\n",
    "            input_logits(np.array)\n",
    "            sometimes also: targets(np.array)\n",
    "        \"\"\"\n",
    "        batch_size, chunk_size = idx.shape\n",
    "\n",
    "        #initalize logits\n",
    "        input_logits = np.zeros((batch_size, chunk_size, (self.token_embedding_table[0].size)))\n",
    "\n",
    "        for batch in range(batch_size):\n",
    "            for chunk in range(chunk_size):\n",
    "                # (B,T,C) b=batch_size, t=\"time\"=chunk_size, c=vocab_size\n",
    "                input_logits[batch][chunk] = self.token_embedding_table[idx[batch][chunk]]\n",
    "                    \n",
    "        if targets is not None:\n",
    "            loss = self.calculate_cross_entropy(targets, input_logits) #if the forward function is called in the training, calculate the loss\n",
    "            return input_logits, loss\n",
    "\n",
    "        return input_logits\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "            \"\"\" generates max_new_tokens new words based on the embedding \"\"\"\n",
    "            collect = idx.tolist()\n",
    "            for _ in range(max_new_tokens):\n",
    "                logits = self.forward(idx)\n",
    "                probs = self.calculate_softmax(logits)\n",
    "                probs = probs[:, -1, :]  # take the last time step\n",
    "                idx_next = np.random.multinomial(1, probs[0])\n",
    "                idx_next = np.where(idx_next==1)\n",
    "                idx[0][:-1] = idx[0][1:]  # shift the array to the left\n",
    "                idx[0][-1] = idx_next[0]\n",
    "                collect[0].append(idx_next[0][0].tolist())\n",
    "\n",
    "            return collect\n",
    "    \n",
    "    def backward(self, input, targets, input_logits):\n",
    "        \"\"\" backward step, calculates the error signal and returns the corresponding gradient\"\"\"\n",
    "    \n",
    "        # need to do the same reshaping as we did for cross entropy\n",
    "        targets_flat = targets.reshape(-1)\n",
    "        inputs_flat = input.reshape(-1)\n",
    "        one_hot_targets = np.eye(self.vocab_size)[targets_flat]\n",
    "        one_hot_inputs =np.eye(self.vocab_size)[inputs_flat]\n",
    "\n",
    "        # reshape to B*C, T\n",
    "        soft_input = self.calculate_softmax(input_logits) # WE ARE UNSURE IF SOFTMAX IS NEEDED HERE\n",
    "        soft_input = soft_input.reshape(soft_input.shape[0]*soft_input.shape[1], soft_input.shape[2])\n",
    "        \n",
    "        # derivation of softmax & crossentropy\n",
    "        delta = soft_input - one_hot_targets\n",
    "        \n",
    "        # want shape (vocab_size, vocab_size) for matrix multiplication, but with correct indices (use one-hot inputs for that)\n",
    "        delta_indexed = np.dot(one_hot_inputs.transpose(),delta)\n",
    "\n",
    "        # compute gradient for weight matrix: dot product between the transpose of the to layer and delta vector computed above\n",
    "        gradient  =   delta_indexed @ self.token_embedding_table.T # WE ARE UNSURE ABOUT WHETHER THIS STEP IS NECESSARY, AND IF THE ORDER OF THE MATMULT + TRANSPOSE IS CORRECT\n",
    "    \n",
    "        return gradient\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T14:16:31.308038Z",
     "start_time": "2025-07-30T14:16:31.290415Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, text,  train_step):\n",
    "    \"\"\" train step, performs a forward step and then passes the loss to the backward step, adapts the embedding table\"\"\"\n",
    "    batch_size = 32\n",
    "    chunk_size = 8\n",
    "    for steps in range(train_step): \n",
    "        # sample batch of data\n",
    "        xb, yb = get_batch(text, batch_size, chunk_size) \n",
    "\n",
    "        # get logits and loss\n",
    "        logits, loss = model.forward(xb, yb)\n",
    "        print(f\"loss: {loss}\")\n",
    "\n",
    "        # pass the logits to get the gradient\n",
    "        gradient = model.backward(xb, yb, logits)\n",
    "        \n",
    "        # parameter update using simple SGD\n",
    "        # we tried using Adam before (numpy and torch version), did not improve our results\n",
    "        model.token_embedding_table = model.token_embedding_table - (gradient  * 0.01) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T15:05:54.724449Z",
     "start_time": "2025-07-30T15:02:12.007409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1830.2738349921983\n",
      "loss: 1827.3886082660943\n",
      "loss: 1824.4981434389665\n",
      "loss: 1832.345920136286\n",
      "loss: 1834.096204555259\n"
     ]
    }
   ],
   "source": [
    "# the vocab size is the length of our vocab list\n",
    "vocab_size = len(vocab_train)\n",
    "\n",
    "# initialize our model\n",
    "my_neural_embedding = neural_embedding(vocab_size)\n",
    "\n",
    "\n",
    "# train the model\n",
    "train(my_neural_embedding, indices_text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "therefore polonius ;staresar ses ghts ows ard eydely\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alice\\AppData\\Local\\Temp\\ipykernel_41380\\2535527461.py:77: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  idx[0][-1] = idx_next[0]\n"
     ]
    }
   ],
   "source": [
    "our_output = my_neural_embedding.generate(np.array([[1,5]]), max_new_tokens=10)\n",
    "\n",
    "decoded = decode_characters(our_output[0])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.232298692286718\n",
      "[[-0.06349684 -0.03597639  0.00543459 -0.08721032]\n",
      " [-0.07104957  0.0362245   0.09139943  0.08982507]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.13295831 -0.035175   -0.09017632  0.10902295]]\n",
      "[[0.90634968 0.98359764 0.77945654 0.98872103]\n",
      " [0.90710496 0.96637755 0.96086006 0.89101749]\n",
      " [0.8        0.78       0.88       0.7       ]\n",
      " [0.97670417 0.9835175  0.79901763 0.78909771]]\n",
      "4.226572934422442\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = neural_embedding(4)\n",
    "# model.token_embedding_table = np.array([[0.1, 0.1, 0.1, 0.1], \n",
    "#                                       [0.5, 0.6, 0.7, 0.8], \n",
    "#                                       [0.9, 0.2, 0.1, 0.2], \n",
    "#                                       [0.3, 0.4, 0.5, 0.6]])\n",
    "model.token_embedding_table = np.array([[0.9, 0.98, 0.78, 0.98], \n",
    "                                      [0.9, 0.97, 0.97, 0.9], \n",
    "                                      [0.8, 0.78, 0.88, 0.7], \n",
    "                                      [0.99, 0.98, 0.79, 0.8]])\n",
    "\n",
    " \n",
    "logits, loss = model.forward(np.array([[0,1,3]]), np.array([[1,3,2]]))\n",
    "print(loss)\n",
    "gradient = model.backward(np.array([[0,1,3]]), np.array([[1,3,2]]), logits)\n",
    "print(gradient)\n",
    "model.token_embedding_table = model.token_embedding_table - (gradient  * 0.1)\n",
    "print(model.token_embedding_table)\n",
    "logits, loss = model.forward(np.array([[0,1,3]]), np.array([[1,3,2]]))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.array([[0.1, 0.2, 0.3], \n",
    "                   [0.2, 0.3, 0.4],\n",
    "                    [0.3, 0.4,0.5]])\n",
    "\n",
    "delta = np.array([[0.3, 0.3, -0.3], \n",
    "                   [0, 0, 0],\n",
    "                    [0.1, -0.1,0.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3,  0. ,  0.1],\n",
       "       [ 0.3,  0. , -0.1],\n",
       "       [-0.3,  0. ,  0.1]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.33066907e-18, 0.00000000e+00, 2.00000000e-02],\n",
       "       [3.00000000e-02, 0.00000000e+00, 3.00000000e-02],\n",
       "       [6.00000000e-02, 0.00000000e+00, 4.00000000e-02]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight @ delta.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Space-based Tokenization**"
      ],
      "metadata": {
        "id": "GBuW6PAhwzuc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Thi_XrGvwNwY",
        "outputId": "e4e2cdfd-838b-4e97-ae4b-de5a8e3b4907"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'shakespeare.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-3331932571.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'shakespeare.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'shakespeare.txt'"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "with open ('shakespeare.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "\n",
        "\n",
        "pattern = r'''(?x)          # set flag to allow verbose regexps\n",
        "        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
        "        | \\w+'\\w+           # contractions\n",
        "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
        "      | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
        "      | \\.\\.\\.              # ellipsis\n",
        "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
        "    '''\n",
        "vocab = nltk.regexp_tokenize(text, pattern)\n",
        "vocab = [word.lower() for word in vocab]\n",
        "print(vocab)\n",
        "unique_words = set(vocab)\n",
        "print(unique_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "from collections import Counter\n",
        "\n",
        "c = Counter(vocab)\n",
        "\n",
        "sorted_dict = {key: value for key, value in sorted(\n",
        "    c.items(), key=lambda item: item[1], reverse=True)}\n",
        "print( sorted_dict.items() )"
      ],
      "metadata": {
        "id": "-4BGh0Xs-_en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_dict"
      ],
      "metadata": {
        "id": "6lp56h6WOey6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Byte Pair Encoding\n",
        "\n",
        "- make Shakespeare into test 1% and train\n",
        "- train the segmenter with varying k (try normalization strategies)\n",
        "- compare the performance against a different set\n",
        "- figure out a measure for accuracy"
      ],
      "metadata": {
        "id": "1Q8eYYkAw6GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "from collections import OrderedDict\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def bpe(dictionary, k):\n",
        "  # just a test!\n",
        "  # initialise empty dictionary to I count adjacent tokens, not individual ones\n",
        "  #word_freqs = defaultdict(int)\n",
        "\n",
        "  # get all unique characters in original corpus\n",
        "  all_keys = \"_ \".join(dictionary.keys())\n",
        "  vocab_bpe = list(set(all_keys))\n",
        "\n",
        "  # Corpus/dictionary in einzelne tokens splitten, nach jedem Wort (VOR space!) \"_\" einfügen\n",
        "    # worte als list of characters\n",
        "  dict_matrix = []\n",
        "  for key, value in dictionary.items():\n",
        "    new_key = list(f\"{str(key)}_ \")\n",
        "    dict_matrix.append([new_key, value])\n",
        "\n",
        "\n",
        "  # NICHT corpus, sondern liste an Wörtern in einzelne tokens splitten,\n",
        "  # >> jede occurence mit counts der Worte multiplizieren\n",
        "  token_freq = defaultdict(int)\n",
        "\n",
        "  for iteration in range(k):\n",
        "    for token_list, value in dict_matrix:\n",
        "      for i in range(len(token_list)-2):\n",
        "        # wollen den und den nächsten token als key\n",
        "        search_key = token_list[i] + token_list[i+1]\n",
        "        # zu dictionary hinzufügen falls key noch nicht existiert\n",
        "        token_freq[search_key] += value\n",
        "    # word_freqs: gehen jede existierende Folge aus zwei tokens in list of words von vorne bis hinten durch\n",
        "\n",
        "    c = Counter(token_freq)\n",
        "\n",
        "    sorted_token_freq = {key: value for key, value in sorted(\n",
        "        c.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "    # Find the most frequent token not already in vocab_bpe\n",
        "    for token in sorted_token_freq.keys():\n",
        "      if token not in vocab_bpe:\n",
        "        first_token = token\n",
        "        break\n",
        "      else:\n",
        "        first_token = None  # Optional: fallback in case all tokens are already in vocab\n",
        "\n",
        "    if first_token:\n",
        "      vocab_bpe.append(first_token)\n",
        "      print(\"New token added:\", first_token)\n",
        "    else:\n",
        "      print(\"No new token to add.\")\n",
        "\n",
        "    # höchster count wird gemerged:\n",
        "    # add to vocab\n",
        "    #first_token = list(sorted_token_freq.keys())[0]\n",
        "    #vocab_bpe.append(first_token)\n",
        "    # replace in list of words\n",
        "    # start again?\n",
        "    for i in range(len(dict_matrix)):\n",
        "        token_list, value = dict_matrix[i]\n",
        "        j = 0\n",
        "        while j < len(token_list) - 1:\n",
        "            search_key = token_list[j] + token_list[j + 1]\n",
        "            if search_key == first_token:\n",
        "                merged_token = search_key\n",
        "                # Merge the tokens\n",
        "                token_list = token_list[:j] + [merged_token] + token_list[j + 2:]\n",
        "                # Don't increment j — might be able to merge again\n",
        "            else:\n",
        "                j += 1\n",
        "        dict_matrix[i][0] = token_list\n",
        "\n",
        "\n",
        "\n",
        "  return vocab_bpe"
      ],
      "metadata": {
        "id": "yoNRHio7P54p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe(sorted_dict, 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21xd4mS4QyOO",
        "outputId": "bb44a6cd-19ad-4e21-8ca4-c1c2e30ac912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New token added: zd\n",
            "New token added: gy\n",
            "New token added: d_\n",
            "New token added: zdg\n",
            "New token added: zdgy\n",
            "New token added: xy\n",
            "New token added: g_\n",
            "New token added: zdgyd_\n",
            "New token added: zdg_\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['y',\n",
              " 'z',\n",
              " '_',\n",
              " 'g',\n",
              " 'x',\n",
              " ' ',\n",
              " 'd',\n",
              " 'zd',\n",
              " 'gy',\n",
              " 'd_',\n",
              " 'zdg',\n",
              " 'zdgy',\n",
              " 'xy',\n",
              " 'g_',\n",
              " 'zdgyd_',\n",
              " 'zdg_']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from collections import OrderedDict\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "sorted_dict = {\"xyzdg\": 2, \"zdgyd\": 4}\n",
        "all_keys = \"\".join(sorted_dict.keys())\n",
        "print(all_keys)\n",
        "vocab_bpe = list(set(all_keys))\n",
        "print(vocab_bpe)\n",
        "\n",
        "#make matrix out of dictionary\n",
        "dict_matrix = []\n",
        "for key, value in sorted_dict.items():\n",
        "  new_key = list(f\"{str(key)}_ \")\n",
        "  dict_matrix.append([new_key, value])\n",
        "\n",
        "print(dict_matrix)\n",
        "\n",
        "token_freq = defaultdict(int)\n",
        "for token_list, value in dict_matrix:\n",
        "  for i in range(len(token_list)-2):\n",
        "    # wollen den und den nächsten token als key\n",
        "    search_key = token_list[i] + token_list[i+1]\n",
        "    # zu dictionary hinzufügen falls key noch nicht existiert\n",
        "    token_freq[search_key] += value\n",
        "\n",
        "\n",
        "print(token_freq.items())\n",
        "\n",
        "c = Counter(token_freq)\n",
        "\n",
        "sorted_token_freq = {key: value for key, value in sorted(\n",
        "    c.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "print(sorted_token_freq.items())\n",
        "print(list(sorted_token_freq.keys())[0])\n",
        "\n",
        "first_token = list(sorted_token_freq.keys())[0]\n",
        "\n",
        "vocab_bpe.append(first_token)\n",
        "print(vocab_bpe)\n",
        "\n",
        "for i in range(len(dict_matrix)):\n",
        "    token_list, value = dict_matrix[i]\n",
        "    j = 0\n",
        "    while j < len(token_list) - 1:\n",
        "        search_key = token_list[j] + token_list[j + 1]\n",
        "        if search_key == first_token:\n",
        "            merged_token = search_key\n",
        "            # Merge the tokens\n",
        "            token_list = token_list[:j] + [merged_token] + token_list[j + 2:]\n",
        "            # Don't increment j — might be able to merge again\n",
        "        else:\n",
        "            j += 1\n",
        "    dict_matrix[i][0] = token_list\n",
        "\n",
        "\"\"\"for token_list, value in dict_matrix:\n",
        "  for i in range(len(token_list)-2):\n",
        "    # wollen den und den nächsten token als key\n",
        "    search_key = token_list[i] + token_list[i+1]\n",
        "    if search_key == first_token:\n",
        "      new_token = [\"\".join([token_list[i], token_list[i+1]])]\n",
        "      token_list = token_list[:i] + new_token + token_list[i+1:]\n",
        "      print(new_token)\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Dict matrix: {dict_matrix}\")"
      ],
      "metadata": {
        "id": "fF0_35OOVIrt",
        "outputId": "d85efb20-3a4f-4a03-d34f-777685763363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xyzdgzdgyd\n",
            "['y', 'z', 'g', 'x', 'd']\n",
            "[[['x', 'y', 'z', 'd', 'g', '_', ' '], 2], [['z', 'd', 'g', 'y', 'd', '_', ' '], 4]]\n",
            "dict_items([('xy', 2), ('yz', 2), ('zd', 6), ('dg', 6), ('g_', 2), ('gy', 4), ('yd', 4), ('d_', 4)])\n",
            "dict_items([('zd', 6), ('dg', 6), ('gy', 4), ('yd', 4), ('d_', 4), ('xy', 2), ('yz', 2), ('g_', 2)])\n",
            "zd\n",
            "['y', 'z', 'g', 'x', 'd', 'zd']\n",
            "Dict matrix: [[['x', 'y', 'zd', 'g', '_', ' '], 2], [['zd', 'g', 'y', 'd', '_', ' '], 4]]\n"
          ]
        }
      ]
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8980ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c859fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SPLIT THE TEXT INTO TRAIN AND TEST SET\"\"\"\n",
    "with open('corpora/indices_text.txt', 'r') as f:\n",
    "    text = eval(f.read())\n",
    "\n",
    "text = torch.tensor(text, dtype=torch.long)\n",
    "n = int(0.9*len(text))\n",
    "train_data = text[:n]\n",
    "val_data = text[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce2488b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, block_size = 8, batch_size = 128):\n",
    "    \"\"\"\n",
    "    Generates a batch of input and target sequences for training or validation.\n",
    "\n",
    "    Args:\n",
    "        split (str): Either 'train' or 'val', determines which dataset to use.\n",
    "\n",
    "    Returns:\n",
    "        x (torch.Tensor): Batch of input sequences of shape (batch_size, block_size).\n",
    "        y (torch.Tensor): Batch of target sequences (next tokens) of shape (batch_size, block_size).\n",
    "    \"\"\"\n",
    "    # Select the appropriate dataset based on the split\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Randomly choose starting indices for each sequence in the batch\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # Create input sequences of length block_size\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    # Create target sequences by shifting input by one position\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769c720f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "batch = get_batch(\"train\", block_size = 8, batch_size = 128)\n",
    "print(len(batch[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d9ea268",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (r\"corpora/Shakespeare_byte.txt\", 'r') as f:\n",
    "   shakespeare_byte_train = eval(f.read())\n",
    "\n",
    "with open('corpora/vocab_train.txt', 'r') as f:\n",
    "    vocab = eval(f.read())\n",
    "\n",
    "indices = np.arange(0,len(vocab),1)\n",
    "inidces = indices.astype(int).tolist()\n",
    "value_byte = dict(zip(indices,vocab))\n",
    "\n",
    "def decode_characters(input):\n",
    "    \"\"\"\n",
    "    Decodes a list of integer indices into their corresponding characters using a predefined vocabulary mapping.\n",
    "\n",
    "    Args:\n",
    "        input (list of int): A list of integer indices representing encoded characters.\n",
    "\n",
    "    Returns:\n",
    "        str: The decoded string, where underscores ('_') are replaced with spaces.\n",
    "\n",
    "    Notes:\n",
    "        - The function relies on a global dictionary `value_byte` that maps indices to characters.\n",
    "        - The decoded characters are joined into a single string.\n",
    "        - Underscores in the decoded string are replaced with spaces for readability.\n",
    "    \"\"\"\n",
    "    decoded = [] #given the input, we will decode it back to characters\n",
    "    for i in range(0,len(input)):\n",
    "        decoded.append(value_byte[input[i]])#using the translation dctionary: value_byte\n",
    "    # make it prettier by joining list to actual words and replacing underscores with spaces\n",
    "    decoded = ''.join(decoded)\n",
    "    decoded = decoded.replace('_', ' ')\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9a789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple bigram language model implemented using PyTorch.\n",
    "    This model learns a (vocab_size x vocab_size) embedding matrix that predicts the next token\n",
    "    given the current token, using a bigram approach.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"\n",
    "        Initializes the BigramLanguageModel.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Initialize the embedding matrix with small random values centered around 0\n",
    "        limit = 1 / np.sqrt(vocab_size)\n",
    "        self.token_embedding_table = (\n",
    "            (np.random.rand(vocab_size, vocab_size).astype(np.float32) * 2 - 1) * limit\n",
    "        )\n",
    "        # Convert the embedding matrix to a torch.nn.Parameter for optimization\n",
    "        self.embedding_param = torch.nn.Parameter(torch.from_numpy(self.token_embedding_table))\n",
    "        self.embedding_param.data = self.embedding_param.data.float()\n",
    "\n",
    "        # AdamW optimizer for parameter updates\n",
    "        self.optimizer = torch.optim.AdamW([self.embedding_param], lr=1e-2)\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # save loss for later reference\n",
    "        self.current_loss = 0\n",
    "        self.val_loss = 0\n",
    "\n",
    "    def calculate_softmax(self, x):\n",
    "        \"\"\"\n",
    "        Computes the softmax of the input tensor along the last dimension.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Softmax probabilities.\n",
    "        \"\"\"\n",
    "        exp_x = torch.exp(x - torch.max(x, dim=-1, keepdim=True).values)\n",
    "        return exp_x / torch.sum(exp_x, dim=-1, keepdim=True)\n",
    "\n",
    "    def calculate_cross_entropy(self, y_hatless, y_hat):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss between the target and predicted logits.\n",
    "\n",
    "        Args:\n",
    "            y_hatless (torch.Tensor): Target tensor of shape (batch, seq_len).\n",
    "            y_hat (torch.Tensor): Predicted logits of shape (batch, seq_len, vocab_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Scalar loss value.\n",
    "        \"\"\"\n",
    "        # Get vocab size from logits shape\n",
    "        _, _, vocab_size = y_hat.shape\n",
    "        # Flatten logits and targets for loss computation\n",
    "        y_hat = y_hat.reshape(y_hat.shape[0] * y_hat.shape[1], y_hat.shape[2])\n",
    "        y_hatless_flat = y_hatless.reshape(-1).long()\n",
    "        # One-hot encode targets\n",
    "        y_hatless_hot = torch.eye(vocab_size)[y_hatless_flat]\n",
    "        # Compute softmax and log probabilities\n",
    "        y_hat = self.calculate_softmax(y_hat)\n",
    "        y_hat = torch.log(y_hat)\n",
    "        # Compute mean cross-entropy loss\n",
    "        return torch.mean(-torch.sum(y_hatless_hot * y_hat, dim=1))\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            idx (torch.Tensor): Input indices of shape (batch, seq_len).\n",
    "            targets (torch.Tensor, optional): Target indices for loss computation.\n",
    "\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Output logits of shape (batch, seq_len, vocab_size).\n",
    "            loss (torch.Tensor, optional): Cross-entropy loss if targets are provided.\n",
    "        \"\"\"\n",
    "        # Lookup embeddings for input indices\n",
    "        logits = self.embedding_param[idx]\n",
    "        if targets is not None:\n",
    "            # Compute loss if targets are provided\n",
    "            loss = self.calculate_cross_entropy(targets, logits)\n",
    "            return logits, loss\n",
    "        return logits\n",
    "\n",
    "    def backward(self, inputs, targets, input_logits):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the loss with respect to the embedding parameters.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Input indices of shape (batch, seq_len).\n",
    "            targets (torch.Tensor): Target indices of shape (batch, seq_len).\n",
    "            input_logits (torch.Tensor): Logits from the forward pass.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Gradient tensor for the embedding parameters.\n",
    "        \"\"\"\n",
    "        # Flatten inputs and targets\n",
    "        targets_flat = targets.reshape(-1)\n",
    "        inputs_flat = inputs.reshape(-1)\n",
    "        # One-hot encode inputs and targets\n",
    "        one_hot_targets = torch.eye(self.vocab_size, dtype=torch.float32)[targets_flat]\n",
    "        one_hot_inputs = torch.eye(self.vocab_size, dtype=torch.float32)[inputs_flat]\n",
    "        # Compute softmax probabilities for logits\n",
    "        soft_input = self.calculate_softmax(input_logits).float()\n",
    "        soft_input = soft_input.reshape(soft_input.shape[0] * soft_input.shape[1], soft_input.shape[2])\n",
    "        # Compute delta for gradient (softmax output - one-hot targets)\n",
    "        delta = soft_input - one_hot_targets\n",
    "        # Compute gradient for the embedding matrix using matrix multiplication\n",
    "        delta_indexed = one_hot_inputs.T @ delta\n",
    "        gradient = delta_indexed\n",
    "        return gradient\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generates a sequence of tokens from the model, given a starting index.\n",
    "\n",
    "        Args:\n",
    "            idx (torch.Tensor): Starting token indices of shape (batch, 1).\n",
    "            max_new_tokens (int): Number of new tokens to generate.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Generated sequence of token indices.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get logits for current sequence\n",
    "            logits = self(idx)\n",
    "            logits = logits[:, -1, :]  # Use last token's logits\n",
    "            # Compute probabilities and sample next token\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append new token to sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "    def train(self, training_steps, validation_steps, max_pacience, blocksize, batchsize):\n",
    "        \"\"\"\n",
    "        Trains the model for a specified number of steps.\n",
    "\n",
    "        Args:\n",
    "            steps (int): Number of training steps.\n",
    "        \"\"\"\n",
    "        pacience = 0\n",
    "        for step in range(training_steps):\n",
    "            # Get a batch of data\n",
    "            xb, yb = get_batch('train', blocksize, batchsize)\n",
    "            # Forward pass\n",
    "            logits, current_loss = self.forward(xb, yb)\n",
    "            self.current_loss = (self.current_loss + current_loss)\n",
    "            # Zero gradients\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            # Compute gradients manually\n",
    "            gradient = self.backward(xb, yb, logits)\n",
    "            self.embedding_param.grad = torch.tensor(gradient, dtype=torch.float32)\n",
    "            # Update parameters\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if(step % validation_steps == 0):\n",
    "                val_loss = 0\n",
    "                \n",
    "                for _ in range( validation_steps//10 ):\n",
    "                    xb, yb = get_batch('val', blocksize, batchsize)\n",
    "                    _, loss = self.forward(xb, yb)\n",
    "                    val_loss = val_loss + loss\n",
    "                val_loss = val_loss / (validation_steps //10)\n",
    "                self.current_loss = self.current_loss / validation_steps\n",
    "                print(f\"loss {self.current_loss.item()} val_loss {val_loss} \")\n",
    "                \n",
    "                if(self.val_loss < val_loss):\n",
    "                    pacience = pacience + 1\n",
    "                else: pacience = 0\n",
    "                if(pacience == max_pacience):\n",
    "                    break\n",
    "                self.val_loss = val_loss\n",
    "                self.current_loss = 0\n",
    "\n",
    "                # plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
    "                # sns.heatmap(self.embedding_param.data) # Use seaborn for a nicer heatmap\n",
    "                # plt.show()\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "178fc94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model( validation_steps = 500, pacience =  5, blocksize = 32, batchsize = 64):   \n",
    "    # train the model and save the embedding\n",
    "    m = BigramLanguageModel(len(vocab))\n",
    "    training_steps = 1000000\n",
    "    m.train(training_steps, validation_steps, pacience, blocksize, batchsize)\n",
    "    trained_embedding = m.embedding_param.data\n",
    "    loss = m.current_loss\n",
    "    val_loss = m.val_loss\n",
    "\n",
    "    # plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
    "    # sns.heatmap(trained_embedding) # Use seaborn for a nicer heatmap\n",
    "    # plt.show()\n",
    "\n",
    "    # check if less than k models have been saved or if the loss is better\n",
    "    k = 5\n",
    "    save_dir = 'n_grams'\n",
    "    pattern = re.compile(r'trained_embedding_loss_([0-9.]+)\\.npy')\n",
    "    existing_files = [f for f in os.listdir(save_dir) if pattern.match(f)]\n",
    "\n",
    "    # Extract losses from filenames\n",
    "    losses = []\n",
    "    for fname in existing_files:\n",
    "        match = pattern.match(fname)\n",
    "        if match:\n",
    "            try:\n",
    "                losses.append(float(match.group(1)))\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    save_embedding = False\n",
    "    if len(existing_files) < k: #less than k models saved\n",
    "        save_embedding = True\n",
    "    elif losses and loss < max(losses): # lower loss\n",
    "        max_loss = max(losses)\n",
    "        max_loss_file = [f for f in existing_files if f'trained_embedding_loss_{max_loss}' in f]\n",
    "        for f in max_loss_file:\n",
    "            os.remove(os.path.join(save_dir, f))\n",
    "        save_embedding = True\n",
    "    if save_embedding:\n",
    "        np.save(os.path.join(save_dir, f'trained_embedding_loss_{loss}.npy'), trained_embedding)\n",
    "    \n",
    "    return loss, val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724f36f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "default_validation_steps = 500\n",
    "default_pacience =  5\n",
    "default_blocksize = 8\n",
    "default_seqsize = 64\n",
    "\n",
    "validation_steps = [100, 250, 500]\n",
    "patience =  [2,5]\n",
    "blocksize = [ 32, 64, 8, 12]\n",
    "seqsize = [256, 32,64,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f6a962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9370/2970167.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embedding_param.grad = torch.tensor(gradient, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.014679783023893833 val_loss 7.336759567260742 \n"
     ]
    }
   ],
   "source": [
    "# Vary blocksize\n",
    "for b in blocksize:\n",
    "    loss, val = eval_model(validation_steps=default_validation_steps, pacience=default_pacience, blocksize=b, batchsize=default_seqsize)\n",
    "    print(f\"blocksize {b}, loss {loss}, val_loss {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746236e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary seqsize\n",
    "for bs in seqsize:\n",
    "    loss, val = eval_model(validation_steps=default_validation_steps, pacience=default_pacience, blocksize=default_blocksize, batchsize=bs)\n",
    "    print(f\"seqsize {bs}, loss {loss}, val_loss {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b4310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vary validation_steps\n",
    "for v in validation_steps:\n",
    "    loss, val = eval_model(validation_steps=v, pacience=default_pacience, blocksize=default_blocksize, batchsize=default_seqsize)\n",
    "    print(f\"validation_steps {v}, loss {loss}, val_loss {val}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74824bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary patience\n",
    "for p in patience:\n",
    "    loss, val = eval_model(validation_steps=default_validation_steps, pacience=p, blocksize=default_blocksize, batchsize=default_seqsize)\n",
    "    print(f\"patience {p}, loss {loss}, val_loss {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f8743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0998c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.load(f\"n_grams/trained_embedding_loss_4.072439670562744.npy\")\n",
    "m = BigramLanguageModel(len(vocab))\n",
    "m.embedding_param.data = torch.from_numpy(embedding).float()\n",
    "plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
    "sns.heatmap(m.embedding_param.data) # Use seaborn for a nicer heatmap\n",
    "plt.show()\n",
    "                \n",
    "# generate a sentence\n",
    "starting_character = torch.zeros((1,1), dtype=torch.long)\n",
    "generated_characters = m.generate(idx = starting_character, max_new_tokens=40)\n",
    "generated_characters = generated_characters[0].tolist()\n",
    "print(decode_characters(generated_characters))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d893d62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(vocab))\n",
    "\n",
    "\n",
    "# # Map each token in shakespeare_byte_train to its index using key_byte\n",
    "# indices_translation = [key_byte[token] for token in shakespeare_byte_train if token in key_byte]\n",
    "\n",
    "# with open('corpora/indices_text.txt', 'w') as indices_text:\n",
    "#     indices_text.write(str(indices_translation))\n",
    "\n",
    "# with open (r\"corpora/indices_text.txt\", 'r') as f:\n",
    "#   indices_text = eval(f.read())\n",
    "\n",
    "\n",
    "# bytes_translation = [value_byte[token] for token in indices_text if token in value_byte]\n",
    "\n",
    "# with open('corpora/bytes_text.txt', 'w') as bytes_text:\n",
    "#     bytes_text.write(str(bytes_translation))\n",
    "\n",
    "# decoded_characters =decode_characters(generated_characters)\n",
    "# print(decoded_characters)\n",
    "\n",
    "\n",
    "# generated_characters = m.generate(idx = starting_character, max_new_tokens=100)\n",
    "# generated_characters = generated_characters[0].tolist()\n",
    "# print(decode_characters(generated_characters))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

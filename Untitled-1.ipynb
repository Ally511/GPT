{
 "cells": [
  {
   "cell_type": "code",
   "id": "f45d93c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:27:08.710298Z",
     "start_time": "2025-07-30T13:27:08.482220Z"
    }
   },
   "source": [
    "import numpy as np\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "99cf4e51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:27:37.855822Z",
     "start_time": "2025-07-30T13:27:37.835044Z"
    }
   },
   "source": [
    "def get_batch(input, batch_size,chunk_size):\n",
    "\n",
    "    input_batch = []\n",
    "    print(type(input_batch))\n",
    "    target_batch = []\n",
    "    idx = np.random.randint(0,len(input)-(chunk_size+1),size=batch_size)\n",
    "    for i in range(0,len(idx)-1):\n",
    "        input_batch.append(input[idx[i]:idx[i]+chunk_size])\n",
    "        target_batch.append(input[idx[i]+1:idx[i]+(chunk_size+1)])\n",
    "    \n",
    "    input_batch = np.array(input_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    return input_batch, target_batch\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09e8ee26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:27:13.453623Z",
     "start_time": "2025-07-30T13:27:12.674455Z"
    }
   },
   "source": [
    "with open (r\"Shakespeare_byte.txt\", 'r') as f:\n",
    "  shakespeare_byte_train = eval(f.read())\n",
    "\n",
    "with open (\"vocab_train.txt\", 'r') as f:\n",
    "   vocab_train = eval(f.read())\n",
    "\n",
    "vocab = vocab_train\n",
    "indices = np.arange(0,len(vocab),1)\n",
    "# inidces = indices.astype(int)\n",
    "indices = indices.tolist()\n",
    "key_byte = dict(zip(vocab, indices))\n",
    "value_byte = dict(zip(indices,vocab))\n",
    "\n",
    "# Map each token in shakespeare_byte_train to its index using key_byte\n",
    "indices_translation = [key_byte[token] for token in shakespeare_byte_train if token in key_byte]\n",
    "\n",
    "with open('indices_text.txt', 'w') as indices_text:\n",
    "    indices_text.write(str(indices_translation))\n",
    "\n",
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "  indices_text = eval(f.read())\n",
    "\n",
    "\n",
    "bytes_translation = [value_byte[token] for token in indices_text if token in value_byte]\n",
    "\n",
    "with open('bytes_text.txt', 'w') as bytes_text:\n",
    "    bytes_text.write(str(bytes_translation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36e0deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(3, 8)\n",
      "[[ 791  179  411 1263 1363 1484  442  180]\n",
      " [ 211  480  781  787  181  440  432   81]\n",
      " [1414 1292  481  439  438 1403  489 1403]]\n",
      "(3, 8)\n",
      "[[ 179  411 1263 1363 1484  442  180 1348]\n",
      " [ 480  781  787  181  440  432   81  785]\n",
      " [1292  481  439  438 1403  489 1403  489]]\n"
     ]
    }
   },
   "source": [
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "  indices_text = eval(f.read())\n",
    "  \n",
    "x,y = get_batch(indices_text,4,8)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da53f0d3",
   "metadata": {},
   "source": [
    "\n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self.forward(idx)\n",
    "            logits = logits[:, -1, :]  # take the last time step\n",
    "            probs = self.calculate_softmax(logits)\n",
    "            idx_next = np.random.multinomial(1, probs, replacement=True)\n",
    "            idx.append(idx_next)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed91e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_characters(input):\n",
    " \"\"\"Decodes a list of indices back to their corresponding characters\n",
    " given the abive defined vocabulary\"\"\"\n",
    "\n",
    " #given the input, we will decode it back to characters\n",
    "    decoded = []\n",
    "    for i in range(0,len(input)):\n",
    "        decoded.append(value_byte[input[i]])#using the translation dctionary: value_byte\n",
    "#make its prettier by joining list to actual words and replacing underscores with spaces\n",
    "    decoded = ''.join(decoded)\n",
    "    decoded = decoded.replace('_', ' ')\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46e507b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tragedy of antony and cleopatra dramatis personae mark antony octavius caesar m \n"
     ]
    }
   ],
   "source": [
    "text1 = [438, 1006, 1434, 853, 797, 27, 439, 1, 1024, 1130, 781, 886, 1184, 1519, 1259, 221, 27, 5, 25, 1294]\n",
    "\n",
    "decoded = decode_characters(text1)\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db365023",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BPE function\n",
    "\n",
    "function to create byte pairs based off a dictionary of tokens and their counts.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "\n",
    "from utility_functions import performance\n",
    "\n",
    "def bpe(dictionary, k=None):\n",
    "  \"\"\"\n",
    "  Input:\n",
    "    dictionary (dict): a dictionary that contains the tokens and their respective counts\n",
    "  return:\n",
    "    vocab_bpe (list): vocabulary of the corpus\n",
    "    sorted_token_freq:\n",
    "    dict_matrix:\n",
    "  \"\"\"\n",
    "\n",
    "  # get all unique characters in original corpus\n",
    "  all_keys = \"_ \".join(dictionary.keys())\n",
    "  vocab_bpe = list(set(all_keys))\n",
    "\n",
    "  # Corpus/dictionary in einzelne tokens splitten, nach jedem Wort (VOR space!) \"_\" einfügen\n",
    "    # worte als list of characters\n",
    "  dict_matrix = []\n",
    "  for key, value in dictionary.items():\n",
    "    new_key = list(f\"{str(key)}_ \")\n",
    "    dict_matrix.append([new_key, value])\n",
    "\n",
    "\n",
    "  # NICHT corpus, sondern liste an Wörtern in einzelne tokens splitten,\n",
    "  # >> jede occurence mit counts der Worte multiplizieren\n",
    "\n",
    "\n",
    "  token_freq = defaultdict(int) # TODO: does that need to be moved outside of the loop?\n",
    "  iteration = True\n",
    "  num_rounds = 0\n",
    "  while iteration:\n",
    "\n",
    "    for token_list, value in dict_matrix:\n",
    "      for i in range(len(token_list)-2):\n",
    "        # wollen den und den nächsten token als key\n",
    "        search_key = token_list[i] + token_list[i+1]\n",
    "        # zu dictionary hinzufügen falls key noch nicht existiert\n",
    "        token_freq[search_key] += value\n",
    "    # word_freqs: gehen jede existierende Folge aus zwei tokens in list of words von vorne bis hinten durch\n",
    "\n",
    "    c = Counter(token_freq)\n",
    "\n",
    "    sorted_token_freq = {key: value for key, value in sorted(\n",
    "        c.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    # Find the most frequent token not already in vocab_bpe\n",
    "    for token in sorted_token_freq.keys():\n",
    "      if token not in vocab_bpe:\n",
    "        first_token = token\n",
    "        break\n",
    "      else:\n",
    "        first_token = None  # Optional: fallback in case all tokens are already in vocab\n",
    "\n",
    "    if first_token:\n",
    "      vocab_bpe.append(first_token)\n",
    "    else:\n",
    "      print(\"No new token to add.\")\n",
    "\n",
    "    # höchster count wird gemerged:\n",
    "    # add to vocab\n",
    "    #first_token = list(sorted_token_freq.keys())[0]\n",
    "    #vocab_bpe.append(first_token)\n",
    "    # replace in list of words\n",
    "    # start again?\n",
    "    for i in range(len(dict_matrix)):\n",
    "        token_list, value = dict_matrix[i]\n",
    "        j = 0\n",
    "        while j < len(token_list) - 1:\n",
    "            search_key = token_list[j] + token_list[j + 1]\n",
    "            if search_key == first_token:\n",
    "                merged_token = search_key\n",
    "                # Merge the tokens\n",
    "                token_list = token_list[:j] + [merged_token] + token_list[j + 2:]\n",
    "                # Don't increment j — might be able to merge again\n",
    "            else:\n",
    "                j += 1\n",
    "        dict_matrix[i][0] = token_list\n",
    "    if k:\n",
    "       k -= 1\n",
    "       iteration = (k > 0)\n",
    "\n",
    "    else:\n",
    "       num_rounds += 1\n",
    "       accuracy = performance(dictionary, vocab_bpe, 500)\n",
    "       iteration != (accuracy > 70)\n",
    "\n",
    "       if num_rounds > 1500:\n",
    "          print(\"exceeded, accuracy: \", accuracy)\n",
    "\n",
    "\n",
    "          iteration = False\n",
    "\n",
    "  return vocab_bpe, sorted_token_freq, dict_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:50:53.809092Z",
     "start_time": "2025-07-30T12:50:53.788786Z"
    }
   },
   "outputs": [],
   "source": [
    "# import necessary imports\n",
    "import numpy as np\n",
    "#from numpy_ml.neural_nets.optimizers import Adam\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:29:35.898826Z",
     "start_time": "2025-07-30T13:29:35.883687Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(input, batch_size,chunk_size):\n",
    "\n",
    "    input_batch = []\n",
    "    # print(type(input_batch))\n",
    "    target_batch = []\n",
    "    idx = np.random.randint(0,len(input)-(chunk_size+1),size=batch_size)\n",
    "    for i in range(0,len(idx)-1):\n",
    "        input_batch.append(input[idx[i]:idx[i]+chunk_size])\n",
    "        target_batch.append(input[idx[i]+1:idx[i]+(chunk_size+1)])\n",
    "    \n",
    "    input_batch = np.array(input_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (r\"Shakespeare_byte.txt\", 'r') as f:\n",
    "   shakespeare_byte_train = eval(f.read())\n",
    "\n",
    "with open (r\"vocab_train.txt\", 'r') as f:\n",
    "   vocab_train = eval(f.read())\n",
    "\n",
    "vocab = vocab_train\n",
    "indices = np.arange(0,len(vocab),1)\n",
    "inidces = indices.astype(int)\n",
    "indices = indices.tolist()\n",
    "key_byte = dict(zip(vocab, indices))\n",
    "value_byte = dict(zip(indices,vocab))\n",
    "\n",
    "# Map each token in shakespeare_byte_train to its index using key_byte\n",
    "indices_translation = [key_byte[token] for token in shakespeare_byte_train if token in key_byte]\n",
    "\n",
    "with open('indices_text.txt', 'w') as indices_text:\n",
    "    indices_text.write(str(indices_translation))\n",
    "\n",
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "  indices_text = eval(f.read())\n",
    "\n",
    "\n",
    "bytes_translation = [value_byte[token] for token in indices_text if token in value_byte]\n",
    "\n",
    "with open('bytes_text.txt', 'w') as bytes_text:\n",
    "    bytes_text.write(str(bytes_translation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:10:22.690210Z",
     "start_time": "2025-07-30T13:10:20.643894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 8)\n",
      "[[1309 1154  176 1219 1263  437 1499 1022]\n",
      " [1264 1262 1196 1267  840 1401 1425  794]\n",
      " [1251 1447 1221 1539 1295  457  110  452]]\n",
      "(3, 8)\n",
      "[[1154  176 1219 1263  437 1499 1022 1260]\n",
      " [1262 1196 1267  840 1401 1425  794 1262]\n",
      " [1447 1221 1539 1295  457  110  452  110]]\n"
     ]
    }
   ],
   "source": [
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "  indices_text = eval(f.read())\n",
    "  \n",
    "x,y = get_batch(indices_text,4,8)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_characters(input):\n",
    "    \"\"\"Decodes a list of indices back to their corresponding characters\n",
    "    given the abive defined vocabulary\"\"\"\n",
    "\n",
    "    decoded = [] #given the input, we will decode it back to characters\n",
    "    for i in range(0,len(input)):\n",
    "        decoded.append(value_byte[input[i]])#using the translation dctionary: value_byte\n",
    "#make its prettier by joining list to actual words and replacing underscores with spaces\n",
    "    decoded = ''.join(decoded)\n",
    "    decoded = decoded.replace('_', ' ')\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:51:07.699419Z",
     "start_time": "2025-07-30T12:51:07.672241Z"
    }
   },
   "outputs": [],
   "source": [
    "class neural_embedding:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding_table = np.random.rand(vocab_size, vocab_size)\n",
    "\n",
    "    def calculate_softmax(self, x):\n",
    "        \"\"\"Takes input array x and returns softmax.\"\"\"\n",
    "        soft_x = np.exp(x - np.max(x))\n",
    "        softer_x = soft_x / np.sum(soft_x)\n",
    "        return softer_x\n",
    "\n",
    "    def calculate_cross_entropy(self, y_hatless, y_hat):\n",
    "        \"\"\"\n",
    "        Takes target (y_hatless) and prediction (y_hat) and computes cross entropy loss.\n",
    "        \"\"\"\n",
    "        # get vocab_size\n",
    "        _, _, vocab_size = y_hat.shape        \n",
    "        y_hat = y_hat.reshape(y_hat.shape[0]*y_hat.shape[1], y_hat.shape[2])\n",
    "        y_hatless_flat = y_hatless.reshape(-1)\n",
    "        # one-hot encode targets\n",
    "        y_hatless_hot = np.eye(vocab_size)[y_hatless_flat]\n",
    "       \n",
    "        y_hat = self.calculate_softmax(y_hat)\n",
    "    \n",
    "        return -np.sum(y_hatless_hot*np.log(y_hat))\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Implements forward pass with an unnecessary logitte function \n",
    "        which i only did not delete because now I'm emotionally attached.\n",
    "        Args:\n",
    "            idx(np.array): (B,T) numpy array of integers\n",
    "            targets(np.array): (B,T) numpy array of integers\n",
    "        Returns:\n",
    "            input_logits(np.array)\n",
    "            sometimes also: targets(np.array)\n",
    "        \"\"\"\n",
    "        batch_size, chunk_size = idx.shape\n",
    "        logits = np.zeros((batch_size, chunk_size, (self.token_embedding_table[0].size)))\n",
    "\n",
    "        def logitte(batch_size, chunk_size, input):\n",
    "            for batch in range(batch_size):\n",
    "                for chunk in range(chunk_size):\n",
    "                    # (B,T,C) b=batch_size, t=\"time\"=chunk_size, c=vocab_size\n",
    "                    logits[batch][chunk] = self.token_embedding_table[input[batch][chunk]]\n",
    "                    \n",
    "            return logits\n",
    "\n",
    "        input_logits = logitte(batch_size, chunk_size, idx)\n",
    "        \n",
    "        if targets is not None:\n",
    "            loss = self.calculate_cross_entropy(targets, input_logits)\n",
    "\n",
    "            return input_logits, loss\n",
    "\n",
    "        return input_logits\n",
    "\n",
    "    \n",
    "    def backward(self, targets, input_logits):\n",
    "        # need to do the same reshaping as we did for cross entropy, apparently\n",
    "        targets_flat = targets.reshape(-1)\n",
    "        one_hot_targets = np.eye(self.vocab_size)[targets_flat]\n",
    "\n",
    "        # shape after: ((batch_size*chunk_size), vocab_size)\n",
    "        input_logits_2d = input_logits.reshape(input_logits.shape[0]*input_logits.shape[1], input_logits.shape[2])\n",
    "\n",
    "        # somehow this is supposedly the combiantion of the derivative of softmax with the derivative of the CCE\n",
    "        delta = one_hot_targets - input_logits_2d\n",
    "\n",
    "        # want shape (80,80) for matrix multiplication, but with correct indices (use one-hot targets for that)\n",
    "        delta_indexed = np.dot(one_hot_targets.transpose(),delta)\n",
    "\n",
    "        # compute gradient for weight matrix: dot product between the transpose of the to layer and delta vector computed above\n",
    "        gradient = (self.token_embedding_table @ delta_indexed) \n",
    "\n",
    "    \n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        collect = idx.tolist()\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self.forward(idx)\n",
    "            logits = logits[:, -1, :]  # take the last time step\n",
    "            probs = self.calculate_softmax(logits)\n",
    "            idx_next = np.random.multinomial(1, probs[0])\n",
    "            print(probs[0][:10])\n",
    "            idx_next = np.where(idx_next==1)\n",
    "            idx[0][:-1] = idx[0][1:]  # shift the array to the left\n",
    "            idx[0][-1] = idx_next[0]\n",
    "            collect[0].append(idx_next[0][0].tolist())\n",
    "\n",
    "\n",
    "        return collect\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:33:52.622952Z",
     "start_time": "2025-07-30T13:33:52.599953Z"
    }
   },
   "outputs": [],
   "source": [
    "# currently do not use loss at all, so something is probably very wrong\n",
    "\n",
    "def train(model, text, optimiser, token_embedding_table, train_step):\n",
    "    batch_size=2\n",
    "    chunk_size = 8\n",
    "    for steps in range(train_step): # TODO: please increase\n",
    "        # sample batch of data\n",
    "        xb, yb = get_batch(text, batch_size, chunk_size) # TODO: pls adapt to above fct\n",
    "        optimiser.zero_grad()  # reset gradients\n",
    "        # get logits and loss\n",
    "        logits, loss = model.forward(xb, yb)\n",
    "        print(f\"loss: {loss}\")\n",
    "\n",
    "        gradient = model.backward(yb, logits)\n",
    "        token_embedding_table.grad = torch.tensor(gradient, dtype=torch.float32)\n",
    "        optimiser.step()  # apply gradients to parameters\n",
    "        # diff = model.token_embedding_table - np.array(token_embedding_table)\n",
    "        # print(f\"Difference in weights before - afte Adam: {diff}\")\n",
    "        model.token_embedding_table = np.array(token_embedding_table)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:28:11.641787Z",
     "start_time": "2025-07-30T13:28:10.773748Z"
    }
   },
   "outputs": [],
   "source": [
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "    indices_text = eval(f.read())\n",
    "with open(r\"vocab_train.txt\", 'r') as f:\n",
    "    vocab_train = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:37:57.602335Z",
     "start_time": "2025-07-30T13:37:46.059846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 74.5139537853827\n",
      "loss: 74.91192117376374\n",
      "loss: 73.7675661446012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belan\\AppData\\Local\\Temp\\ipykernel_17832\\132296554.py:19: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  model.token_embedding_table = np.array(token_embedding_table)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 75.37738648500098\n",
      "loss: 76.4522760817284\n",
      "loss: 75.41412950683558\n",
      "loss: 74.55913979838354\n",
      "loss: 76.77050666774176\n",
      "loss: 75.36549151945742\n",
      "loss: 75.82237788682792\n",
      "loss: 75.50843739260122\n",
      "loss: 76.87292274367081\n",
      "loss: 74.48379499518774\n",
      "loss: 74.095378370072\n",
      "loss: 75.66558194824427\n",
      "loss: 75.70560631715075\n",
      "loss: 76.31180124927506\n",
      "loss: 74.4879029714773\n",
      "loss: 74.9173735424579\n",
      "loss: 76.567640992413\n",
      "loss: 75.38890600344729\n",
      "loss: 76.31815641743697\n",
      "loss: 75.0395383365445\n",
      "loss: 74.87798741885408\n",
      "loss: 73.8185319409175\n",
      "loss: 74.37850037074217\n",
      "loss: 75.23641365215428\n",
      "loss: 76.07787868986561\n",
      "loss: 74.91016039786203\n",
      "loss: 76.21192156018476\n",
      "loss: 75.5957645012733\n",
      "loss: 76.25522698917185\n",
      "loss: 76.69349906204522\n",
      "loss: 75.04949829600648\n",
      "loss: 77.08005714622156\n",
      "loss: 74.92377839550926\n",
      "loss: 74.92039459968353\n",
      "loss: 75.61232710597946\n",
      "loss: 76.12435396313003\n",
      "loss: 76.04878266459357\n",
      "loss: 75.26548777857333\n",
      "loss: 76.42248083439583\n",
      "loss: 75.34182771608401\n",
      "loss: 74.97076900013012\n",
      "loss: 76.94319163673683\n",
      "loss: 75.56229569931452\n",
      "loss: 74.71390977207759\n",
      "loss: 75.21475062207507\n",
      "loss: 75.75459492338916\n",
      "loss: 75.72417842924358\n",
      "loss: 75.93453767754411\n",
      "loss: 76.51428320775881\n",
      "loss: 76.33038793534203\n",
      "loss: 75.07859186195816\n",
      "loss: 76.28529185847393\n",
      "loss: 75.02460580262269\n",
      "loss: 77.50359025229666\n",
      "loss: 74.25478170663557\n",
      "loss: 74.46785136420215\n",
      "loss: 74.67766655196701\n",
      "loss: 75.6334862146634\n",
      "loss: 75.8708230297955\n",
      "loss: 75.17430889742494\n",
      "loss: 75.67924183174907\n",
      "loss: 74.677307990198\n",
      "loss: 75.67424309942592\n",
      "loss: 74.55801190056086\n",
      "loss: 74.97197653394181\n",
      "loss: 76.7645625357195\n",
      "loss: 77.16521718939188\n",
      "loss: 74.40208357813441\n",
      "loss: 75.19554536598382\n",
      "loss: 75.52794015948594\n",
      "loss: 76.795352335009\n",
      "loss: 76.57188444580268\n",
      "loss: 73.98430211111466\n",
      "loss: 74.29142703336937\n",
      "loss: 75.28975931211123\n",
      "loss: 75.66197026387886\n",
      "loss: 75.9729805275343\n",
      "loss: 75.98054682482932\n",
      "loss: 76.3052854036503\n",
      "loss: 74.86415811485786\n",
      "loss: 75.85572535651146\n",
      "loss: 75.78677140633681\n",
      "loss: 74.08841866427943\n",
      "loss: 77.00576533954705\n",
      "loss: 73.74268259604293\n",
      "loss: 76.14470721664081\n",
      "loss: 75.89918110890203\n",
      "loss: 77.74853195905344\n",
      "loss: 75.96279747704529\n",
      "loss: 76.34424638678188\n",
      "loss: 76.1020377576233\n",
      "loss: 75.90059663348572\n",
      "loss: 76.11403530080082\n",
      "loss: 75.46530463614914\n",
      "loss: 75.80487977099737\n",
      "loss: 75.99198563225124\n",
      "loss: 76.71202004590135\n",
      "[0.00086485 0.00040755 0.00047681 0.00070679 0.00093632 0.00058526\n",
      " 0.0004617  0.00063609 0.00061735 0.00086301]\n",
      "[0.00059578 0.00081993 0.00094509 0.00046218 0.00055601 0.00067285\n",
      " 0.00082443 0.00045974 0.00088192 0.00048896]\n",
      "[0.00057172 0.00054227 0.00068317 0.00085166 0.000758   0.00044942\n",
      " 0.00048713 0.00040582 0.00042328 0.00082436]\n",
      "[0.00077685 0.00048669 0.00061428 0.00046493 0.00075143 0.00084299\n",
      " 0.00091241 0.00070295 0.00046394 0.00038577]\n",
      "[0.00048211 0.00049514 0.00046686 0.0007423  0.00047377 0.00077579\n",
      " 0.00061234 0.00080823 0.00062018 0.00042374]\n",
      "[0.0005086  0.00053173 0.00050016 0.00051475 0.0008663  0.00046143\n",
      " 0.00057185 0.00075317 0.00095748 0.00042924]\n",
      "[0.00039843 0.00095641 0.00038395 0.00039229 0.0004444  0.00057609\n",
      " 0.00062472 0.0005749  0.00041655 0.00086324]\n",
      "[0.00041374 0.00042547 0.00073125 0.00042913 0.0007786  0.0006844\n",
      " 0.00076202 0.0005554  0.0005979  0.00092532]\n",
      "[0.00049238 0.00087624 0.00090348 0.00053349 0.00086749 0.00094799\n",
      " 0.00093634 0.00080494 0.00065937 0.00055822]\n",
      "[0.0006807  0.00041308 0.00044179 0.00054185 0.00072056 0.00093308\n",
      " 0.00097023 0.00088322 0.00059926 0.00042526]\n",
      "senger enobpolonius cks himend pratus den thin row still eni\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belan\\AppData\\Local\\Temp\\ipykernel_17832\\3722754344.py:90: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  idx[0][-1] = idx_next[0]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_train)\n",
    "\n",
    "my_neural_embedding = neural_embedding(vocab_size)\n",
    "param_dict = {\"weight\": my_neural_embedding.token_embedding_table} # one entry, key is weight and value is my_neural\n",
    "\n",
    "tensor = torch.tensor(my_neural_embedding.token_embedding_table, dtype=torch.float32)\n",
    "optimiser = optim.Adam([tensor], lr=0.05)\n",
    "train(my_neural_embedding, indices_text, optimiser, tensor, 100) \n",
    "our_output = my_neural_embedding.generate(np.array([[45,603,5]]), max_new_tokens=10)\n",
    "\n",
    "decoded = decode_characters(our_output[0])\n",
    "print(decoded)\n",
    "\n",
    " # example usage of generate method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

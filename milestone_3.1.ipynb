{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:50:53.809092Z",
     "start_time": "2025-07-30T12:50:53.788786Z"
    }
   },
   "outputs": [],
   "source": [
    "# import necessary imports\n",
    "import numpy as np\n",
    "#from numpy_ml.neural_nets.optimizers import Adam\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:29:35.898826Z",
     "start_time": "2025-07-30T13:29:35.883687Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(input, batch_size,chunk_size):\n",
    "\n",
    "    input_batch = []\n",
    "    # print(type(input_batch))\n",
    "    target_batch = []\n",
    "    idx = np.random.randint(0,len(input)-(chunk_size+1),size=batch_size)\n",
    "    for i in range(0,len(idx)-1):\n",
    "        input_batch.append(input[idx[i]:idx[i]+chunk_size])\n",
    "        target_batch.append(input[idx[i]+1:idx[i]+(chunk_size+1)])\n",
    "    \n",
    "    input_batch = np.array(input_batch)\n",
    "    target_batch = np.array(target_batch)\n",
    "\n",
    "    return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (r\"Shakespeare_byte.txt\", 'r') as f:\n",
    "   shakespeare_byte_train = eval(f.read())\n",
    "\n",
    "with open (r\"vocab_train.txt\", 'r') as f:\n",
    "   vocab_train = eval(f.read())\n",
    "\n",
    "vocab = vocab_train\n",
    "indices = np.arange(0,len(vocab),1)\n",
    "inidces = indices.astype(int)\n",
    "indices = indices.tolist()\n",
    "key_byte = dict(zip(vocab, indices))\n",
    "value_byte = dict(zip(indices,vocab))\n",
    "\n",
    "# Map each token in shakespeare_byte_train to its index using key_byte\n",
    "indices_translation = [key_byte[token] for token in shakespeare_byte_train if token in key_byte]\n",
    "\n",
    "with open('indices_text.txt', 'w') as indices_text:\n",
    "    indices_text.write(str(indices_translation))\n",
    "\n",
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "  indices_text = eval(f.read())\n",
    "\n",
    "\n",
    "bytes_translation = [value_byte[token] for token in indices_text if token in value_byte]\n",
    "\n",
    "with open('bytes_text.txt', 'w') as bytes_text:\n",
    "    bytes_text.write(str(bytes_translation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:10:22.690210Z",
     "start_time": "2025-07-30T13:10:20.643894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 8)\n",
      "[[1309 1154  176 1219 1263  437 1499 1022]\n",
      " [1264 1262 1196 1267  840 1401 1425  794]\n",
      " [1251 1447 1221 1539 1295  457  110  452]]\n",
      "(3, 8)\n",
      "[[1154  176 1219 1263  437 1499 1022 1260]\n",
      " [1262 1196 1267  840 1401 1425  794 1262]\n",
      " [1447 1221 1539 1295  457  110  452  110]]\n"
     ]
    }
   ],
   "source": [
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "  indices_text = eval(f.read())\n",
    "  \n",
    "x,y = get_batch(indices_text,4,8)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_characters(input):\n",
    "    \"\"\"Decodes a list of indices back to their corresponding characters\n",
    "    given the abive defined vocabulary\"\"\"\n",
    "\n",
    "    decoded = [] #given the input, we will decode it back to characters\n",
    "    for i in range(0,len(input)):\n",
    "        decoded.append(value_byte[input[i]])#using the translation dctionary: value_byte\n",
    "#make its prettier by joining list to actual words and replacing underscores with spaces\n",
    "    decoded = ''.join(decoded)\n",
    "    decoded = decoded.replace('_', ' ')\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:51:07.699419Z",
     "start_time": "2025-07-30T12:51:07.672241Z"
    }
   },
   "outputs": [],
   "source": [
    "class neural_embedding:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding_table = np.random.rand(vocab_size, vocab_size)\n",
    "\n",
    "    def calculate_softmax(self, x):\n",
    "        \"\"\"Takes input array x and returns softmax.\"\"\"\n",
    "        soft_x = np.exp(x - np.max(x))\n",
    "        softer_x = soft_x / np.sum(soft_x)\n",
    "        return softer_x\n",
    "\n",
    "    def calculate_cross_entropy(self, y_hatless, y_hat):\n",
    "        \"\"\"\n",
    "        Takes target (y_hatless) and prediction (y_hat) and computes cross entropy loss.\n",
    "        \"\"\"\n",
    "        # get vocab_size\n",
    "        _, _, vocab_size = y_hat.shape        \n",
    "        y_hat = y_hat.reshape(y_hat.shape[0]*y_hat.shape[1], y_hat.shape[2])\n",
    "        y_hatless_flat = y_hatless.reshape(-1)\n",
    "        # one-hot encode targets\n",
    "        y_hatless_hot = np.eye(vocab_size)[y_hatless_flat]\n",
    "       \n",
    "        y_hat = self.calculate_softmax(y_hat)\n",
    "    \n",
    "        return -np.sum(y_hatless_hot*np.log(y_hat))\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Implements forward pass with an unnecessary logitte function \n",
    "        which i only did not delete because now I'm emotionally attached.\n",
    "        Args:\n",
    "            idx(np.array): (B,T) numpy array of integers\n",
    "            targets(np.array): (B,T) numpy array of integers\n",
    "        Returns:\n",
    "            input_logits(np.array)\n",
    "            sometimes also: targets(np.array)\n",
    "        \"\"\"\n",
    "        batch_size, chunk_size = idx.shape\n",
    "        logits = np.zeros((batch_size, chunk_size, (self.token_embedding_table[0].size)))\n",
    "\n",
    "        def logitte(batch_size, chunk_size, input):\n",
    "            for batch in range(batch_size):\n",
    "                for chunk in range(chunk_size):\n",
    "                    # (B,T,C) b=batch_size, t=\"time\"=chunk_size, c=vocab_size\n",
    "                    logits[batch][chunk] = self.token_embedding_table[input[batch][chunk]]\n",
    "                    \n",
    "            return logits\n",
    "\n",
    "        input_logits = logitte(batch_size, chunk_size, idx)\n",
    "        \n",
    "        if targets is not None:\n",
    "            loss = self.calculate_cross_entropy(targets, input_logits)\n",
    "\n",
    "            return input_logits, loss\n",
    "\n",
    "        return input_logits\n",
    "\n",
    "    \n",
    "    def backward(self, targets, input_logits):\n",
    "        # need to do the same reshaping as we did for cross entropy, apparently\n",
    "        targets_flat = targets.reshape(-1)\n",
    "        one_hot_targets = np.eye(self.vocab_size)[targets_flat]\n",
    "\n",
    "        # shape after: ((batch_size*chunk_size), vocab_size)\n",
    "        input_logits_2d = input_logits.reshape(input_logits.shape[0]*input_logits.shape[1], input_logits.shape[2])\n",
    "\n",
    "        # somehow this is supposedly the combiantion of the derivative of softmax with the derivative of the CCE\n",
    "        delta = one_hot_targets - input_logits_2d\n",
    "\n",
    "        # want shape (80,80) for matrix multiplication, but with correct indices (use one-hot targets for that)\n",
    "        delta_indexed = np.dot(one_hot_targets.transpose(),delta)\n",
    "\n",
    "        # compute gradient for weight matrix: dot product between the transpose of the to layer and delta vector computed above\n",
    "        gradient = (self.token_embedding_table @ delta_indexed) \n",
    "\n",
    "    \n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        collect = idx.tolist()\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self.forward(idx)\n",
    "            logits = logits[:, -1, :]  # take the last time step\n",
    "            probs = self.calculate_softmax(logits)\n",
    "            idx_next = np.random.multinomial(1, probs[0])\n",
    "            print(probs[0][:10])\n",
    "            idx_next = np.where(idx_next==1)\n",
    "            idx[0][:-1] = idx[0][1:]  # shift the array to the left\n",
    "            idx[0][-1] = idx_next[0]\n",
    "            collect[0].append(idx_next[0][0].tolist())\n",
    "\n",
    "\n",
    "        return collect\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:33:52.622952Z",
     "start_time": "2025-07-30T13:33:52.599953Z"
    }
   },
   "outputs": [],
   "source": [
    "# currently do not use loss at all, so something is probably very wrong\n",
    "\n",
    "def train(model, text, optimiser, token_embedding_table, train_step):\n",
    "    batch_size=2\n",
    "    chunk_size = 8\n",
    "    for steps in range(train_step): # TODO: please increase\n",
    "        # sample batch of data\n",
    "        xb, yb = get_batch(text, batch_size, chunk_size) # TODO: pls adapt to above fct\n",
    "        optimiser.zero_grad()  # reset gradients\n",
    "        # get logits and loss\n",
    "        logits, loss = model.forward(xb, yb)\n",
    "        print(f\"loss: {loss}\")\n",
    "\n",
    "        gradient = model.backward(yb, logits)\n",
    "        token_embedding_table.grad = torch.tensor(gradient, dtype=torch.float32)\n",
    "        optimiser.step()  # apply gradients to parameters\n",
    "        # diff = model.token_embedding_table - np.array(token_embedding_table)\n",
    "        # print(f\"Difference in weights before - afte Adam: {diff}\")\n",
    "        model.token_embedding_table = np.array(token_embedding_table)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:28:11.641787Z",
     "start_time": "2025-07-30T13:28:10.773748Z"
    }
   },
   "outputs": [],
   "source": [
    "with open (r\"indices_text.txt\", 'r') as f:\n",
    "    indices_text = eval(f.read())\n",
    "with open(r\"vocab_train.txt\", 'r') as f:\n",
    "    vocab_train = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:37:57.602335Z",
     "start_time": "2025-07-30T13:37:46.059846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 74.8841135208031\n",
      "loss: 73.90905389118407\n",
      "loss: 77.08283622393236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belan\\AppData\\Local\\Temp\\ipykernel_17832\\132296554.py:19: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  model.token_embedding_table = np.array(token_embedding_table)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 74.6184399737403\n",
      "loss: 75.33619550185036\n",
      "loss: 76.46228423951331\n",
      "loss: 75.37052029241359\n",
      "loss: 75.8539667224812\n",
      "loss: 76.4819888354655\n",
      "loss: 75.47418249705825\n",
      "[0.00060778 0.00039001 0.00059341 0.00090653 0.00057909 0.00054772\n",
      " 0.00099646 0.00053803 0.00039882 0.00040291]\n",
      "[0.00055205 0.00085624 0.00044872 0.00042573 0.00062704 0.00060983\n",
      " 0.00040216 0.0004451  0.00065721 0.0008329 ]\n",
      "[0.00101701 0.00099932 0.00038361 0.00084106 0.00054851 0.00067634\n",
      " 0.00084708 0.00043439 0.00059267 0.00037695]\n",
      "[0.00082889 0.00052736 0.00070014 0.00094304 0.00046292 0.00099572\n",
      " 0.0006982  0.00046387 0.00061094 0.00055781]\n",
      "[0.00087146 0.00043275 0.00049824 0.0004156  0.00060558 0.00070125\n",
      " 0.00067006 0.00073844 0.00082479 0.00078393]\n",
      "[0.00100984 0.00083845 0.00098293 0.00047278 0.00053314 0.00053373\n",
      " 0.00045362 0.0005239  0.00100085 0.00067128]\n",
      "[0.00048743 0.00061111 0.00054496 0.00045846 0.00055329 0.00043714\n",
      " 0.00062214 0.00093085 0.00069166 0.00068655]\n",
      "[0.00040566 0.00063968 0.00099095 0.00077728 0.00042244 0.00075642\n",
      " 0.00058178 0.00071706 0.00090187 0.00057647]\n",
      "[0.0006595  0.00056978 0.00101902 0.00043193 0.00060951 0.00084452\n",
      " 0.00091231 0.00040961 0.00041767 0.00086061]\n",
      "[0.00046148 0.0004761  0.00039829 0.0007076  0.00071407 0.00077826\n",
      " 0.00096746 0.00059768 0.00064809 0.0007045 ]\n",
      "brutus domitius himself desdemona has ents esmatter racauripplose dondoth ste \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belan\\AppData\\Local\\Temp\\ipykernel_17832\\3722754344.py:90: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  idx[0][-1] = idx_next[0]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_train)\n",
    "\n",
    "my_neural_embedding = neural_embedding(vocab_size)\n",
    "param_dict = {\"weight\": my_neural_embedding.token_embedding_table} # one entry, key is weight and value is my_neural\n",
    "\n",
    "tensor = torch.tensor(my_neural_embedding.token_embedding_table, dtype=torch.float32)\n",
    "optimiser = optim.Adam([tensor], lr=0.05)\n",
    "train(my_neural_embedding, indices_text, optimiser, tensor, 10) \n",
    "our_output = my_neural_embedding.generate(np.array([[27,7,20,3,666]]), max_new_tokens=10)\n",
    "\n",
    "decoded = decode_characters(our_output[0])\n",
    "print(decoded)\n",
    "\n",
    " # example usage of generate method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

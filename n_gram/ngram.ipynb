{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd05c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import n_gram\n",
    "import generator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf5f1011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intrinsic_eval_set(n_gram_list, test_corpus):\n",
    "    print(\"starting intrinsic eval for set\")\n",
    "    perplexities = []\n",
    "    for n_gram in n_gram_list:\n",
    "        print(\"N-gram of order: \", n_gram.ndim)\n",
    "        print(test_corpus[:20])\n",
    "        perplexity = n_gram.perplexity(test_corpus)\n",
    "        perplexities.append(perplexity)\n",
    "    perplexities = np.array(perplexities)\n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d4db886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intrinsic_eval_all(paths_train, paths_test, vocab_list):\n",
    "    print(\"starting intrinsic evaluation\")\n",
    "    all_perplexities = []\n",
    "    all_n_grams = []\n",
    "    for path_train, path_test, vocab in zip(paths_train, paths_test, vocab_list):\n",
    "       \n",
    "        with open(path_train, \"r\") as f:\n",
    "            n_gram_corps_train = f.read().split() \n",
    "\n",
    "        with open(path_test, \"r\") as f:\n",
    "            n_gram_corps_test = f.read().split() \n",
    "        \n",
    "        \n",
    "        with open(vocab, \"r\", encoding=\"utf-8\") as f:\n",
    "            vocab = json.load(f)\n",
    "\n",
    "        our_n_grams = generate_n_grams(n_gram_corps_train, 4 , vocab)\n",
    "        # unigram always idx 0, bigram always idx 1, etc.\n",
    "        perplexities = intrinsic_eval_set(our_n_grams, n_gram_corps_test)\n",
    "        all_perplexities.append(perplexities)\n",
    "        all_n_grams.append(our_n_grams)\n",
    "    all_perplexities = np.array(all_perplexities)\n",
    "    return all_perplexities, all_n_grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81d45cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perplexities(all_perplexities, fig_path):\n",
    "    n_grams = [1, 2, 3, 4]\n",
    "    versions = [\"Best\", \"2nd\", \"3rd\"]\n",
    "    plt.figure(figsize=(8,5))\n",
    "    for i in range(all_perplexities.shape[0]):\n",
    "        plt.plot(n_grams, all_perplexities[i], marker='o', label=versions[i])\n",
    "        plt.xticks(n_grams)\n",
    "        plt.xlabel(\"N-gram Order\")\n",
    "        plt.ylabel(\"Perplexity\")\n",
    "        plt.title(\"N-gram Perplexity Across Dataset Versions\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(fig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edea9468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_prplx_diff(all_perplexities, fig_path):\n",
    "    n_grams = [1, 2, 3, 4]\n",
    "    versions = [\"2nd\", \"3rd\"]\n",
    "\n",
    "    # compute differences relative to \"Best\"\n",
    "    diffs = all_perplexities[1:] - all_perplexities[0]\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    for i in range(diffs.shape[0]):\n",
    "        plt.plot(n_grams, diffs[i], marker='o', label=versions[i])\n",
    "\n",
    "    plt.xticks(n_grams)\n",
    "    plt.xlabel(\"N-gram Order\")\n",
    "    plt.ylabel(\"Δ Perplexity (vs. Best)\")\n",
    "    plt.title(\"Perplexity Difference Relative to Best Merge\")\n",
    "    plt.axhline(0, color=\"black\", linewidth=1, linestyle=\"--\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(fig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4406266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrinsic_eval_all(all_n_grams, all_vocabs, context=\"cleopatra is my\", out_path=None):\n",
    "    # all_n_grams: list of lists of N_gram objects for each dataset version\n",
    "    all_outputs = []\n",
    "    for idx, n_gram_list in enumerate(all_n_grams):\n",
    "        vocab = all_vocabs[idx]\n",
    "        # convert objects → dicts expected by generate()\n",
    "        ngram_dicts = [ng.n_gram_probs for ng in n_gram_list]\n",
    "        outputs = []\n",
    "        for order in range(1, len(ngram_dicts)+1):\n",
    "            outputs.append(generator.generate(context, ngram_dicts, order, vocab))\n",
    "        all_outputs.append(outputs)\n",
    "\n",
    "    # Optional: to a markdown table\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(\n",
    "            all_outputs,\n",
    "            columns=[\"Unigram\", \"Bigram\", \"Trigram\", \"4-gram\"],\n",
    "            index=[f\"Version {i+1}\" for i in range(len(all_outputs))]\n",
    "        )\n",
    "        if out_path:\n",
    "            df.to_markdown(out_path)\n",
    "        else:\n",
    "            print(df.to_markdown())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return all_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "660e4087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrinsic_eval_all_prob_fine(all_n_grams, all_vocabs, context=\"cleopatra is my\", out_path=None):\n",
    "    \"\"\"\n",
    "    Run extrinsic evaluation on multiple models/versions and return generated samples.\n",
    "\n",
    "    Args:\n",
    "        all_n_grams: list of lists, each inner list = [unigram_model, bigram_model, ...]\n",
    "        all_vocab: list of vocabs for each merge, each contains list of subword tokens\n",
    "        context: str, starting context for generation\n",
    "        out_path: optional path to save results as CSV/Markdown\n",
    "\n",
    "    Returns:\n",
    "        results: list of lists of strings\n",
    "            results[i][j] = sample from model i, n-gram order j\n",
    "    \"\"\"\n",
    "    print(\"Starting extrinsic eval\")\n",
    "    all_outputs = []\n",
    "\n",
    "    for model_idx, n_gram_list in enumerate(all_n_grams):\n",
    "        print(f\"Next dataset version: {model_idx+1}\")\n",
    "        vocab = all_vocabs[model_idx]\n",
    "        outputs_per_model = []\n",
    "        n_list = [ng.n_gram_probs for ng in n_gram_list]  # dicts only\n",
    "        for n in range(1, len(n_list)+1):\n",
    "            print(f\"  Generating with {n}-gram\")\n",
    "            n_gram_out = generator.generate(context, n_list, n, vocab)\n",
    "            outputs_per_model.append(n_gram_out)\n",
    "        \n",
    "        all_outputs.append(outputs_per_model)\n",
    "\n",
    "    # Optional: save as markdown table\n",
    "    if out_path:\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(\n",
    "            all_outputs,\n",
    "            columns=[\"Unigram\", \"Bigram\", \"Trigram\", \"4-gram\"],\n",
    "            index=[f\"Merge {i+1}\" for i in range(len(all_outputs))]\n",
    "        )\n",
    "        df.to_markdown(out_path)\n",
    "\n",
    "    return all_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b11f91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'intrinsic_eval_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m all_test \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Shakespeare_best_merge_test.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Shakespeare_2nd_best_merge_test.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Shakespeare_3rd_best_merge_test.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m all_vocab \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../vocab_best.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../vocab_2nd.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../vocab_3rd.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m all_perplexities, all_n_grams \u001b[38;5;241m=\u001b[39m \u001b[43mintrinsic_eval_all\u001b[49m(all_train, all_test, all_vocab)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_perplexities)\n\u001b[1;32m     11\u001b[0m list_of_n_grams \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'intrinsic_eval_all' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "all_train = (\"../Shakespeare_best_merge_train.txt\", \"../Shakespeare_2nd_best_merge_train.txt\", \"../Shakespeare_3rd_best_merge_train.txt\")\n",
    "all_test = (\"../Shakespeare_best_merge_test.txt\", \"../Shakespeare_2nd_best_merge_test.txt\", \"../Shakespeare_3rd_best_merge_test.txt\")\n",
    "all_vocab = (\"../vocab_best.json\", \"../vocab_2nd.json\", \"../vocab_3rd.json\")\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99323d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import n_gram\n",
    "\n",
    "with open(\"../vocab_best.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab = json.load(f)\n",
    "\n",
    "with open(\"../Shakespeare_best_merge_train.txt\", \"r\") as f:\n",
    "    n_gram_corps_train = f.read().split() \n",
    "\n",
    "with open(\"../Shakespeare_best_merge_test.txt\", \"r\") as f:\n",
    "    n_gram_corps_test = f.read().split() \n",
    "\n",
    "best_fourgram = n_gram.N_gram(n_gram_corps_train, 4, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4befaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fourgram.perplexity(n_gram_corps_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24481df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76cee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_path_1 = Path('img') / 'n_gram_perplexities.png'\n",
    "fig_path_2 = Path('img') / 'n_gram_prplx_diff.png'\n",
    "plot_perplexities(all_perplexities, fig_path_1)\n",
    "plot_prplx_diff(all_perplexities, fig_path_2)\n",
    "out_path_df = Path('n_gram') / 'n_gram_sample.md'\n",
    "all_text_output = extrinsic_eval_all(all_n_grams, all_vocab, context=\"cleopatra is my\", out_path=out_path_df)\n",
    "print(all_text_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
